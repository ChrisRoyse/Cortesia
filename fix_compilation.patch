diff --git a/src/enhanced_knowledge_storage/ai_components/hybrid_model_backend.rs b/src/enhanced_knowledge_storage/ai_components/hybrid_model_backend.rs
index 1234567..2345678 100644
--- a/src/enhanced_knowledge_storage/ai_components/hybrid_model_backend.rs
+++ b/src/enhanced_knowledge_storage/ai_components/hybrid_model_backend.rs
@@ -215,10 +215,11 @@ impl HybridModelBackend {
                     capabilities: ModelCapabilities {
                         embeddings: true,
-                        ner: backend_model_id.contains("ner"),
-                        classification: true,
-                        generation: backend_model_id.contains("generation"),
-                        max_sequence_length: 512,
+                        text_generation: backend_model_id.contains("generation"),
+                        entity_extraction: backend_model_id.contains("ner"),
+                        max_sequence_length: 512,
+                        embedding_dimensions: match model_id {
+                            "sentence-transformers/all-MiniLM-L6-v2" => Some(384),
+                            _ => Some(768),
+                        },
                     },
                     backend_type: if local_available { BackendType::Local } else { BackendType::Remote },
                     memory_usage,
@@ -298,7 +298,7 @@ impl HybridModelBackend {
         // No models available at all
-        Err(EnhancedStorageError::ServiceUnavailable(
+        Err(EnhancedStorageError::ModelNotFound(
             "No suitable model found in either local or remote backends".to_string()
         ))
     }

diff --git a/src/enhanced_knowledge_storage/ai_components/candle_loader.rs b/src/enhanced_knowledge_storage/ai_components/candle_loader.rs
index 1234567..2345678 100644
--- a/src/enhanced_knowledge_storage/ai_components/candle_loader.rs
+++ b/src/enhanced_knowledge_storage/ai_components/candle_loader.rs
@@ -18,12 +18,15 @@ impl CandleModelLoader {
     
     pub fn load_bert_model(&self, model_path: &Path) -> candle_core::Result<BertModel> {
         // Load config
         let config_path = model_path.join("config.json");
         let config_str = std::fs::read_to_string(config_path)?;
-        let config: Config = serde_json::from_str(&config_str)?;
+        let config: Config = serde_json::from_str(&config_str)
+            .map_err(|e| candle_core::Error::Msg(format!("Failed to parse config: {}", e)))?;
         
         // Load weights
         let weights_path = model_path.join("model.safetensors");
         let weights = candle_core::safetensors::load(&weights_path, &self.device)?;

diff --git a/src/enhanced_knowledge_storage/model_management/model_cache.rs b/src/enhanced_knowledge_storage/model_management/model_cache.rs
index 1234567..2345678 100644
--- a/src/enhanced_knowledge_storage/model_management/model_cache.rs
+++ b/src/enhanced_knowledge_storage/model_management/model_cache.rs
@@ -9,6 +9,13 @@ use crate::enhanced_knowledge_storage::types::*;
 use tracing::{debug, info, warn};
 use std::time::{Duration, Instant};
 
+/// Backend type for model loading  
+#[derive(Debug, Clone, Copy, PartialEq)]
+pub enum BackendType {
+    Local,
+    Remote,
+    Mock,
+}
+
 /// Model handle containing loaded model metadata
 #[derive(Clone, Debug)]
 pub struct ModelHandle {
@@ -17,6 +24,7 @@ pub struct ModelHandle {
     pub metadata: ModelMetadata,
     pub loaded_at: Instant,
     pub memory_usage: u64,
+    pub backend_type: BackendType,
 }

diff --git a/src/enhanced_knowledge_storage/types.rs b/src/enhanced_knowledge_storage/types.rs
index 1234567..2345678 100644
--- a/src/enhanced_knowledge_storage/types.rs
+++ b/src/enhanced_knowledge_storage/types.rs
@@ -101,6 +101,15 @@ pub struct ModelMetadata {
     pub supported_tasks: Vec<String>,
 }
 
+/// Model capabilities  
+#[derive(Debug, Clone)]
+pub struct ModelCapabilities {
+    pub embeddings: bool,
+    pub text_generation: bool,
+    pub entity_extraction: bool,
+    pub max_sequence_length: usize,
+    pub embedding_dimensions: Option<usize>,
+}
+
 /// Task complexity assessment
 #[derive(Debug, Clone, Copy)]
 pub struct TaskComplexity {