# Phase 5.2: Automated Performance Testing and Monitoring
name: Performance Monitoring

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run comprehensive performance tests nightly
    - cron: '0 1 * * *'
  workflow_dispatch:
    inputs:
      scenario:
        description: 'Performance test scenario'
        required: true
        default: 'comprehensive'
        type: choice
        options:
        - quick
        - comprehensive
        - stress
        - memory
        - concurrency
      baseline_comparison:
        description: 'Compare against baseline'
        required: false
        default: true
        type: boolean

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1
  LLMKG_PERFORMANCE_MODE: true

jobs:
  performance-tests:
    name: Performance Tests
    runs-on: ${{ matrix.os }}
    timeout-minutes: 120
    
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest]
        scenario: [quick, comprehensive]
        include:
          - os: ubuntu-latest
            scenario: quick
            timeout: 30
          - os: ubuntu-latest  
            scenario: comprehensive
            timeout: 90
          # Only run stress tests on schedule or manual trigger
          - os: ubuntu-latest
            scenario: stress
            timeout: 180
            only-on: schedule
    
    # Skip stress tests on regular pushes/PRs
    if: matrix.only-on != 'schedule' || github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Need full history for baseline comparison
        
    - name: Setup Rust toolchain
      uses: dtolnay/rust-toolchain@stable
      with:
        components: rustfmt, clippy
        
    - name: Cache Rust dependencies
      uses: Swatinem/rust-cache@v2
      with:
        key: ${{ matrix.os }}-performance-${{ matrix.scenario }}
        
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          build-essential \
          pkg-config \
          libssl-dev \
          valgrind \
          linux-tools-common \
          linux-tools-generic \
          sysstat \
          bc \
          python3 \
          python3-pip
          
    - name: Install Python dependencies for analysis
      run: |
        pip3 install --user pandas matplotlib seaborn numpy scipy
        
    - name: Setup performance monitoring
      run: |
        sudo sysctl -w kernel.perf_event_paranoid=-1
        sudo sysctl -w kernel.kptr_restrict=0
        
    - name: Build optimized release version
      run: |
        cargo build --release --all-features
        
    - name: Run comprehensive Criterion benchmarks
      run: |
        # Create results directory
        mkdir -p performance-results
        
        # Run our new comprehensive benchmarks
        cargo bench --bench comprehensive_benchmarks \
          -- --output-format json | tee performance-results/criterion-comprehensive.json
          
        cargo bench --bench performance_targets \
          -- --output-format json | tee performance-results/criterion-targets.json
          
        cargo bench --bench simple_zero_copy_bench \
          -- --output-format json | tee performance-results/criterion-zero-copy.json
          
    - name: Run performance integration tests
      env:
        SCENARIO: ${{ matrix.scenario }}
        OUTPUT_DIR: ${{ github.workspace }}/performance-results
        TEST_TIMEOUT: ${{ matrix.timeout * 60 }}
        DEBUG_MODE: true
      run: |
        chmod +x scripts/performance-test-runner.sh
        scripts/performance-test-runner.sh
        
    - name: Download baseline performance data
      if: github.event_name == 'pull_request' || github.event.inputs.baseline_comparison == 'true'
      uses: actions/download-artifact@v3
      with:
        name: performance-baseline-${{ matrix.os }}-${{ matrix.scenario }}
        path: baseline-results/
      continue-on-error: true
      
    - name: Analyze performance regression
      if: github.event_name == 'pull_request' || github.event.inputs.baseline_comparison == 'true'
      run: |
        python3 scripts/performance-regression-analyzer.py \
          --current-results performance-results/ \
          --baseline-results baseline-results/ \
          --output-report performance-results/regression-analysis.json \
          --threshold-degradation 10.0 \
          --threshold-improvement 5.0 \
          --generate-plots
          
    - name: Generate performance dashboard
      run: |
        python3 scripts/generate-performance-dashboard.py \
          --input-dir performance-results/ \
          --output performance-results/dashboard.html \
          --include-regression-analysis \
          --include-system-metrics
          
    - name: Upload performance results
      uses: actions/upload-artifact@v3
      with:
        name: performance-results-${{ matrix.os }}-${{ matrix.scenario }}-${{ github.sha }}
        path: performance-results/
        retention-days: 30
        
    - name: Store performance baseline
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      uses: actions/upload-artifact@v3
      with:
        name: performance-baseline-${{ matrix.os }}-${{ matrix.scenario }}
        path: performance-results/
        retention-days: 90
        
    - name: Comment performance results on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = 'performance-results/regression-analysis.json';
          
          if (fs.existsSync(path)) {
            const analysis = JSON.parse(fs.readFileSync(path, 'utf8'));
            
            let comment = `## 📊 Performance Analysis - ${{ matrix.scenario }}\n\n`;
            
            if (analysis.summary.regressions.length > 0) {
              comment += `### ⚠️ Performance Regressions Detected\n\n`;
              for (const regression of analysis.summary.regressions) {
                comment += `- **${regression.test}**: ${regression.degradation}% slower\n`;
              }
              comment += `\n`;
            }
            
            if (analysis.summary.improvements.length > 0) {
              comment += `### 🚀 Performance Improvements\n\n`;
              for (const improvement of analysis.summary.improvements) {
                comment += `- **${improvement.test}**: ${improvement.improvement}% faster\n`;
              }
              comment += `\n`;
            }
            
            comment += `### 📈 Key Metrics\n\n`;
            comment += `| Test Suite | Status | Change |\n`;
            comment += `|------------|--------|---------|\n`;
            
            for (const metric of analysis.detailed_metrics) {
              const status = metric.regression ? '⚠️' : metric.improvement ? '🚀' : '✅';
              const change = metric.change ? `${metric.change > 0 ? '+' : ''}${metric.change.toFixed(1)}%` : 'No change';
              comment += `| ${metric.name} | ${status} | ${change} |\n`;
            }
            
            comment += `\n[View detailed dashboard](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          }
          
    - name: Fail on significant performance regression
      if: github.event_name == 'pull_request'
      run: |
        if [ -f "performance-results/regression-analysis.json" ]; then
          python3 -c "
import json
import sys

with open('performance-results/regression-analysis.json', 'r') as f:
    analysis = json.load(f)

critical_regressions = [r for r in analysis['summary']['regressions'] if r['degradation'] > 20.0]

if critical_regressions:
    print('CRITICAL: Performance regressions > 20% detected:')
    for r in critical_regressions:
        print(f'  - {r[\"test\"]}: {r[\"degradation\"]}% slower')
    sys.exit(1)
else:
    print('No critical performance regressions detected')
"
        fi

  benchmark-comparison:
    name: Benchmark Comparison
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    
    steps:
    - name: Checkout PR head
      uses: actions/checkout@v4
      with:
        ref: ${{ github.event.pull_request.head.sha }}
        
    - name: Setup Rust
      uses: dtolnay/rust-toolchain@stable
      
    - name: Run PR benchmarks
      run: |
        cargo bench --bench comprehensive_benchmarks > pr-benchmarks.txt
        
    - name: Checkout main branch
      uses: actions/checkout@v4
      with:
        ref: main
        
    - name: Run main benchmarks
      run: |
        cargo bench --bench comprehensive_benchmarks > main-benchmarks.txt
        
    - name: Compare benchmarks
      run: |
        python3 - << 'EOF'
import re
import sys

def parse_benchmarks(filename):
    benchmarks = {}
    with open(filename, 'r') as f:
        content = f.read()
        # Parse Criterion output format
        pattern = r'(\w+/\w+)\s+time:\s+\[(\d+\.?\d*)\s*(\w+)'
        matches = re.findall(pattern, content)
        for name, time, unit in matches:
            # Convert to nanoseconds for comparison
            time_ns = float(time)
            if unit == 'μs':
                time_ns *= 1000
            elif unit == 'ms':
                time_ns *= 1000000
            elif unit == 's':
                time_ns *= 1000000000
            benchmarks[name] = time_ns
    return benchmarks

try:
    main_benchmarks = parse_benchmarks('main-benchmarks.txt')
    pr_benchmarks = parse_benchmarks('pr-benchmarks.txt')
    
    print("# Benchmark Comparison")
    print("| Benchmark | Main | PR | Change |")
    print("|-----------|------|-------|---------|")
    
    significant_changes = []
    
    for name in sorted(set(main_benchmarks.keys()) | set(pr_benchmarks.keys())):
        if name in main_benchmarks and name in pr_benchmarks:
            main_time = main_benchmarks[name]
            pr_time = pr_benchmarks[name]
            change = ((pr_time - main_time) / main_time) * 100
            
            change_str = f"{change:+.1f}%"
            if abs(change) > 10:
                change_str += " ⚠️"
                significant_changes.append((name, change))
            elif change < -5:
                change_str += " 🚀"
                
            print(f"| {name} | {main_time:.0f}ns | {pr_time:.0f}ns | {change_str} |")
        elif name in pr_benchmarks:
            print(f"| {name} | - | {pr_benchmarks[name]:.0f}ns | NEW |")
        else:
            print(f"| {name} | {main_benchmarks[name]:.0f}ns | - | REMOVED |")
    
    if significant_changes:
        print("\n## Significant Changes (>10%)")
        for name, change in significant_changes:
            print(f"- {name}: {change:+.1f}%")
            
except Exception as e:
    print(f"Error comparing benchmarks: {e}")
    sys.exit(1)
EOF

  performance-analysis:
    name: Performance Analysis
    needs: performance-tests
    runs-on: ubuntu-latest
    if: always()
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Download all performance results
      uses: actions/download-artifact@v3
      with:
        pattern: performance-results-*
        path: all-results/
        
    - name: Install analysis tools
      run: |
        pip3 install --user pandas matplotlib seaborn numpy scipy plotly
        
    - name: Generate comprehensive analysis
      run: |
        python3 scripts/comprehensive-performance-analysis.py \
          --input-dir all-results/ \
          --output analysis-report/ \
          --generate-trends \
          --generate-comparisons \
          --include-recommendations
          
    - name: Upload analysis report
      uses: actions/upload-artifact@v3
      with:
        name: performance-analysis-report
        path: analysis-report/
        
    - name: Update performance tracking
      if: github.ref == 'refs/heads/main'
      run: |
        # Update performance tracking database/metrics
        python3 scripts/update-performance-tracking.py \
          --results-dir all-results/ \
          --commit-sha ${{ github.sha }} \
          --timestamp "$(date -Iseconds)"