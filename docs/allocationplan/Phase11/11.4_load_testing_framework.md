# 11.4 Load Testing Framework - Micro Tasks

## Overview
Build comprehensive load testing framework to validate system performance under production-scale workloads, identify bottlenecks, and ensure scalability targets are met.

## Micro Tasks

### Task 11.4.1: Create Load Test Harness Core
**Objective**: Build extensible framework for generating various load patterns.

**Requirements**:
- Configurable load generators
- Multiple workload patterns
- Real-time metrics collection
- Distributed load generation

**Implementation**:
```rust
// Create src/loadtest/mod.rs
use tokio::sync::Semaphore;
use std::sync::atomic::{AtomicU64, Ordering};
use async_trait::async_trait;

#[async_trait]
pub trait LoadGenerator: Send + Sync {
    async fn generate_request(&self) -> Request;
    fn name(&self) -> &str;
}

pub struct LoadTestConfig {
    pub duration: Duration,
    pub target_rps: f64,
    pub ramp_up_time: Duration,
    pub concurrent_users: usize,
    pub workload_mix: Vec<(Box<dyn LoadGenerator>, f64)>, // (generator, weight)
}

pub struct LoadTestHarness {
    config: LoadTestConfig,
    metrics: Arc<LoadTestMetrics>,
}

pub struct LoadTestMetrics {
    total_requests: AtomicU64,
    successful_requests: AtomicU64,
    failed_requests: AtomicU64,
    total_latency_ns: AtomicU64,
}

impl LoadTestHarness {
    pub async fn run(&self) -> LoadTestResult {
        let start_time = Instant::now();
        let semaphore = Arc::new(Semaphore::new(self.config.concurrent_users));
        let rate_limiter = Arc::new(RateLimiter::new(self.config.target_rps));
        
        let mut handles = Vec::new();
        
        // Spawn load generation tasks
        for _ in 0..self.config.concurrent_users {
            let sem = Arc::clone(&semaphore);
            let rate_limiter = Arc::clone(&rate_limiter);
            let metrics = Arc::clone(&self.metrics);
            let generators = self.config.workload_mix.clone();
            let duration = self.config.duration;
            let ramp_up = self.config.ramp_up_time;
            
            let handle = tokio::spawn(async move {
                while start_time.elapsed() < duration {
                    let _permit = sem.acquire().await.unwrap();
                    rate_limiter.acquire().await;
                    
                    // Apply ramp-up
                    let elapsed = start_time.elapsed();
                    if elapsed < ramp_up {
                        let ramp_factor = elapsed.as_secs_f64() / ramp_up.as_secs_f64();
                        tokio::time::sleep(Duration::from_secs_f64(
                            (1.0 - ramp_factor) * 0.1
                        )).await;
                    }
                    
                    // Select generator based on weights
                    let generator = Self::select_generator(&generators);
                    let request = generator.generate_request().await;
                    
                    // Execute request
                    let request_start = Instant::now();
                    let result = self.execute_request(request).await;
                    let latency = request_start.elapsed();
                    
                    // Update metrics
                    metrics.total_requests.fetch_add(1, Ordering::Relaxed);
                    metrics.total_latency_ns.fetch_add(
                        latency.as_nanos() as u64,
                        Ordering::Relaxed
                    );
                    
                    match result {
                        Ok(_) => {
                            metrics.successful_requests.fetch_add(1, Ordering::Relaxed);
                        }
                        Err(_) => {
                            metrics.failed_requests.fetch_add(1, Ordering::Relaxed);
                        }
                    }
                }
            });
            
            handles.push(handle);
        }
        
        // Wait for all tasks to complete
        futures::future::join_all(handles).await;
        
        self.generate_report()
    }
    
    fn generate_report(&self) -> LoadTestResult {
        let total = self.metrics.total_requests.load(Ordering::Relaxed);
        let successful = self.metrics.successful_requests.load(Ordering::Relaxed);
        let failed = self.metrics.failed_requests.load(Ordering::Relaxed);
        let total_latency_ns = self.metrics.total_latency_ns.load(Ordering::Relaxed);
        
        let avg_latency_ms = if total > 0 {
            (total_latency_ns / total) as f64 / 1_000_000.0
        } else {
            0.0
        };
        
        LoadTestResult {
            total_requests: total,
            successful_requests: successful,
            failed_requests: failed,
            success_rate: (successful as f64 / total as f64) * 100.0,
            average_latency_ms: avg_latency_ms,
            requests_per_second: total as f64 / self.config.duration.as_secs_f64(),
        }
    }
}
```

**Validation**:
- Framework compiles successfully
- Rate limiting works correctly
- Metrics are accurate
- Concurrent execution safe

---

### Task 11.4.2: Implement Allocation Workload Generator
**Objective**: Generate realistic allocation request patterns.

**Requirements**:
- Various input vector sizes
- Different pattern distributions
- Burst traffic simulation
- Edge case generation

**Implementation**:
```rust
// Create src/loadtest/workloads/allocation.rs
use super::*;
use rand::{Rng, distributions::{Distribution, Uniform}};

pub struct AllocationWorkloadGenerator {
    vector_size_dist: Uniform<usize>,
    value_dist: Uniform<f32>,
    burst_probability: f64,
    burst_size: usize,
}

impl AllocationWorkloadGenerator {
    pub fn new(config: AllocationWorkloadConfig) -> Self {
        Self {
            vector_size_dist: Uniform::new(
                config.min_vector_size,
                config.max_vector_size
            ),
            value_dist: Uniform::new(0.0, 1.0),
            burst_probability: config.burst_probability,
            burst_size: config.burst_size,
        }
    }
    
    fn generate_input_vector(&self) -> Vec<f32> {
        let mut rng = rand::thread_rng();
        let size = self.vector_size_dist.sample(&mut rng);
        
        (0..size)
            .map(|_| self.value_dist.sample(&mut rng))
            .collect()
    }
}

#[async_trait]
impl LoadGenerator for AllocationWorkloadGenerator {
    async fn generate_request(&self) -> Request {
        let mut rng = rand::thread_rng();
        
        // Determine if this is part of a burst
        let is_burst = rng.gen_bool(self.burst_probability);
        
        if is_burst {
            // Generate burst of similar requests
            let base_vector = self.generate_input_vector();
            let requests = (0..self.burst_size)
                .map(|i| {
                    let mut vector = base_vector.clone();
                    // Add small variations
                    for v in &mut vector {
                        *v += rng.gen_range(-0.1..0.1);
                        *v = v.clamp(0.0, 1.0);
                    }
                    Request::Allocate(AllocationRequest {
                        input: vector,
                        correlation_id: format!("burst-{}-{}", 
                            rng.gen::<u64>(), i),
                    })
                })
                .collect();
            
            Request::Burst(requests)
        } else {
            Request::Allocate(AllocationRequest {
                input: self.generate_input_vector(),
                correlation_id: format!("single-{}", rng.gen::<u64>()),
            })
        }
    }
    
    fn name(&self) -> &str {
        "allocation_workload"
    }
}

// Specialized workload patterns
pub struct SpikeWorkloadGenerator {
    base_generator: AllocationWorkloadGenerator,
    spike_interval: Duration,
    spike_multiplier: f64,
    last_spike: Mutex<Instant>,
}

#[async_trait]
impl LoadGenerator for SpikeWorkloadGenerator {
    async fn generate_request(&self) -> Request {
        let mut last_spike = self.last_spike.lock().await;
        
        if last_spike.elapsed() > self.spike_interval {
            *last_spike = Instant::now();
            
            // Generate spike load
            let spike_requests = (0..self.spike_multiplier as usize)
                .map(|_| self.base_generator.generate_request())
                .collect::<Vec<_>>();
            
            let futures = spike_requests.into_iter()
                .map(|gen| async move { gen.await });
            
            let requests = futures::future::join_all(futures).await;
            Request::Burst(requests)
        } else {
            self.base_generator.generate_request().await
        }
    }
    
    fn name(&self) -> &str {
        "spike_workload"
    }
}
```

**Validation**:
- Request patterns match specifications
- Burst generation works correctly
- Distribution parameters applied
- Thread-safe generation

---

### Task 11.4.3: Create Query Pattern Load Generator
**Objective**: Test knowledge graph query performance under load.

**Requirements**:
- Various query complexities
- Read/write mix ratios
- Hot path testing
- Cache behavior validation

**Implementation**:
```rust
// Create src/loadtest/workloads/query.rs
pub struct QueryWorkloadGenerator {
    query_templates: Vec<QueryTemplate>,
    read_write_ratio: f64,
    hot_entity_probability: f64,
    hot_entities: Vec<String>,
}

#[derive(Clone)]
pub struct QueryTemplate {
    pub query_type: QueryType,
    pub complexity: QueryComplexity,
    pub expected_latency_ms: f64,
}

#[derive(Clone)]
pub enum QueryType {
    EntityLookup,
    RelationTraversal { depth: usize },
    SimilaritySearch { k: usize },
    AggregationQuery,
    ComplexJoin,
}

#[async_trait]
impl LoadGenerator for QueryWorkloadGenerator {
    async fn generate_request(&self) -> Request {
        let mut rng = rand::thread_rng();
        
        // Determine read vs write
        if rng.gen_bool(self.read_write_ratio) {
            // Generate read query
            let template = self.query_templates
                .choose(&mut rng)
                .unwrap();
            
            let entity = if rng.gen_bool(self.hot_entity_probability) {
                // Select hot entity
                self.hot_entities.choose(&mut rng).unwrap().clone()
            } else {
                // Generate random entity
                format!("entity_{}", rng.gen::<u64>())
            };
            
            match &template.query_type {
                QueryType::EntityLookup => {
                    Request::Query(QueryRequest::EntityLookup { 
                        entity_id: entity 
                    })
                }
                QueryType::RelationTraversal { depth } => {
                    Request::Query(QueryRequest::GraphTraversal {
                        start_entity: entity,
                        max_depth: *depth,
                        relation_filter: None,
                    })
                }
                QueryType::SimilaritySearch { k } => {
                    Request::Query(QueryRequest::Similarity {
                        reference_entity: entity,
                        top_k: *k,
                        threshold: 0.7,
                    })
                }
                QueryType::AggregationQuery => {
                    Request::Query(QueryRequest::Aggregate {
                        entity_type: "document",
                        group_by: "category",
                        metric: "count",
                    })
                }
                QueryType::ComplexJoin => {
                    Request::Query(QueryRequest::ComplexJoin {
                        entities: vec![entity.clone(), 
                            format!("{}_related", entity)],
                        join_conditions: vec!["has_relation"],
                    })
                }
            }
        } else {
            // Generate write operation
            Request::Write(WriteRequest::UpdateEntity {
                entity_id: format!("entity_{}", rng.gen::<u64>()),
                attributes: HashMap::from([
                    ("updated_at".to_string(), 
                     chrono::Utc::now().to_rfc3339()),
                    ("value".to_string(), 
                     rng.gen::<f64>().to_string()),
                ]),
            })
        }
    }
    
    fn name(&self) -> &str {
        "query_workload"
    }
}
```

**Validation**:
- Query patterns realistic
- Hot path simulation works
- Read/write ratio maintained
- Complexity distribution correct

---

### Task 11.4.4: Implement Stress Test Scenarios
**Objective**: Push system to limits with extreme load patterns.

**Requirements**:
- Memory exhaustion tests
- CPU saturation tests
- Network bandwidth tests
- Concurrent connection limits

**Implementation**:
```rust
// Create src/loadtest/scenarios/stress.rs
pub struct StressTestScenario {
    scenario_type: StressType,
    target_resource: ResourceTarget,
    duration: Duration,
    intensity: f64,
}

pub enum StressType {
    MemoryPressure,
    CpuSaturation,
    NetworkFlood,
    ConnectionExhaustion,
    DiskIoPressure,
}

impl StressTestScenario {
    pub async fn execute(&self) -> StressTestResult {
        match self.scenario_type {
            StressType::MemoryPressure => {
                self.run_memory_pressure_test().await
            }
            StressType::CpuSaturation => {
                self.run_cpu_saturation_test().await
            }
            StressType::NetworkFlood => {
                self.run_network_flood_test().await
            }
            StressType::ConnectionExhaustion => {
                self.run_connection_exhaustion_test().await
            }
            StressType::DiskIoPressure => {
                self.run_disk_io_pressure_test().await
            }
        }
    }
    
    async fn run_memory_pressure_test(&self) -> StressTestResult {
        let start = Instant::now();
        let mut allocations = Vec::new();
        let allocation_size = (1024 * 1024 * self.intensity) as usize; // MB
        
        while start.elapsed() < self.duration {
            // Allocate large vectors
            let large_vec: Vec<u8> = vec![0; allocation_size];
            allocations.push(large_vec);
            
            // Generate allocation requests with large payloads
            let request = Request::Allocate(AllocationRequest {
                input: vec![0.5; 10000], // Large input vector
                correlation_id: format!("mem-stress-{}", 
                    allocations.len()),
            });
            
            let result = self.execute_request(request).await;
            
            // Monitor memory metrics
            let memory_usage = self.get_memory_usage();
            if memory_usage > 0.9 {
                warn!("Memory usage at {}%", memory_usage * 100.0);
            }
            
            tokio::time::sleep(Duration::from_millis(10)).await;
        }
        
        StressTestResult {
            scenario: "memory_pressure",
            duration: start.elapsed(),
            peak_resource_usage: self.get_peak_memory_usage(),
            errors_encountered: self.get_error_count(),
            recovery_time: self.measure_recovery_time().await,
        }
    }
    
    async fn run_cpu_saturation_test(&self) -> StressTestResult {
        let start = Instant::now();
        let cpu_count = num_cpus::get();
        let mut handles = Vec::new();
        
        // Spawn CPU-intensive tasks
        for _ in 0..cpu_count * 2 {
            let duration = self.duration;
            let intensity = self.intensity;
            
            let handle = tokio::task::spawn_blocking(move || {
                let thread_start = Instant::now();
                while thread_start.elapsed() < duration {
                    // CPU-intensive computation
                    let mut result = 1.0;
                    for i in 1..1000 {
                        result = (result * i as f64).sqrt();
                    }
                    std::hint::black_box(result);
                    
                    // Occasional yield
                    if thread_start.elapsed().as_millis() % 100 == 0 {
                        std::thread::yield_now();
                    }
                }
            });
            
            handles.push(handle);
        }
        
        // Run allocation requests concurrently
        let load_handle = tokio::spawn(async move {
            let harness = LoadTestHarness::new(LoadTestConfig {
                duration: self.duration,
                target_rps: 1000.0 * self.intensity,
                concurrent_users: 100,
                ..Default::default()
            });
            harness.run().await
        });
        
        futures::future::join_all(handles).await;
        let load_result = load_handle.await.unwrap();
        
        StressTestResult {
            scenario: "cpu_saturation",
            duration: start.elapsed(),
            peak_resource_usage: self.get_peak_cpu_usage(),
            errors_encountered: load_result.failed_requests,
            recovery_time: self.measure_recovery_time().await,
        }
    }
}
```

**Validation**:
- Stress tests reach limits
- System remains stable
- Recovery metrics accurate
- No permanent damage

---

### Task 11.4.5: Create Chaos Testing Integration
**Objective**: Test system resilience with failure injection.

**Requirements**:
- Network partition simulation
- Process crash simulation
- Resource starvation
- Byzantine failures

**Implementation**:
```rust
// Create src/loadtest/chaos/mod.rs
use tokio::sync::RwLock;

pub struct ChaosEngine {
    active_faults: Arc<RwLock<Vec<Fault>>>,
    fault_probability: f64,
}

#[derive(Clone)]
pub enum Fault {
    NetworkDelay { min_ms: u64, max_ms: u64 },
    NetworkDrop { drop_rate: f64 },
    ProcessCrash { target: String },
    ResourceExhaustion { resource: String, amount: f64 },
    CorruptResponse { corruption_rate: f64 },
    ClockSkew { skew_ms: i64 },
}

impl ChaosEngine {
    pub async fn inject_fault(&self, fault: Fault) {
        let mut faults = self.active_faults.write().await;
        faults.push(fault.clone());
        
        info!("Injected fault: {:?}", fault);
        metrics::counter!("chaos_faults_injected")
            .increment(1);
    }
    
    pub async fn apply_network_chaos(&self, request: &mut Request) -> Result<()> {
        let faults = self.active_faults.read().await;
        
        for fault in faults.iter() {
            match fault {
                Fault::NetworkDelay { min_ms, max_ms } => {
                    let delay = rand::thread_rng()
                        .gen_range(*min_ms..*max_ms);
                    tokio::time::sleep(Duration::from_millis(delay)).await;
                }
                Fault::NetworkDrop { drop_rate } => {
                    if rand::thread_rng().gen_bool(*drop_rate) {
                        return Err(anyhow!("Network packet dropped"));
                    }
                }
                Fault::CorruptResponse { corruption_rate } => {
                    if rand::thread_rng().gen_bool(*corruption_rate) {
                        // Corrupt request data
                        if let Request::Allocate(ref mut alloc) = request {
                            for v in &mut alloc.input {
                                *v = rand::thread_rng().gen();
                            }
                        }
                    }
                }
                _ => {}
            }
        }
        
        Ok(())
    }
}

pub struct ChaosLoadTest {
    base_config: LoadTestConfig,
    chaos_engine: Arc<ChaosEngine>,
    fault_schedule: Vec<(Duration, Fault)>,
}

impl ChaosLoadTest {
    pub async fn run(&self) -> ChaosTestResult {
        let harness = LoadTestHarness::new(self.base_config.clone());
        
        // Start base load
        let load_handle = tokio::spawn({
            let harness = harness.clone();
            async move { harness.run().await }
        });
        
        // Apply fault schedule
        let chaos_handle = tokio::spawn({
            let engine = Arc::clone(&self.chaos_engine);
            let schedule = self.fault_schedule.clone();
            
            async move {
                for (delay, fault) in schedule {
                    tokio::time::sleep(delay).await;
                    engine.inject_fault(fault).await;
                }
            }
        });
        
        let (load_result, _) = tokio::join!(load_handle, chaos_handle);
        let load_result = load_result.unwrap();
        
        // Analyze impact
        ChaosTestResult {
            baseline_result: load_result,
            faults_injected: self.fault_schedule.len(),
            recovery_times: self.measure_recovery_times().await,
            availability_during_chaos: self.calculate_availability(),
            data_consistency_check: self.verify_consistency().await,
        }
    }
}
```

**Validation**:
- Faults inject correctly
- System recovers from failures
- Data remains consistent
- Availability targets met

---

### Task 11.4.6: Implement Performance Regression Detection
**Objective**: Automatically detect performance degradations between versions.

**Requirements**:
- Baseline establishment
- Statistical comparison
- Automated alerting
- Trend analysis

**Implementation**:
```rust
// Create src/loadtest/regression/mod.rs
use statrs::statistics::Statistics;

pub struct PerformanceBaseline {
    version: String,
    timestamp: DateTime<Utc>,
    metrics: BaselineMetrics,
}

#[derive(Clone, Serialize, Deserialize)]
pub struct BaselineMetrics {
    pub p50_latency_ms: f64,
    pub p95_latency_ms: f64,
    pub p99_latency_ms: f64,
    pub throughput_rps: f64,
    pub error_rate: f64,
    pub memory_usage_mb: f64,
    pub cpu_usage_percent: f64,
}

pub struct RegressionDetector {
    baseline_store: Arc<BaselineStore>,
    sensitivity: f64, // Percentage threshold
}

impl RegressionDetector {
    pub async fn analyze(&self, current: LoadTestResult) -> RegressionAnalysis {
        let baseline = self.baseline_store.get_latest().await?;
        let current_metrics = self.extract_metrics(&current);
        
        let regressions = vec![
            self.check_latency_regression(&baseline.metrics, &current_metrics),
            self.check_throughput_regression(&baseline.metrics, &current_metrics),
            self.check_error_regression(&baseline.metrics, &current_metrics),
            self.check_resource_regression(&baseline.metrics, &current_metrics),
        ]
        .into_iter()
        .flatten()
        .collect();
        
        RegressionAnalysis {
            baseline_version: baseline.version,
            current_version: env!("CARGO_PKG_VERSION").to_string(),
            regressions,
            improvement_areas: self.find_improvements(&baseline.metrics, &current_metrics),
            statistical_significance: self.calculate_significance(&baseline.metrics, &current_metrics),
        }
    }
    
    fn check_latency_regression(
        &self,
        baseline: &BaselineMetrics,
        current: &BaselineMetrics
    ) -> Option<Regression> {
        let metrics = vec![
            ("p50", baseline.p50_latency_ms, current.p50_latency_ms),
            ("p95", baseline.p95_latency_ms, current.p95_latency_ms),
            ("p99", baseline.p99_latency_ms, current.p99_latency_ms),
        ];
        
        for (percentile, baseline_val, current_val) in metrics {
            let increase_pct = (current_val - baseline_val) / baseline_val * 100.0;
            
            if increase_pct > self.sensitivity {
                return Some(Regression {
                    metric: format!("{}_latency", percentile),
                    baseline_value: baseline_val,
                    current_value: current_val,
                    degradation_percent: increase_pct,
                    severity: if increase_pct > self.sensitivity * 2.0 {
                        Severity::High
                    } else {
                        Severity::Medium
                    },
                });
            }
        }
        
        None
    }
    
    pub async fn run_comparison_suite(&self) -> ComparisonResult {
        let scenarios = vec![
            ("standard_load", self.create_standard_scenario()),
            ("spike_load", self.create_spike_scenario()),
            ("sustained_load", self.create_sustained_scenario()),
            ("mixed_workload", self.create_mixed_scenario()),
        ];
        
        let mut results = HashMap::new();
        
        for (name, scenario) in scenarios {
            info!("Running {} scenario", name);
            
            let harness = LoadTestHarness::new(scenario);
            let result = harness.run().await;
            
            let analysis = self.analyze(result).await?;
            results.insert(name.to_string(), analysis);
        }
        
        ComparisonResult {
            timestamp: Utc::now(),
            scenarios: results,
            overall_health: self.calculate_overall_health(&results),
            recommended_actions: self.generate_recommendations(&results),
        }
    }
}
```

**Validation**:
- Regression detection accurate
- False positive rate < 5%
- Statistical tests valid
- Trends identified correctly

---

### Task 11.4.7a: Create Test Phase Management
**Objective**: Implement test phase sequencing and lifecycle management.

**Requirements**:
- Test phase definition and structure
- Phase state management
- Sequential execution coordination
- Phase transition control

**Implementation**:
```rust
// Create src/loadtest/orchestration/phase_manager.rs
pub struct TestPhaseManager {
    phases: Vec<TestPhase>,
    current_phase: Option<usize>,
    state: PhaseState,
}

pub struct TestPhase {
    pub id: String,
    pub name: String,
    pub duration: Duration,
    pub workload: LoadTestConfig,
    pub success_criteria: Vec<SuccessCriterion>,
    pub dependencies: Vec<String>,
}

#[derive(Debug, Clone)]
pub enum PhaseState {
    NotStarted,
    Running { start_time: Instant },
    Completed { result: PhaseResult },
    Failed { error: String },
}

impl TestPhaseManager {
    pub fn new(phases: Vec<TestPhase>) -> Self {
        Self {
            phases,
            current_phase: None,
            state: PhaseState::NotStarted,
        }
    }
    
    pub async fn execute_next_phase(&mut self) -> Result<Option<PhaseResult>> {
        if let Some(phase_index) = self.get_next_executable_phase() {
            let phase = &self.phases[phase_index];
            self.current_phase = Some(phase_index);
            self.state = PhaseState::Running { 
                start_time: Instant::now() 
            };
            
            info!("Starting phase: {}", phase.name);
            
            let result = self.execute_phase(phase).await?;
            
            self.state = if result.passed {
                PhaseState::Completed { result: result.clone() }
            } else {
                PhaseState::Failed { 
                    error: format!("Phase {} failed criteria", phase.name) 
                }
            };
            
            Ok(Some(result))
        } else {
            Ok(None)
        }
    }
    
    fn get_next_executable_phase(&self) -> Option<usize> {
        self.phases.iter().enumerate()
            .find(|(i, phase)| self.can_execute_phase(*i, phase))
            .map(|(i, _)| i)
    }
    
    fn can_execute_phase(&self, index: usize, phase: &TestPhase) -> bool {
        // Check if already executed
        if let Some(current) = self.current_phase {
            if index <= current {
                return false;
            }
        }
        
        // Check dependencies
        phase.dependencies.iter().all(|dep_id| {
            self.phases.iter().enumerate()
                .any(|(i, p)| p.id == *dep_id && i < index)
        })
    }
}
```

**Validation**:
- Phase transitions follow dependency order
- State management accurate
- Phase execution isolated
- Error handling robust

---

### Task 11.4.7b: Create Resource Coordination System
**Objective**: Manage test resources and environment coordination.

**Requirements**:
- Resource allocation and cleanup
- Environment state management
- Resource conflict detection
- Cleanup on test failure

**Implementation**:
```rust
// Create src/loadtest/orchestration/resource_manager.rs
pub struct ResourceManager {
    allocated_resources: HashMap<String, AllocatedResource>,
    resource_limits: ResourceLimits,
    cleanup_handlers: Vec<Box<dyn Fn() -> BoxFuture<'static, Result<()>>>>,
}

pub struct AllocatedResource {
    pub id: String,
    pub resource_type: ResourceType,
    pub allocated_at: Instant,
    pub cleanup_fn: Box<dyn Fn() -> BoxFuture<'static, Result<()>>>,
}

pub enum ResourceType {
    TestDatabase { connection_count: usize },
    LoadGenerators { count: usize },
    NetworkBandwidth { mbps: u64 },
    CpuCores { count: usize },
    Memory { gb: usize },
}

impl ResourceManager {
    pub async fn allocate_resource(
        &mut self,
        resource_type: ResourceType,
        phase_id: &str
    ) -> Result<String> {
        let resource_id = format!("{}_{}", phase_id, Uuid::new_v4());
        
        // Check resource availability
        if !self.can_allocate(&resource_type)? {
            return Err(anyhow!("Insufficient resources available"));
        }
        
        // Perform allocation
        let cleanup_fn = self.create_cleanup_function(&resource_type).await?;
        
        let resource = AllocatedResource {
            id: resource_id.clone(),
            resource_type,
            allocated_at: Instant::now(),
            cleanup_fn,
        };
        
        self.allocated_resources.insert(resource_id.clone(), resource);
        
        info!("Allocated resource: {}", resource_id);
        Ok(resource_id)
    }
    
    pub async fn release_resource(&mut self, resource_id: &str) -> Result<()> {
        if let Some(resource) = self.allocated_resources.remove(resource_id) {
            (resource.cleanup_fn)().await?;
            info!("Released resource: {}", resource_id);
        }
        Ok(())
    }
    
    pub async fn cleanup_all(&mut self) -> Result<()> {
        let resource_ids: Vec<_> = self.allocated_resources.keys().cloned().collect();
        
        for resource_id in resource_ids {
            if let Err(e) = self.release_resource(&resource_id).await {
                warn!("Failed to cleanup resource {}: {}", resource_id, e);
            }
        }
        
        Ok(())
    }
    
    fn can_allocate(&self, resource_type: &ResourceType) -> Result<bool> {
        match resource_type {
            ResourceType::TestDatabase { connection_count } => {
                let current_connections = self.count_allocated_connections();
                Ok(current_connections + connection_count <= self.resource_limits.max_db_connections)
            }
            ResourceType::LoadGenerators { count } => {
                let current_generators = self.count_load_generators();
                Ok(current_generators + count <= self.resource_limits.max_load_generators)
            }
            ResourceType::CpuCores { count } => {
                let current_cores = self.count_allocated_cores();
                Ok(current_cores + count <= self.resource_limits.max_cpu_cores)
            }
            _ => Ok(true)
        }
    }
}
```

**Validation**:
- Resources allocated without conflicts
- Cleanup functions execute properly
- Resource limits respected
- Failure recovery cleans up resources

---

### Task 11.4.7c: Create Results Aggregation Engine
**Objective**: Aggregate and analyze results across multiple test phases.

**Requirements**:
- Cross-phase result aggregation
- Statistical analysis
- Performance trend detection
- Summary report generation

**Implementation**:
```rust
// Create src/loadtest/orchestration/results_aggregator.rs
pub struct ResultsAggregator {
    aggregation_strategies: HashMap<String, AggregationStrategy>,
}

pub enum AggregationStrategy {
    Average,
    Maximum,
    Minimum,
    Percentile(f64),
    Sum,
    WeightedAverage,
}

pub struct AggregatedMetrics {
    pub overall_throughput: f64,
    pub average_latency_ms: f64,
    pub p99_latency_ms: f64,
    pub total_requests: u64,
    pub total_errors: u64,
    pub success_rate: f64,
    pub phase_breakdown: Vec<PhaseMetrics>,
    pub performance_trends: PerformanceTrends,
}

impl ResultsAggregator {
    pub async fn aggregate(&self, phase_results: &[PhaseResult]) -> AggregatedMetrics {
        let overall_throughput = self.aggregate_throughput(phase_results);
        let latency_metrics = self.aggregate_latency(phase_results);
        let error_metrics = self.aggregate_errors(phase_results);
        let trends = self.analyze_trends(phase_results);
        
        AggregatedMetrics {
            overall_throughput,
            average_latency_ms: latency_metrics.average,
            p99_latency_ms: latency_metrics.p99,
            total_requests: error_metrics.total_requests,
            total_errors: error_metrics.total_errors,
            success_rate: error_metrics.success_rate,
            phase_breakdown: self.create_phase_breakdown(phase_results),
            performance_trends: trends,
        }
    }
    
    fn aggregate_throughput(&self, phase_results: &[PhaseResult]) -> f64 {
        let total_requests: u64 = phase_results.iter()
            .map(|r| r.load_test_result.total_requests)
            .sum();
        let total_duration: f64 = phase_results.iter()
            .map(|r| r.duration.as_secs_f64())
            .sum();
        
        if total_duration > 0.0 {
            total_requests as f64 / total_duration
        } else {
            0.0
        }
    }
    
    fn aggregate_latency(&self, phase_results: &[PhaseResult]) -> LatencyMetrics {
        let mut all_latencies = Vec::new();
        
        for result in phase_results {
            // Collect latency samples from each phase
            if let Some(samples) = &result.load_test_result.latency_samples {
                all_latencies.extend(samples.iter().cloned());
            }
        }
        
        if all_latencies.is_empty() {
            return LatencyMetrics { average: 0.0, p99: 0.0 };
        }
        
        all_latencies.sort_by(|a, b| a.partial_cmp(b).unwrap());
        
        let average = all_latencies.iter().sum::<f64>() / all_latencies.len() as f64;
        let p99_index = ((all_latencies.len() as f64) * 0.99) as usize;
        let p99 = all_latencies.get(p99_index).copied().unwrap_or(0.0);
        
        LatencyMetrics { average, p99 }
    }
    
    fn analyze_trends(&self, phase_results: &[PhaseResult]) -> PerformanceTrends {
        let mut throughput_trend = Vec::new();
        let mut latency_trend = Vec::new();
        
        for (i, result) in phase_results.iter().enumerate() {
            throughput_trend.push((i as f64, result.load_test_result.requests_per_second));
            latency_trend.push((i as f64, result.load_test_result.average_latency_ms));
        }
        
        PerformanceTrends {
            throughput_slope: self.calculate_trend_slope(&throughput_trend),
            latency_slope: self.calculate_trend_slope(&latency_trend),
            performance_stability: self.calculate_stability(&throughput_trend),
        }
    }
    
    fn calculate_trend_slope(&self, data_points: &[(f64, f64)]) -> f64 {
        if data_points.len() < 2 {
            return 0.0;
        }
        
        let n = data_points.len() as f64;
        let sum_x: f64 = data_points.iter().map(|(x, _)| x).sum();
        let sum_y: f64 = data_points.iter().map(|(_, y)| y).sum();
        let sum_xy: f64 = data_points.iter().map(|(x, y)| x * y).sum();
        let sum_x2: f64 = data_points.iter().map(|(x, _)| x * x).sum();
        
        (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)
    }
}
```

**Validation**:
- Aggregation algorithms correct
- Trend analysis accurate
- Summary metrics calculated properly
- Performance insights meaningful

---

### Task 11.4.8: Add Real-time Load Test Monitoring
**Objective**: Monitor load test progress with live metrics and visualizations.

**Requirements**:
- Real-time metric streaming
- Live dashboard updates
- Anomaly detection
- Test control interface

**Implementation**:
```rust
// Create src/loadtest/monitoring/mod.rs
use tokio::sync::broadcast;

pub struct LoadTestMonitor {
    metrics_channel: broadcast::Sender<MetricUpdate>,
    anomaly_detector: Arc<AnomalyDetector>,
    dashboard_server: Arc<DashboardServer>,
}

#[derive(Clone, Serialize)]
pub struct MetricUpdate {
    timestamp: DateTime<Utc>,
    metric_type: MetricType,
    value: f64,
    labels: HashMap<String, String>,
}

impl LoadTestMonitor {
    pub async fn start(&self) -> Result<()> {
        // Start metrics collection
        let metrics_handle = self.start_metrics_collection();
        
        // Start anomaly detection
        let anomaly_handle = self.start_anomaly_detection();
        
        // Start dashboard server
        let dashboard_handle = self.dashboard_server.start().await?;
        
        // Start WebSocket server for real-time updates
        let ws_handle = self.start_websocket_server().await?;
        
        tokio::select! {
            r = metrics_handle => r?,
            r = anomaly_handle => r?,
            r = dashboard_handle => r?,
            r = ws_handle => r?,
        }
    }
    
    async fn start_websocket_server(&self) -> Result<JoinHandle<()>> {
        let (tx, _rx) = self.metrics_channel.clone();
        
        Ok(tokio::spawn(async move {
            let app = Router::new()
                .route("/ws", ws(handle_websocket))
                .with_state(tx);
            
            axum::Server::bind(&"0.0.0.0:8081".parse().unwrap())
                .serve(app.into_make_service())
                .await
                .unwrap();
        }))
    }
    
    async fn handle_websocket(
        ws: WebSocketUpgrade,
        State(metrics_tx): State<broadcast::Sender<MetricUpdate>>
    ) -> impl IntoResponse {
        ws.on_upgrade(|socket| async move {
            let mut rx = metrics_tx.subscribe();
            let (mut sender, mut receiver) = socket.split();
            
            // Send metrics to client
            let send_task = tokio::spawn(async move {
                while let Ok(metric) = rx.recv().await {
                    let msg = Message::Text(
                        serde_json::to_string(&metric).unwrap()
                    );
                    if sender.send(msg).await.is_err() {
                        break;
                    }
                }
            });
            
            // Handle control messages from client
            let recv_task = tokio::spawn(async move {
                while let Some(Ok(msg)) = receiver.next().await {
                    if let Message::Text(text) = msg {
                        // Handle control commands
                        match serde_json::from_str::<ControlCommand>(&text) {
                            Ok(cmd) => handle_control_command(cmd).await,
                            Err(_) => continue,
                        }
                    }
                }
            });
            
            tokio::select! {
                _ = send_task => {},
                _ = recv_task => {},
            }
        })
    }
}

// Real-time dashboard HTML/JS
const DASHBOARD_HTML: &str = r#"
<!DOCTYPE html>
<html>
<head>
    <title>Load Test Monitor</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
</head>
<body>
    <div id="metrics-container">
        <canvas id="throughput-chart"></canvas>
        <canvas id="latency-chart"></canvas>
        <canvas id="error-chart"></canvas>
    </div>
    
    <script>
        const ws = new WebSocket('ws://localhost:8081/ws');
        const charts = initializeCharts();
        
        ws.onmessage = (event) => {
            const metric = JSON.parse(event.data);
            updateCharts(metric);
        };
        
        function initializeCharts() {
            // Initialize Chart.js charts
            return {
                throughput: new Chart(/* ... */),
                latency: new Chart(/* ... */),
                errors: new Chart(/* ... */)
            };
        }
        
        function updateCharts(metric) {
            // Update charts with new metric data
        }
    </script>
</body>
</html>
"#;
```

**Validation**:
- Real-time updates work
- WebSocket stable under load
- Anomalies detected correctly
- Dashboard responsive

---

### Task 11.4.9: Implement Load Test Report Generation
**Objective**: Generate comprehensive reports with insights and recommendations.

**Requirements**:
- Multiple output formats
- Statistical analysis
- Visualization generation
- Executive summary

**Implementation**:
```rust
// Create src/loadtest/reporting/mod.rs
use plotters::prelude::*;
use tera::{Tera, Context};

pub struct ReportGenerator {
    template_engine: Tera,
    plot_generator: PlotGenerator,
}

impl ReportGenerator {
    pub async fn generate_report(
        &self,
        results: &LoadTestResult,
        format: ReportFormat
    ) -> Result<Report> {
        let analysis = self.analyze_results(results).await?;
        
        match format {
            ReportFormat::Html => self.generate_html_report(results, analysis).await,
            ReportFormat::Pdf => self.generate_pdf_report(results, analysis).await,
            ReportFormat::Json => self.generate_json_report(results, analysis).await,
            ReportFormat::Markdown => self.generate_markdown_report(results, analysis).await,
        }
    }
    
    async fn generate_html_report(
        &self,
        results: &LoadTestResult,
        analysis: Analysis
    ) -> Result<Report> {
        // Generate plots
        let latency_plot = self.plot_generator
            .create_latency_distribution(&results.latency_samples).await?;
        let throughput_plot = self.plot_generator
            .create_throughput_timeline(&results.throughput_timeline).await?;
        let error_plot = self.plot_generator
            .create_error_timeline(&results.error_timeline).await?;
        
        // Create context for template
        let mut context = Context::new();
        context.insert("results", results);
        context.insert("analysis", &analysis);
        context.insert("latency_plot", &latency_plot);
        context.insert("throughput_plot", &throughput_plot);
        context.insert("error_plot", &error_plot);
        context.insert("timestamp", &Utc::now().to_rfc3339());
        
        // Render template
        let html = self.template_engine.render("load_test_report.html", &context)?;
        
        Ok(Report {
            format: ReportFormat::Html,
            content: html.into_bytes(),
            metadata: ReportMetadata {
                generated_at: Utc::now(),
                test_duration: results.duration,
                total_requests: results.total_requests,
                summary: analysis.executive_summary,
            },
        })
    }
    
    fn analyze_results(&self, results: &LoadTestResult) -> Result<Analysis> {
        let latency_stats = self.calculate_latency_statistics(&results.latency_samples);
        let throughput_stats = self.calculate_throughput_statistics(&results.throughput_timeline);
        let error_analysis = self.analyze_errors(&results.errors);
        
        Ok(Analysis {
            latency_statistics: latency_stats,
            throughput_statistics: throughput_stats,
            error_analysis,
            bottlenecks: self.identify_bottlenecks(results),
            recommendations: self.generate_recommendations(results),
            executive_summary: self.create_executive_summary(results),
        })
    }
}

// Report template
const REPORT_TEMPLATE: &str = r#"
<!DOCTYPE html>
<html>
<head>
    <title>Load Test Report - {{ timestamp }}</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; }
        .summary { background: #f0f0f0; padding: 20px; border-radius: 5px; }
        .metric { display: inline-block; margin: 10px 20px; }
        .chart { margin: 20px 0; }
        .recommendations { background: #fffacd; padding: 15px; }
    </style>
</head>
<body>
    <h1>Load Test Report</h1>
    
    <div class="summary">
        <h2>Executive Summary</h2>
        <p>{{ analysis.executive_summary }}</p>
        
        <div class="metrics">
            <div class="metric">
                <strong>Total Requests:</strong> {{ results.total_requests }}
            </div>
            <div class="metric">
                <strong>Success Rate:</strong> {{ results.success_rate }}%
            </div>
            <div class="metric">
                <strong>Avg Latency:</strong> {{ results.average_latency_ms }}ms
            </div>
        </div>
    </div>
    
    <div class="charts">
        <h2>Performance Metrics</h2>
        <div class="chart">{{ latency_plot | safe }}</div>
        <div class="chart">{{ throughput_plot | safe }}</div>
        <div class="chart">{{ error_plot | safe }}</div>
    </div>
    
    <div class="recommendations">
        <h2>Recommendations</h2>
        <ul>
        {% for rec in analysis.recommendations %}
            <li>{{ rec }}</li>
        {% endfor %}
        </ul>
    </div>
</body>
</html>
"#;
```

**Validation**:
- Reports generate correctly
- Visualizations render properly
- Analysis accurate
- Multiple formats work

---

### Task 11.4.10: Create Load Test CI/CD Integration
**Objective**: Integrate load testing into continuous integration pipeline.

**Requirements**:
- Automated test execution
- Performance gates
- Result archiving
- Trend tracking

**Implementation**:
```yaml
# Create .github/workflows/load-test.yml
name: Load Testing

on:
  pull_request:
    types: [opened, synchronize]
  schedule:
    - cron: '0 2 * * *' # Nightly load tests

jobs:
  load-test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Setup Rust
      uses: actions-rs/toolchain@v1
      with:
        profile: minimal
        toolchain: stable
    
    - name: Build Load Test Binary
      run: cargo build --release --bin load-test
    
    - name: Start Application
      run: |
        docker-compose up -d
        ./scripts/wait-for-healthy.sh
    
    - name: Run Load Tests
      run: |
        ./target/release/load-test \
          --config load-test-configs/ci-standard.yaml \
          --output results/
    
    - name: Analyze Results
      id: analyze
      run: |
        ./scripts/analyze-load-test.sh results/latest.json
        echo "::set-output name=passed::$?"
    
    - name: Upload Results
      uses: actions/upload-artifact@v3
      with:
        name: load-test-results
        path: results/
    
    - name: Comment PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const results = JSON.parse(
            fs.readFileSync('results/summary.json', 'utf8')
          );
          
          const comment = `## Load Test Results
          
          **Success Rate:** ${results.success_rate}%
          **Avg Latency:** ${results.avg_latency_ms}ms
          **P99 Latency:** ${results.p99_latency_ms}ms
          **Throughput:** ${results.throughput_rps} RPS
          
          ${results.passed ? '✅ All performance criteria met' : '❌ Performance regression detected'}
          
          [Full Report](${results.report_url})`;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });
    
    - name: Store Baseline
      if: github.ref == 'refs/heads/main' && success()
      run: |
        ./scripts/store-baseline.sh results/latest.json
```

**Validation**:
- CI pipeline runs successfully
- Results properly analyzed
- Comments posted to PRs
- Baselines stored correctly

## Dependencies
- tokio = "1.35"
- futures = "0.3"
- rand = "0.8"
- statrs = "0.16"
- plotters = "0.3"
- tera = "1.19"

## Success Metrics
- ✅ Generate 10K+ RPS sustained load
- ✅ < 5% performance overhead
- ✅ Detect regressions > 10%
- ✅ 100% CI pipeline reliability
- ✅ Reports generated in < 30s