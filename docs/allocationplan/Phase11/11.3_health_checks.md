# 11.3 Health Checks - Micro Tasks

## Overview
Implement comprehensive health checking system for the neuromorphic knowledge graph with automatic recovery, dependency checks, and detailed diagnostics.

## Micro Tasks

### Task 11.3.1: Create Core Health Check Framework
**Objective**: Build extensible health check system with multiple check types.

**Requirements**:
- Liveness checks (process alive)
- Readiness checks (ready to serve)
- Startup probes (initialization)
- Dependency health aggregation

**Implementation**:
```rust
// Create src/health/mod.rs
use async_trait::async_trait;
use serde::{Serialize, Deserialize};
use std::time::Duration;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum HealthStatus {
    Healthy,
    Degraded(String),
    Unhealthy(String),
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HealthCheckResult {
    pub name: String,
    pub status: HealthStatus,
    pub latency_ms: u64,
    pub details: serde_json::Value,
}

#[async_trait]
pub trait HealthCheck: Send + Sync {
    async fn check(&self) -> Result<HealthCheckResult>;
    fn name(&self) -> &str;
    fn timeout(&self) -> Duration {
        Duration::from_secs(5)
    }
}

pub struct HealthCheckRegistry {
    checks: Vec<Box<dyn HealthCheck>>,
}

impl HealthCheckRegistry {
    pub async fn run_all(&self) -> Vec<HealthCheckResult> {
        let mut results = Vec::new();
        
        for check in &self.checks {
            let start = std::time::Instant::now();
            let timeout = check.timeout();
            
            match tokio::time::timeout(timeout, check.check()).await {
                Ok(Ok(mut result)) => {
                    result.latency_ms = start.elapsed().as_millis() as u64;
                    results.push(result);
                }
                Ok(Err(e)) => {
                    results.push(HealthCheckResult {
                        name: check.name().to_string(),
                        status: HealthStatus::Unhealthy(e.to_string()),
                        latency_ms: start.elapsed().as_millis() as u64,
                        details: serde_json::json!({}),
                    });
                }
                Err(_) => {
                    results.push(HealthCheckResult {
                        name: check.name().to_string(),
                        status: HealthStatus::Unhealthy("Timeout".to_string()),
                        latency_ms: timeout.as_millis() as u64,
                        details: serde_json::json!({}),
                    });
                }
            }
        }
        
        results
    }
}
```

**Validation**:
- Framework compiles and tests pass
- Timeout handling works correctly
- Results serialize to JSON
- Concurrent execution safe

---

### Task 11.3.2: Implement Allocation Engine Health Check
**Objective**: Monitor allocation engine performance and resource usage.

**Requirements**:
- Check allocation latency
- Verify column availability
- Monitor memory pool usage
- Track thread pool health

**Implementation**:
```rust
// Create src/health/allocation_check.rs
use super::*;
use crate::allocation::AllocationEngine;

pub struct AllocationHealthCheck {
    engine: Arc<AllocationEngine>,
}

#[async_trait]
impl HealthCheck for AllocationHealthCheck {
    async fn check(&self) -> Result<HealthCheckResult> {
        let start = Instant::now();
        
        // Test allocation
        let test_input = vec![0.1, 0.2, 0.3];
        let allocation_result = self.engine.allocate(&test_input).await?;
        let allocation_time = start.elapsed();
        
        // Check metrics
        let stats = self.engine.get_stats().await?;
        let available_columns = stats.available_columns;
        let total_columns = stats.total_columns;
        let utilization = 1.0 - (available_columns as f64 / total_columns as f64);
        
        let status = if allocation_time.as_millis() > 100 {
            HealthStatus::Degraded("High allocation latency".to_string())
        } else if utilization > 0.9 {
            HealthStatus::Degraded("High column utilization".to_string())
        } else {
            HealthStatus::Healthy
        };
        
        Ok(HealthCheckResult {
            name: self.name().to_string(),
            status,
            latency_ms: 0, // Set by framework
            details: serde_json::json!({
                "allocation_latency_ms": allocation_time.as_millis(),
                "available_columns": available_columns,
                "total_columns": total_columns,
                "utilization_percent": utilization * 100.0,
                "allocations_per_second": stats.allocations_per_second,
            }),
        })
    }
    
    fn name(&self) -> &str {
        "allocation_engine"
    }
}
```

**Validation**:
- Health check runs successfully
- Metrics are accurate
- Degraded states trigger correctly
- No memory leaks

---

### Task 11.3.3: Add Neural Network Health Monitoring
**Objective**: Verify neural network components are functioning correctly.

**Requirements**:
- Test inference latency
- Verify model loading
- Check weight integrity
- Monitor SIMD operations

**Implementation**:
```rust
// Create src/health/neural_network_check.rs
use super::*;
use crate::neural::{NeuralNetwork, ModelType};

pub struct NeuralNetworkHealthCheck {
    networks: HashMap<ModelType, Arc<NeuralNetwork>>,
}

#[async_trait]
impl HealthCheck for NeuralNetworkHealthCheck {
    async fn check(&self) -> Result<HealthCheckResult> {
        let mut network_statuses = HashMap::new();
        
        for (model_type, network) in &self.networks {
            let test_input = vec![0.5; network.input_size()];
            let start = Instant::now();
            
            match network.inference(&test_input) {
                Ok(output) => {
                    let inference_time = start.elapsed();
                    
                    // Verify output sanity
                    let output_valid = output.iter()
                        .all(|&x| x.is_finite() && x >= 0.0 && x <= 1.0);
                    
                    network_statuses.insert(
                        model_type.to_string(),
                        serde_json::json!({
                            "status": if output_valid { "healthy" } else { "degraded" },
                            "inference_ms": inference_time.as_micros() as f64 / 1000.0,
                            "output_size": output.len(),
                            "weights_loaded": network.weights_loaded(),
                        })
                    );
                }
                Err(e) => {
                    network_statuses.insert(
                        model_type.to_string(),
                        serde_json::json!({
                            "status": "unhealthy",
                            "error": e.to_string(),
                        })
                    );
                }
            }
        }
        
        let all_healthy = network_statuses.values()
            .all(|v| v["status"] == "healthy");
        
        Ok(HealthCheckResult {
            name: self.name().to_string(),
            status: if all_healthy {
                HealthStatus::Healthy
            } else {
                HealthStatus::Degraded("Some networks unhealthy".to_string())
            },
            latency_ms: 0,
            details: serde_json::json!({
                "networks": network_statuses,
                "total_networks": self.networks.len(),
            }),
        })
    }
    
    fn name(&self) -> &str {
        "neural_networks"
    }
}
```

**Validation**:
- All network types tested
- Inference times reasonable
- Output validation works
- Error handling correct

---

### Task 11.3.4: Create Database Connection Health Check
**Objective**: Monitor database connectivity and performance.

**Requirements**:
- Connection pool health
- Query latency check
- Replication lag monitoring
- Connection count limits

**Implementation**:
```rust
// Create src/health/database_check.rs
use super::*;
use sqlx::{postgres::PgPool, Row};

pub struct DatabaseHealthCheck {
    pool: PgPool,
    max_connections: u32,
}

#[async_trait]
impl HealthCheck for DatabaseHealthCheck {
    async fn check(&self) -> Result<HealthCheckResult> {
        // Test basic connectivity
        let start = Instant::now();
        let row = sqlx::query("SELECT 1 as health_check")
            .fetch_one(&self.pool)
            .await?;
        let query_time = start.elapsed();
        
        // Check connection pool stats
        let pool_options = self.pool.options();
        let pool_size = self.pool.size();
        let idle_connections = self.pool.num_idle();
        
        // Check replication lag if applicable
        let lag_result = sqlx::query(
            "SELECT EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp()))::int as lag_seconds"
        )
        .fetch_optional(&self.pool)
        .await?;
        
        let replication_lag = lag_result
            .and_then(|row| row.try_get::<i32, _>("lag_seconds").ok());
        
        // Determine health status
        let status = if query_time.as_millis() > 100 {
            HealthStatus::Degraded("High query latency".to_string())
        } else if pool_size > self.max_connections * 9 / 10 {
            HealthStatus::Degraded("Connection pool near limit".to_string())
        } else if let Some(lag) = replication_lag {
            if lag > 10 {
                HealthStatus::Degraded(format!("High replication lag: {}s", lag))
            } else {
                HealthStatus::Healthy
            }
        } else {
            HealthStatus::Healthy
        };
        
        Ok(HealthCheckResult {
            name: self.name().to_string(),
            status,
            latency_ms: 0,
            details: serde_json::json!({
                "query_latency_ms": query_time.as_millis(),
                "pool_size": pool_size,
                "idle_connections": idle_connections,
                "max_connections": self.max_connections,
                "replication_lag_seconds": replication_lag,
            }),
        })
    }
    
    fn name(&self) -> &str {
        "database"
    }
}
```

**Validation**:
- Database queries succeed
- Pool metrics accurate
- Replication lag detected
- Connection limits respected

---

### Task 11.3.5: Implement Memory Health Check
**Objective**: Monitor system memory usage and allocation patterns.

**Requirements**:
- Heap usage tracking
- Allocation rate monitoring
- Memory leak detection
- GC pressure indicators

**Implementation**:
```rust
// Create src/health/memory_check.rs
use super::*;
use tikv_jemalloc_ctl::{stats, epoch};

pub struct MemoryHealthCheck {
    max_memory_bytes: usize,
    allocation_rate_threshold: f64,
}

#[async_trait]
impl HealthCheck for MemoryHealthCheck {
    async fn check(&self) -> Result<HealthCheckResult> {
        // Update jemalloc epoch
        epoch::advance()?;
        
        // Get memory stats
        let allocated = stats::allocated::read()?;
        let active = stats::active::read()?;
        let resident = stats::resident::read()?;
        let retained = stats::retained::read()?;
        
        // Calculate fragmentation
        let fragmentation = if allocated > 0 {
            (active as f64 - allocated as f64) / allocated as f64
        } else {
            0.0
        };
        
        // Get allocation rate (requires previous sample)
        let allocation_rate = self.calculate_allocation_rate(allocated).await;
        
        // Determine health status
        let status = if allocated > self.max_memory_bytes {
            HealthStatus::Unhealthy("Memory limit exceeded".to_string())
        } else if allocated > self.max_memory_bytes * 9 / 10 {
            HealthStatus::Degraded("High memory usage".to_string())
        } else if fragmentation > 0.5 {
            HealthStatus::Degraded("High memory fragmentation".to_string())
        } else if allocation_rate > self.allocation_rate_threshold {
            HealthStatus::Degraded("High allocation rate".to_string())
        } else {
            HealthStatus::Healthy
        };
        
        Ok(HealthCheckResult {
            name: self.name().to_string(),
            status,
            latency_ms: 0,
            details: serde_json::json!({
                "allocated_mb": allocated / 1024 / 1024,
                "active_mb": active / 1024 / 1024,
                "resident_mb": resident / 1024 / 1024,
                "retained_mb": retained / 1024 / 1024,
                "fragmentation_percent": fragmentation * 100.0,
                "allocation_rate_mb_per_sec": allocation_rate / 1024.0 / 1024.0,
                "max_memory_mb": self.max_memory_bytes / 1024 / 1024,
            }),
        })
    }
    
    fn name(&self) -> &str {
        "memory"
    }
}
```

**Validation**:
- Memory stats accurate
- Fragmentation calculated correctly
- Allocation rate tracked
- Thresholds trigger appropriately

---

### Task 11.3.6: Add Disk I/O Health Check
**Objective**: Monitor disk performance and available space.

**Requirements**:
- Disk space monitoring
- I/O latency tracking
- Write throughput check
- Queue depth monitoring

**Implementation**:
```rust
// Create src/health/disk_check.rs
use super::*;
use sysinfo::{System, SystemExt, DiskExt};
use std::fs;

pub struct DiskHealthCheck {
    data_dir: PathBuf,
    min_free_space_gb: u64,
}

#[async_trait]
impl HealthCheck for DiskHealthCheck {
    async fn check(&self) -> Result<HealthCheckResult> {
        let mut system = System::new_all();
        system.refresh_disks();
        
        // Find disk containing data directory
        let disk = system.disks()
            .iter()
            .find(|disk| self.data_dir.starts_with(disk.mount_point()))
            .ok_or_else(|| anyhow!("Data directory disk not found"))?;
        
        let total_space = disk.total_space();
        let available_space = disk.available_space();
        let used_percent = ((total_space - available_space) as f64 / total_space as f64) * 100.0;
        
        // Test write performance
        let test_file = self.data_dir.join(".health_check_test");
        let test_data = vec![0u8; 1024 * 1024]; // 1MB
        let start = Instant::now();
        fs::write(&test_file, &test_data)?;
        fs::remove_file(&test_file)?;
        let write_time = start.elapsed();
        
        let write_throughput_mb_s = 1.0 / write_time.as_secs_f64();
        
        // Determine health status
        let available_gb = available_space / 1024 / 1024 / 1024;
        let status = if available_gb < self.min_free_space_gb {
            HealthStatus::Unhealthy("Insufficient disk space".to_string())
        } else if used_percent > 90.0 {
            HealthStatus::Degraded("High disk usage".to_string())
        } else if write_throughput_mb_s < 10.0 {
            HealthStatus::Degraded("Low disk write performance".to_string())
        } else {
            HealthStatus::Healthy
        };
        
        Ok(HealthCheckResult {
            name: self.name().to_string(),
            status,
            latency_ms: 0,
            details: serde_json::json!({
                "total_gb": total_space / 1024 / 1024 / 1024,
                "available_gb": available_gb,
                "used_percent": used_percent,
                "write_throughput_mb_s": write_throughput_mb_s,
                "mount_point": disk.mount_point().to_string_lossy(),
                "file_system": format!("{:?}", disk.file_system()),
            }),
        })
    }
    
    fn name(&self) -> &str {
        "disk"
    }
}
```

**Validation**:
- Disk stats accurate
- Write test completes
- Space calculations correct
- Performance thresholds work

---

### Task 11.3.7: Create HTTP Endpoint Health Checks
**Objective**: Expose health check results via HTTP endpoints.

**Requirements**:
- /health/live endpoint
- /health/ready endpoint
- /health/startup endpoint
- Detailed JSON responses

**Implementation**:
```rust
// Create src/health/http.rs
use axum::{
    Router,
    Json,
    extract::State,
    http::StatusCode,
    routing::get,
};

pub struct HealthHttpService {
    registry: Arc<HealthCheckRegistry>,
}

impl HealthHttpService {
    pub fn router(&self) -> Router {
        Router::new()
            .route("/health/live", get(Self::liveness_handler))
            .route("/health/ready", get(Self::readiness_handler))
            .route("/health/startup", get(Self::startup_handler))
            .route("/health", get(Self::detailed_health_handler))
            .with_state(Arc::clone(&self.registry))
    }
    
    async fn liveness_handler() -> StatusCode {
        // Basic liveness - process is running
        StatusCode::OK
    }
    
    async fn readiness_handler(
        State(registry): State<Arc<HealthCheckRegistry>>
    ) -> (StatusCode, Json<serde_json::Value>) {
        let results = registry.run_all().await;
        
        let all_healthy = results.iter()
            .all(|r| matches!(r.status, HealthStatus::Healthy | HealthStatus::Degraded(_)));
        
        let status_code = if all_healthy {
            StatusCode::OK
        } else {
            StatusCode::SERVICE_UNAVAILABLE
        };
        
        let body = serde_json::json!({
            "status": if all_healthy { "ready" } else { "not_ready" },
            "checks": results.into_iter()
                .map(|r| (r.name.clone(), r.status))
                .collect::<HashMap<_, _>>(),
        });
        
        (status_code, Json(body))
    }
    
    async fn detailed_health_handler(
        State(registry): State<Arc<HealthCheckRegistry>>
    ) -> Json<serde_json::Value> {
        let results = registry.run_all().await;
        
        let overall_status = if results.iter().all(|r| matches!(r.status, HealthStatus::Healthy)) {
            "healthy"
        } else if results.iter().any(|r| matches!(r.status, HealthStatus::Unhealthy(_))) {
            "unhealthy"
        } else {
            "degraded"
        };
        
        Json(serde_json::json!({
            "status": overall_status,
            "timestamp": chrono::Utc::now().to_rfc3339(),
            "checks": results,
        }))
    }
}
```

**Validation**:
- Endpoints return correct codes
- JSON responses valid
- Concurrent requests handled
- Response times < 100ms

---

### Task 11.3.8: Implement Circuit Breaker Pattern
**Objective**: Prevent cascading failures with circuit breaker protection.

**Requirements**:
- Failure threshold detection
- Automatic circuit opening
- Half-open state testing
- Recovery monitoring

**Implementation**:
```rust
// Create src/health/circuit_breaker.rs
use std::sync::atomic::{AtomicU64, AtomicBool, Ordering};

#[derive(Debug, Clone, Copy, PartialEq)]
pub enum CircuitState {
    Closed,
    Open,
    HalfOpen,
}

pub struct CircuitBreaker {
    failure_threshold: u64,
    success_threshold: u64,
    timeout: Duration,
    
    failure_count: AtomicU64,
    success_count: AtomicU64,
    last_failure_time: AtomicU64,
    state: Arc<RwLock<CircuitState>>,
}

impl CircuitBreaker {
    pub async fn call<F, T, E>(&self, f: F) -> Result<T, E>
    where
        F: Future<Output = Result<T, E>>,
        E: From<CircuitBreakerError>,
    {
        let state = self.get_state().await;
        
        match state {
            CircuitState::Open => {
                Err(CircuitBreakerError::OpenCircuit.into())
            }
            CircuitState::Closed | CircuitState::HalfOpen => {
                match f.await {
                    Ok(result) => {
                        self.on_success().await;
                        Ok(result)
                    }
                    Err(error) => {
                        self.on_failure().await;
                        Err(error)
                    }
                }
            }
        }
    }
    
    async fn get_state(&self) -> CircuitState {
        let state = *self.state.read().await;
        
        if state == CircuitState::Open {
            let last_failure = self.last_failure_time.load(Ordering::Relaxed);
            let now = SystemTime::now()
                .duration_since(UNIX_EPOCH)
                .unwrap()
                .as_secs();
            
            if now - last_failure > self.timeout.as_secs() {
                let mut state_guard = self.state.write().await;
                *state_guard = CircuitState::HalfOpen;
                self.failure_count.store(0, Ordering::Relaxed);
                self.success_count.store(0, Ordering::Relaxed);
                CircuitState::HalfOpen
            } else {
                CircuitState::Open
            }
        } else {
            state
        }
    }
    
    async fn on_success(&self) {
        self.success_count.fetch_add(1, Ordering::Relaxed);
        let success_count = self.success_count.load(Ordering::Relaxed);
        
        let mut state = self.state.write().await;
        if *state == CircuitState::HalfOpen && success_count >= self.success_threshold {
            *state = CircuitState::Closed;
            self.failure_count.store(0, Ordering::Relaxed);
        }
    }
    
    async fn on_failure(&self) {
        self.failure_count.fetch_add(1, Ordering::Relaxed);
        let failure_count = self.failure_count.load(Ordering::Relaxed);
        
        self.last_failure_time.store(
            SystemTime::now()
                .duration_since(UNIX_EPOCH)
                .unwrap()
                .as_secs(),
            Ordering::Relaxed
        );
        
        let mut state = self.state.write().await;
        if failure_count >= self.failure_threshold {
            *state = CircuitState::Open;
        }
    }
}
```

**Validation**:
- Circuit opens on threshold
- Half-open state works
- Recovery succeeds
- Thread-safe operation

---

### Task 11.3.9: Add Deadlock Detection
**Objective**: Detect and recover from potential deadlock conditions.

**Requirements**:
- Thread state monitoring
- Lock timeout detection
- Automatic recovery attempts
- Deadlock alerting

**Implementation**:
```rust
// Create src/health/deadlock_detector.rs
use parking_lot::deadlock;
use std::thread;

pub struct DeadlockDetector {
    check_interval: Duration,
}

impl DeadlockDetector {
    pub fn start(self) -> JoinHandle<()> {
        thread::spawn(move || {
            loop {
                thread::sleep(self.check_interval);
                
                let deadlocks = deadlock::check_deadlock();
                if !deadlocks.is_empty() {
                    error!("Deadlock detected!");
                    
                    for (i, threads) in deadlocks.iter().enumerate() {
                        error!("Deadlock #{}", i);
                        for thread in threads {
                            error!(
                                "Thread {:?}: {:?}",
                                thread.thread_id(),
                                thread.backtrace()
                            );
                        }
                    }
                    
                    // Record metric
                    metrics::counter!("deadlocks_detected").increment(1);
                    
                    // Attempt recovery (application-specific)
                    self.attempt_recovery();
                }
            }
        })
    }
    
    fn attempt_recovery(&self) {
        // Application-specific recovery logic
        // e.g., restart affected subsystems
        warn!("Attempting deadlock recovery...");
        
        // For critical deadlocks, might need to panic
        // to trigger process restart
        if self.is_critical_deadlock() {
            panic!("Critical deadlock detected, restarting process");
        }
    }
}

// Integrate with health check
pub struct DeadlockHealthCheck {
    last_check: Arc<RwLock<Instant>>,
}

#[async_trait]
impl HealthCheck for DeadlockHealthCheck {
    async fn check(&self) -> Result<HealthCheckResult> {
        let last_check = *self.last_check.read().await;
        let time_since_check = Instant::now().duration_since(last_check);
        
        let status = if time_since_check > Duration::from_secs(60) {
            HealthStatus::Unhealthy("Deadlock detector not running".to_string())
        } else {
            HealthStatus::Healthy
        };
        
        Ok(HealthCheckResult {
            name: self.name().to_string(),
            status,
            latency_ms: 0,
            details: serde_json::json!({
                "last_check_seconds_ago": time_since_check.as_secs(),
                "detector_active": time_since_check < Duration::from_secs(10),
            }),
        })
    }
    
    fn name(&self) -> &str {
        "deadlock_detector"
    }
}
```

**Validation**:
- Deadlocks detected correctly
- Recovery attempts logged
- Metrics recorded
- No false positives

---

### Task 11.3.10: Create Health Check Dashboard
**Objective**: Visualize system health in real-time dashboard.

**Requirements**:
- Real-time health status
- Historical health trends
- Dependency graph view
- Alert integration

**Implementation**:
```yaml
# Create monitoring/dashboards/health-checks.json
{
  "dashboard": {
    "title": "System Health Checks",
    "panels": [
      {
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0},
        "title": "Overall Health Status",
        "targets": [{
          "expr": "sum by (check) (health_check_status)"
        }],
        "type": "stat",
        "options": {
          "colorMode": "background",
          "graphMode": "none",
          "orientation": "auto"
        }
      },
      {
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0},
        "title": "Health Check Latencies",
        "targets": [{
          "expr": "histogram_quantile(0.99, health_check_duration_bucket)"
        }],
        "type": "graph",
        "yaxes": [{
          "format": "ms",
          "label": "Latency"
        }]
      },
      {
        "gridPos": {"h": 8, "w": 24, "x": 0, "y": 8},
        "title": "Health Check History",
        "targets": [{
          "expr": "health_check_status"
        }],
        "type": "heatmap",
        "dataFormat": "timeseries"
      }
    ],
    "refresh": "5s",
    "time": {
      "from": "now-1h",
      "to": "now"
    }
  }
}

# Create health check alert rules
groups:
  - name: health_checks
    interval: 30s
    rules:
      - alert: SystemUnhealthy
        expr: health_check_status{status="unhealthy"} > 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "System health check failing"
          description: "{{ $labels.check }} is unhealthy"
      
      - alert: SystemDegraded
        expr: health_check_status{status="degraded"} > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "System health degraded"
          description: "{{ $labels.check }} is degraded: {{ $labels.reason }}"
```

**Validation**:
- Dashboard loads correctly
- Real-time updates work
- Alerts fire appropriately
- Historical data retained

## Dependencies
- tokio = "1.35"
- async-trait = "0.1"
- serde = "1.0"
- sysinfo = "0.30"
- parking_lot = "0.12"
- tikv-jemalloc-ctl = "0.5"

## Success Metrics
- ✅ < 100ms health check latency
- ✅ 100% deadlock detection rate
- ✅ Zero false positive alerts
- ✅ Automatic recovery success > 90%
- ✅ Health endpoint availability > 99.99%