# 11.1 Monitoring Infrastructure - Micro Tasks

## Overview
Implement comprehensive monitoring for the neuromorphic knowledge graph system with real-time metrics, alerting, and observability.

## Micro Tasks

### Task 11.1.1: Create Core Metrics Registry
**Objective**: Implement a centralized metrics registry for all system components.

**Requirements**:
- Use `metrics` crate for Rust metrics collection
- Create metric types: counters, gauges, histograms
- Thread-safe metric registration
- Lazy static initialization

**Implementation**:
```rust
// Create src/monitoring/metrics.rs
use metrics::{Counter, Gauge, Histogram, Key, Recorder};
use once_cell::sync::Lazy;

pub struct SystemMetrics {
    pub allocations_total: Counter,
    pub active_columns: Gauge,
    pub allocation_duration: Histogram,
    pub memory_usage_bytes: Gauge,
}
```

**Validation**:
- Unit tests for metric creation
- Thread safety tests
- Performance impact < 1% overhead

---

### Task 11.1.2: Implement Allocation Metrics
**Objective**: Track detailed metrics for the allocation engine.

**Requirements**:
- Track allocation requests, successes, failures
- Measure allocation latency distribution
- Monitor column utilization rates
- Track winner-take-all convergence times

**Implementation**:
```rust
// Extend src/monitoring/allocation_metrics.rs
impl AllocationMetrics {
    pub fn record_allocation(&self, duration: Duration, success: bool) {
        self.total_allocations.increment(1);
        if success {
            self.successful_allocations.increment(1);
        }
        self.allocation_histogram.record(duration.as_micros() as f64);
    }
}
```

**Validation**:
- Metrics accuracy within 0.1%
- No memory leaks under continuous recording
- Histogram percentiles correct

---

### Task 11.1.3a: Create Prometheus Metrics Exporter
**Objective**: Set up Prometheus metrics recording and formatting.

**Requirements**:
- Prometheus text format export
- Metric labels and metadata
- Proper metric registration
- Thread-safe metrics recording

**Implementation**:
```rust
// Create src/monitoring/prometheus_exporter.rs
use metrics_exporter_prometheus::{PrometheusBuilder, PrometheusHandle};
use metrics::{counter, gauge, histogram};

pub struct PrometheusExporter {
    handle: PrometheusHandle,
}

impl PrometheusExporter {
    pub fn new() -> Result<Self> {
        let builder = PrometheusBuilder::new();
        let handle = builder.install_recorder()?;
        
        Ok(Self { handle })
    }
    
    pub fn render_metrics(&self) -> String {
        self.handle.render()
    }
    
    pub fn record_metric(&self, name: &str, value: f64, labels: &[(&str, &str)]) {
        match name {
            name if name.ends_with("_total") => {
                counter!(name, labels.iter().cloned().collect()).increment(value as u64);
            }
            name if name.ends_with("_seconds") => {
                histogram!(name, labels.iter().cloned().collect()).record(value);
            }
            _ => {
                gauge!(name, labels.iter().cloned().collect()).set(value);
            }
        }
    }
}
```

**Validation**:
- Metrics exported in Prometheus format
- Labels correctly applied
- Metric types properly categorized
- Thread-safe operation verified

---

### Task 11.1.3b: Create HTTP Metrics Endpoint Server
**Objective**: Implement HTTP server to expose metrics endpoint.

**Requirements**:
- HTTP endpoint on configurable port
- GET /metrics route
- Concurrent request handling
- Graceful shutdown

**Implementation**:
```rust
// Create src/monitoring/metrics_server.rs
use axum::{Router, routing::get, response::Response, http::StatusCode};
use std::sync::Arc;
use tokio::signal;

pub struct MetricsServer {
    port: u16,
    exporter: Arc<PrometheusExporter>,
}

impl MetricsServer {
    pub fn new(port: u16, exporter: Arc<PrometheusExporter>) -> Self {
        Self { port, exporter }
    }
    
    pub async fn start(&self) -> Result<()> {
        let app = self.create_router();
        
        let listener = tokio::net::TcpListener::bind(
            format!("0.0.0.0:{}", self.port)
        ).await?;
        
        info!("Metrics server starting on port {}", self.port);
        
        axum::serve(listener, app)
            .with_graceful_shutdown(shutdown_signal())
            .await?;
        
        Ok(())
    }
    
    fn create_router(&self) -> Router {
        let exporter = Arc::clone(&self.exporter);
        
        Router::new()
            .route("/metrics", get(move || {
                let exporter = exporter.clone();
                async move {
                    match exporter.render_metrics() {
                        metrics => {
                            Response::builder()
                                .status(StatusCode::OK)
                                .header("Content-Type", "text/plain; version=0.0.4")
                                .body(metrics)
                                .unwrap()
                        }
                    }
                }
            }))
            .route("/health", get(|| async { "OK" }))
    }
}

async fn shutdown_signal() {
    let ctrl_c = async {
        signal::ctrl_c()
            .await
            .expect("failed to install Ctrl+C handler");
    };

    #[cfg(unix)]
    let terminate = async {
        signal::unix::signal(signal::unix::SignalKind::terminate())
            .expect("failed to install signal handler")
            .recv()
            .await;
    };

    #[cfg(not(unix))]
    let terminate = std::future::pending::<()>();

    tokio::select! {
        _ = ctrl_c => {},
        _ = terminate => {},
    }

    info!("Shutdown signal received, starting graceful shutdown");
}
```

**Validation**:
- HTTP server starts on configured port
- /metrics endpoint returns Prometheus format
- Concurrent requests handled correctly
- Graceful shutdown works properly

---

### Task 11.1.4: Add OpenTelemetry Tracing
**Objective**: Implement distributed tracing for request flow analysis.

**Requirements**:
- Trace spans for major operations
- Context propagation
- Sampling configuration
- Export to Jaeger/OTLP

**Implementation**:
```rust
// Create src/monitoring/tracing.rs
use opentelemetry::{global, sdk::trace as sdktrace};
use tracing_subscriber::{layer::SubscriberExt, Registry};

pub fn init_tracing(service_name: &str) -> Result<()> {
    let tracer = opentelemetry_otlp::new_pipeline()
        .tracing()
        .with_exporter(opentelemetry_otlp::new_exporter().tonic())
        .with_trace_config(sdktrace::config().with_resource(
            Resource::new(vec![KeyValue::new("service.name", service_name)])
        ))
        .install_batch(opentelemetry::runtime::Tokio)?;
}
```

**Validation**:
- Traces visible in Jaeger UI
- Parent-child span relationships correct
- Sampling rate configurable

---

### Task 11.1.5: Implement Custom Dashboards
**Objective**: Create Grafana dashboard definitions for system monitoring.

**Requirements**:
- JSON dashboard templates
- Key system metrics visualization
- Alert thresholds configuration
- Auto-provisioning support

**Implementation**:
```json
// Create monitoring/dashboards/neuromorphic-overview.json
{
  "dashboard": {
    "title": "Neuromorphic Knowledge Graph Overview",
    "panels": [
      {
        "title": "Allocation Rate",
        "targets": [
          {
            "expr": "rate(allocations_total[5m])"
          }
        ]
      }
    ]
  }
}
```

**Validation**:
- Dashboards load in Grafana
- Queries return data
- Visualizations render correctly

---

### Task 11.1.6: Add Resource Usage Monitoring
**Objective**: Monitor system resource consumption (CPU, memory, disk).

**Requirements**:
- Process-level metrics
- Thread pool utilization
- Memory allocation tracking
- Disk I/O metrics

**Implementation**:
```rust
// Create src/monitoring/resources.rs
use sysinfo::{System, SystemExt, ProcessExt};

pub struct ResourceMonitor {
    system: System,
}

impl ResourceMonitor {
    pub fn update_metrics(&mut self) {
        self.system.refresh_all();
        let process = self.system.process(std::process::id());
        
        if let Some(proc) = process {
            metrics::gauge!("process_cpu_usage_percent")
                .set(proc.cpu_usage() as f64);
            metrics::gauge!("process_memory_bytes")
                .set(proc.memory() as f64 * 1024);
        }
    }
}
```

**Validation**:
- Resource metrics accurate within 5%
- Updates don't impact performance
- Cross-platform compatibility

---

### Task 11.1.7: Create Alerting Rules
**Objective**: Define Prometheus alerting rules for critical conditions.

**Requirements**:
- High latency alerts (p99 > 100ms)
- Error rate alerts (> 1%)
- Resource exhaustion alerts
- Deadlock detection alerts

**Implementation**:
```yaml
# Create monitoring/alerts/production.yml
groups:
  - name: neuromorphic_alerts
    rules:
      - alert: HighAllocationLatency
        expr: histogram_quantile(0.99, allocation_duration_bucket) > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High allocation latency detected"
```

**Validation**:
- Alerts fire on test conditions
- No false positives in 24h test
- Alert routing works correctly

---

### Task 11.1.8: Implement Logging Integration
**Objective**: Structured logging with correlation to metrics and traces.

**Requirements**:
- JSON structured logs
- Log levels and filtering
- Correlation IDs with traces
- Log aggregation support

**Implementation**:
```rust
// Update src/monitoring/logging.rs
use tracing_subscriber::fmt::format::FmtSpan;
use serde_json::json;

pub fn init_logging() {
    let subscriber = tracing_subscriber::fmt()
        .with_target(false)
        .with_span_events(FmtSpan::CLOSE)
        .json()
        .with_current_span(true)
        .init();
}
```

**Validation**:
- Logs parse correctly in ELK/Loki
- Trace IDs present in logs
- Performance impact < 2%

---

### Task 11.1.9: Add Performance Profiling Hooks
**Objective**: Enable runtime performance profiling for optimization.

**Requirements**:
- CPU profiling with pprof
- Memory profiling support
- Async runtime metrics
- Profile export endpoints

**Implementation**:
```rust
// Create src/monitoring/profiling.rs
use pprof::{ProfilerGuard, ProfilerGuardBuilder};

pub struct ProfilingService {
    guard: Option<ProfilerGuard<'static>>,
}

impl ProfilingService {
    pub fn start_profiling(&mut self, frequency: i32) -> Result<()> {
        self.guard = Some(
            ProfilerGuardBuilder::default()
                .frequency(frequency)
                .blocklist(&["libc", "libgcc", "pthread"])
                .build()?
        );
        Ok(())
    }
}
```

**Validation**:
- Profiles viewable in pprof
- Overhead < 5% when active
- Accurate flame graphs

---

### Task 11.1.10: Create Monitoring Tests
**Objective**: Comprehensive test suite for monitoring components.

**Requirements**:
- Unit tests for all metrics
- Integration tests with Prometheus
- Load tests for metric overhead
- End-to-end monitoring validation

**Implementation**:
```rust
// Create tests/monitoring_integration.rs
#[tokio::test]
async fn test_metrics_export() {
    let metrics_port = start_test_metrics_server().await;
    
    // Generate test metrics
    metrics::counter!("test_counter").increment(42);
    
    // Scrape metrics endpoint
    let response = reqwest::get(&format!("http://localhost:{}/metrics", metrics_port))
        .await.unwrap();
    
    let body = response.text().await.unwrap();
    assert!(body.contains("test_counter 42"));
}
```

**Validation**:
- All tests pass consistently
- Coverage > 90%
- No flaky tests

## Dependencies
- metrics = "0.21"
- metrics-exporter-prometheus = "0.12"
- opentelemetry = "0.20"
- tracing = "0.1"
- axum = "0.6"
- sysinfo = "0.29"

## Success Metrics
- ✅ < 2% performance overhead
- ✅ All metrics exported successfully
- ✅ Traces correlate with logs
- ✅ Dashboards load in < 2s
- ✅ Zero metric data loss