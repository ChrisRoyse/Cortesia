# Micro-Phase 0.7.3: Implement Automated Benchmark Suite

## Objective
Create automated benchmark execution with CLI tools, reporting, and continuous performance monitoring.

## Prerequisites
- GitHub Actions CI/CD setup (0.7.1 complete)
- Test coverage and quality gates (0.7.2 complete)
- Benchmark framework implemented (0.6.1-0.6.3 complete)

## Input
- Benchmark framework from cortex-benchmarks crate
- CI/CD workflow requirements
- Performance monitoring needs

## Task Details

### Step 1: Create Benchmark CLI Tools
Build command-line tools for benchmark execution, comparison, and reporting.

### Step 2: Implement Benchmark Runners
Create automated benchmark runners for different test scenarios.

### Step 3: Add Reporting Infrastructure
Build comprehensive reporting with JSON, HTML, and markdown output.

### Step 4: Create Performance Monitoring
Implement continuous performance monitoring and alerting.

## Expected Output
- Benchmark CLI tools (benchmark, compare-baseline, etc.)
- Automated benchmark runners
- Multi-format reporting system
- Performance monitoring dashboard

## Verification Steps
1. Run benchmark CLI tools locally
2. Verify baseline comparison works
3. Check report generation
4. Confirm performance monitoring

## Time Estimate
50-55 minutes

## AI Execution Prompt
```
Implement automated benchmark suite with CLI tools and reporting.

1. Create crates/cortex-benchmarks/src/bin/benchmark.rs:

```rust
//! Main benchmark execution CLI tool

use clap::Parser;
use cortex_benchmarks::prelude::*;
use cortex_benchmarks::runners::BenchmarkRunner;
use serde_json;
use std::fs;
use std::path::PathBuf;
use std::time::Instant;

#[derive(Parser)]
#[command(name = "benchmark")]
#[command(about = "Run CortexKG performance benchmarks")]
struct Args {
    /// Type of benchmark to run
    #[arg(long, value_enum)]
    benchmark_type: BenchmarkType,
    
    /// Number of iterations
    #[arg(long, default_value = "1000")]
    iterations: usize,
    
    /// Number of warmup iterations
    #[arg(long, default_value = "100")]
    warmup_iterations: usize,
    
    /// Data size for benchmark
    #[arg(long, default_value = "100")]
    data_size: usize,
    
    /// Number of concurrent workers
    #[arg(long, default_value = "1")]
    concurrency: usize,
    
    /// Duration for stress tests (seconds)
    #[arg(long)]
    duration: Option<u64>,
    
    /// Output file for results
    #[arg(long)]
    output: Option<PathBuf>,
    
    /// Save detailed measurements
    #[arg(long)]
    save_details: bool,
    
    /// Verbose output
    #[arg(short, long)]
    verbose: bool,
}

fn main() -> Result<(), Box<dyn std::error::Error>> {
    let args = Args::parse();
    
    if args.verbose {
        env_logger::init();
    }
    
    println!("üöÄ Starting {} benchmark...", args.benchmark_type.description());
    println!("Configuration:");
    println!("  Iterations: {}", args.iterations);
    println!("  Warmup: {}", args.warmup_iterations);
    println!("  Data size: {}", args.data_size);
    println!("  Concurrency: {}", args.concurrency);
    if let Some(duration) = args.duration {
        println!("  Duration: {}s", duration);
    }
    println!();
    
    let config = BenchmarkConfig {
        benchmark_type: args.benchmark_type.clone(),
        iterations: args.iterations,
        warmup_iterations: args.warmup_iterations,
        data_size: args.data_size,
        concurrency: args.concurrency,
        duration: args.duration.map(std::time::Duration::from_secs),
        save_details: args.save_details,
        output_dir: args.output.as_ref().and_then(|p| p.parent()).map(|p| p.to_string_lossy().to_string()),
        ..Default::default()
    };
    
    let start_time = Instant::now();
    
    // Run benchmark
    let runner = BenchmarkRunner::new();
    let result = runner.run_benchmark(config)?;
    
    let total_time = start_time.elapsed();
    
    // Print results
    print_results(&result, total_time);
    
    // Save results if requested
    if let Some(output_path) = args.output {
        save_results(&result, &output_path)?;
        println!("‚úÖ Results saved to {}", output_path.display());
    }
    
    // Check if benchmark passed
    if result.passed {
        println!("üéâ Benchmark PASSED all performance thresholds!");
        Ok(())
    } else {
        println!("‚ö†Ô∏è  Benchmark FAILED some performance thresholds:");
        for failure in &result.failures {
            println!("  - {}", failure);
        }
        std::process::exit(1);
    }
}

fn print_results(result: &BenchmarkResult, total_time: std::time::Duration) {
    println!("üìä Benchmark Results:");
    println!("  Type: {}", result.config.benchmark_type.description());
    println!("  Status: {}", if result.passed { "‚úÖ PASSED" } else { "‚ùå FAILED" });
    println!("  Total time: {:.2}s", total_time.as_secs_f64());
    println!();
    
    println!("Performance Metrics:");
    println!("  Average duration: {:.3}ms", result.metrics.avg_duration.as_secs_f64() * 1000.0);
    println!("  Median duration: {:.3}ms", result.metrics.median_duration.as_secs_f64() * 1000.0);
    println!("  95th percentile: {:.3}ms", result.metrics.p95_duration.as_secs_f64() * 1000.0);
    println!("  99th percentile: {:.3}ms", result.metrics.p99_duration.as_secs_f64() * 1000.0);
    println!("  Throughput: {:.2} ops/sec", result.metrics.throughput);
    println!("  Memory usage: {:.2} MB", result.metrics.memory_usage_mb);
    println!("  Peak memory: {:.2} MB", result.metrics.peak_memory_mb);
    println!("  Error rate: {:.2}%", result.metrics.error_rate * 100.0);
    println!("  Successful ops: {}", result.metrics.successful_ops);
    println!("  Failed ops: {}", result.metrics.failed_ops);
    
    if !result.metrics.custom_metrics.is_empty() {
        println!("Custom Metrics:");
        for (key, value) in &result.metrics.custom_metrics {
            println!("  {}: {:.3}", key, value);
        }
    }
}

fn save_results(result: &BenchmarkResult, path: &PathBuf) -> Result<(), Box<dyn std::error::Error>> {
    if let Some(parent) = path.parent() {
        fs::create_dir_all(parent)?;
    }
    
    let json = serde_json::to_string_pretty(result)?;
    fs::write(path, json)?;
    
    Ok(())
}
```

2. Create crates/cortex-benchmarks/src/bin/compare-baseline.rs:

```rust
//! Compare benchmark results with baseline

use clap::Parser;
use cortex_benchmarks::prelude::*;
use cortex_benchmarks::baseline::{BaselineManager, RegressionSeverity};
use serde_json;
use std::fs;
use std::path::PathBuf;

#[derive(Parser)]
#[command(name = "compare-baseline")]
#[command(about = "Compare benchmark results with baseline")]
struct Args {
    /// Directory containing benchmark results
    #[arg(long)]
    results_dir: PathBuf,
    
    /// Baseline file to compare against
    #[arg(long)]
    baseline_file: PathBuf,
    
    /// Output file for comparison report
    #[arg(long)]
    output: PathBuf,
    
    /// Fail if any regressions detected
    #[arg(long)]
    fail_on_regression: bool,
    
    /// Maximum allowed performance decrease (%)
    #[arg(long, default_value = "10.0")]
    max_regression: f64,
    
    /// Verbose output
    #[arg(short, long)]
    verbose: bool,
}

fn main() -> Result<(), Box<dyn std::error::Error>> {
    let args = Args::parse();
    
    if args.verbose {
        env_logger::init();
    }
    
    println!("üìä Comparing benchmark results with baseline...");
    
    // Load baseline
    let mut baseline_manager = BaselineManager::with_storage(&args.baseline_file);
    baseline_manager.load_baselines()?;
    
    // Load benchmark results
    let results = load_results_from_dir(&args.results_dir)?;
    
    if results.is_empty() {
        eprintln!("‚ùå No benchmark results found in {}", args.results_dir.display());
        std::process::exit(1);
    }
    
    println!("Found {} benchmark result(s)", results.len());
    
    // Compare with baselines
    let mut comparisons = Vec::new();
    let mut has_regressions = false;
    let mut max_severity = RegressionSeverity::None;
    
    for result in &results {
        if let Some(analysis) = baseline_manager.check_regression(
            &result.config.benchmark_type, 
            &result.metrics
        ) {
            if analysis.has_regression {
                has_regressions = true;
                if analysis.severity > max_severity {
                    max_severity = analysis.severity;
                }
            }
            
            print_analysis(&result.config.benchmark_type, &analysis);
            comparisons.push((result.config.benchmark_type.clone(), analysis));
        } else {
            println!("‚ö†Ô∏è  No baseline found for {}", result.config.benchmark_type.description());
        }
    }
    
    // Generate comparison report
    let report = ComparisonReport {
        timestamp: chrono::Utc::now(),
        baseline_file: args.baseline_file.to_string_lossy().to_string(),
        results_count: results.len(),
        has_regressions,
        max_severity: format!("{:?}", max_severity),
        comparisons: comparisons.into_iter().collect(),
    };
    
    // Save report
    let json = serde_json::to_string_pretty(&report)?;
    fs::write(&args.output, json)?;
    
    println!("üìÑ Comparison report saved to {}", args.output.display());
    
    // Summary
    if has_regressions {
        println!("‚ö†Ô∏è  Performance regressions detected (severity: {:?})", max_severity);
        if args.fail_on_regression {
            std::process::exit(1);
        }
    } else {
        println!("‚úÖ No performance regressions detected");
    }
    
    Ok(())
}

fn load_results_from_dir(dir: &PathBuf) -> Result<Vec<BenchmarkResult>, Box<dyn std::error::Error>> {
    let mut results = Vec::new();
    
    for entry in fs::read_dir(dir)? {
        let entry = entry?;
        let path = entry.path();
        
        if path.extension().and_then(|s| s.to_str()) == Some("json") {
            let content = fs::read_to_string(&path)?;
            let result: BenchmarkResult = serde_json::from_str(&content)?;
            results.push(result);
        }
    }
    
    Ok(results)
}

fn print_analysis(benchmark_type: &BenchmarkType, analysis: &cortex_benchmarks::baseline::RegressionAnalysis) {
    println!("üîç Analysis for {}:", benchmark_type.description());
    
    if analysis.has_regression {
        println!("  Status: ‚ùå REGRESSION DETECTED");
        println!("  Severity: {:?}", analysis.severity);
        println!("  Issues:");
        
        for issue in &analysis.issues {
            println!("    - {}: {:.2}% change (threshold: {:.2}%)", 
                issue.metric, issue.change_percent, issue.threshold);
        }
    } else {
        println!("  Status: ‚úÖ NO REGRESSION");
    }
    
    println!("  Performance changes:");
    println!("    Throughput: {:.2}%", analysis.comparison.throughput_change_percent);
    println!("    Duration: {:.2}%", analysis.comparison.duration_change_percent);
    println!("    Memory: {:.2}%", analysis.comparison.memory_change_percent);
    println!("    Error rate: {:.4}", analysis.comparison.error_rate_delta);
    println!();
}

#[derive(serde::Serialize)]
struct ComparisonReport {
    timestamp: chrono::DateTime<chrono::Utc>,
    baseline_file: String,
    results_count: usize,
    has_regressions: bool,
    max_severity: String,
    comparisons: std::collections::HashMap<BenchmarkType, cortex_benchmarks::baseline::RegressionAnalysis>,
}
```

3. Create crates/cortex-benchmarks/src/bin/update-baseline.rs:

```rust
//! Update performance baseline from benchmark results

use clap::Parser;
use cortex_benchmarks::prelude::*;
use cortex_benchmarks::baseline::BaselineManager;
use serde_json;
use std::fs;
use std::path::PathBuf;

#[derive(Parser)]
#[command(name = "update-baseline")]
#[command(about = "Update performance baseline from results")]
struct Args {
    /// Directory containing benchmark results
    #[arg(long)]
    results_dir: PathBuf,
    
    /// Baseline file to update
    #[arg(long)]
    baseline_file: PathBuf,
    
    /// Git commit hash
    #[arg(long)]
    git_commit: Option<String>,
    
    /// Notes for baseline update
    #[arg(long)]
    notes: Option<String>,
    
    /// Force update even if not an improvement
    #[arg(long)]
    force: bool,
    
    /// Verbose output
    #[arg(short, long)]
    verbose: bool,
}

fn main() -> Result<(), Box<dyn std::error::Error>> {
    let args = Args::parse();
    
    if args.verbose {
        env_logger::init();
    }
    
    println!("üìä Updating performance baseline...");
    
    // Load baseline manager
    let mut baseline_manager = BaselineManager::with_storage(&args.baseline_file);
    baseline_manager.load_baselines().unwrap_or_else(|_| {
        println!("üìù Creating new baseline file");
    });
    
    // Load benchmark results
    let results = load_results_from_dir(&args.results_dir)?;
    
    if results.is_empty() {
        eprintln!("‚ùå No benchmark results found in {}", args.results_dir.display());
        std::process::exit(1);
    }
    
    println!("Found {} benchmark result(s)", results.len());
    
    let mut updated_count = 0;
    
    for result in &results {
        let benchmark_type = result.config.benchmark_type.clone();
        
        if args.force {
            // Force update
            baseline_manager.set_baseline(
                benchmark_type.clone(),
                result.metrics.clone(),
                args.git_commit.clone(),
                args.notes.clone(),
            )?;
            
            println!("üîÑ Force updated baseline for {}", benchmark_type.description());
            updated_count += 1;
        } else {
            // Only update if improvement
            let was_updated = baseline_manager.maybe_update_baseline(
                benchmark_type.clone(),
                result.metrics.clone(),
                args.git_commit.clone(),
            )?;
            
            if was_updated {
                println!("‚úÖ Updated baseline for {} (improvement detected)", benchmark_type.description());
                updated_count += 1;
            } else {
                println!("‚è≠Ô∏è  No update for {} (no improvement)", benchmark_type.description());
            }
        }
    }
    
    if updated_count > 0 {
        println!("üìÅ Baseline file updated with {} benchmark(s)", updated_count);
        
        // Generate summary report
        let report = baseline_manager.generate_report();
        println!("üìä Baseline Summary:");
        println!("  Total baselines: {}", report.total_baselines);
        for entry in &report.entries {
            println!("  {}: v{}, {:.2} ops/sec, {:.2}ms avg",
                entry.benchmark_type.description(),
                entry.version,
                entry.throughput,
                entry.avg_duration_ms
            );
        }
    } else {
        println!("‚ÑπÔ∏è  No baselines updated");
    }
    
    Ok(())
}

fn load_results_from_dir(dir: &PathBuf) -> Result<Vec<BenchmarkResult>, Box<dyn std::error::Error>> {
    let mut results = Vec::new();
    
    for entry in fs::read_dir(dir)? {
        let entry = entry?;
        let path = entry.path();
        
        if path.extension().and_then(|s| s.to_str()) == Some("json") {
            let content = fs::read_to_string(&path)?;
            let result: BenchmarkResult = serde_json::from_str(&content)?;
            results.push(result);
        }
    }
    
    Ok(results)
}
```

4. Create crates/cortex-benchmarks/src/bin/check-regressions.rs:

```rust
//! Check for performance regressions in comparison report

use clap::Parser;
use cortex_benchmarks::baseline::RegressionSeverity;
use serde_json;
use std::fs;
use std::path::PathBuf;

#[derive(Parser)]
#[command(name = "check-regressions")]
#[command(about = "Check for performance regressions")]
struct Args {
    /// Comparison report file
    #[arg(long)]
    comparison_report: PathBuf,
    
    /// Fail on any regression
    #[arg(long)]
    fail_on_regression: bool,
    
    /// Minimum severity to fail on
    #[arg(long, default_value = "minor")]
    min_severity: String,
    
    /// Verbose output
    #[arg(short, long)]
    verbose: bool,
}

fn main() -> Result<(), Box<dyn std::error::Error>> {
    let args = Args::parse();
    
    if args.verbose {
        env_logger::init();
    }
    
    // Load comparison report
    let content = fs::read_to_string(&args.comparison_report)?;
    let report: serde_json::Value = serde_json::from_str(&content)?;
    
    let has_regressions = report["has_regressions"].as_bool().unwrap_or(false);
    let max_severity = report["max_severity"].as_str().unwrap_or("None");
    
    println!("üîç Regression Check Results:");
    println!("  Report: {}", args.comparison_report.display());
    println!("  Has regressions: {}", if has_regressions { "‚ùå YES" } else { "‚úÖ NO" });
    println!("  Max severity: {}", max_severity);
    
    if has_regressions {
        let comparisons = report["comparisons"].as_object().unwrap_or(&serde_json::Map::new());
        
        println!("üìã Regression Details:");
        for (benchmark_type, analysis) in comparisons {
            let analysis_obj = analysis.as_object().unwrap();
            let has_regression = analysis_obj["has_regression"].as_bool().unwrap_or(false);
            
            if has_regression {
                let severity = analysis_obj["severity"].as_str().unwrap_or("Unknown");
                let issues = analysis_obj["issues"].as_array().unwrap_or(&vec![]);
                
                println!("  üî∏ {}: {} ({})", benchmark_type, severity, issues.len());
                
                for issue in issues {
                    let issue_obj = issue.as_object().unwrap();
                    let metric = issue_obj["metric"].as_str().unwrap_or("unknown");
                    let change = issue_obj["change_percent"].as_f64().unwrap_or(0.0);
                    
                    println!("    - {}: {:.2}% change", metric, change);
                }
            }
        }
    }
    
    // Determine if we should fail
    let should_fail = if args.fail_on_regression {
        has_regressions
    } else {
        has_regressions && severity_exceeds_threshold(max_severity, &args.min_severity)
    };
    
    if should_fail {
        println!("üí• Failing due to performance regressions");
        std::process::exit(1);
    } else {
        println!("‚úÖ Regression check passed");
    }
    
    Ok(())
}

fn severity_exceeds_threshold(actual: &str, threshold: &str) -> bool {
    let severity_value = |s: &str| match s.to_lowercase().as_str() {
        "none" => 0,
        "minor" => 1,
        "major" => 2,
        "critical" => 3,
        _ => 0,
    };
    
    severity_value(actual) >= severity_value(threshold)
}
```

5. Create crates/cortex-benchmarks/src/bin/format-report.rs:

```rust
//! Format benchmark reports in various formats

use clap::Parser;
use serde_json;
use std::fs;
use std::path::PathBuf;

#[derive(Parser)]
#[command(name = "format-report")]
#[command(about = "Format benchmark reports")]
struct Args {
    /// Input report file
    #[arg(long)]
    input: PathBuf,
    
    /// Output format
    #[arg(long, value_enum, default_value = "markdown")]
    format: OutputFormat,
    
    /// Output file (stdout if not specified)
    #[arg(long)]
    output: Option<PathBuf>,
}

#[derive(clap::ValueEnum, Clone)]
enum OutputFormat {
    Markdown,
    Html,
    Json,
    Csv,
}

fn main() -> Result<(), Box<dyn std::error::Error>> {
    let args = Args::parse();
    
    // Load report
    let content = fs::read_to_string(&args.input)?;
    let report: serde_json::Value = serde_json::from_str(&content)?;
    
    // Format output
    let formatted = match args.format {
        OutputFormat::Markdown => format_markdown(&report),
        OutputFormat::Html => format_html(&report),
        OutputFormat::Json => serde_json::to_string_pretty(&report)?,
        OutputFormat::Csv => format_csv(&report)?,
    };
    
    // Write output
    if let Some(output_path) = args.output {
        fs::write(output_path, formatted)?;
    } else {
        println!("{}", formatted);
    }
    
    Ok(())
}

fn format_markdown(report: &serde_json::Value) -> String {
    let mut output = String::new();
    
    output.push_str("# Benchmark Comparison Report\n\n");
    
    if let Some(timestamp) = report["timestamp"].as_str() {
        output.push_str(&format!("**Generated:** {}\n\n", timestamp));
    }
    
    let has_regressions = report["has_regressions"].as_bool().unwrap_or(false);
    let status_emoji = if has_regressions { "‚ùå" } else { "‚úÖ" };
    let status_text = if has_regressions { "REGRESSIONS DETECTED" } else { "NO REGRESSIONS" };
    
    output.push_str(&format!("**Status:** {} {}\n\n", status_emoji, status_text));
    
    if let Some(comparisons) = report["comparisons"].as_object() {
        output.push_str("## Benchmark Results\n\n");
        output.push_str("| Benchmark | Status | Throughput Change | Duration Change | Memory Change |\n");
        output.push_str("|-----------|--------|-------------------|-----------------|---------------|\n");
        
        for (benchmark_type, analysis) in comparisons {
            let analysis_obj = analysis.as_object().unwrap();
            let has_regression = analysis_obj["has_regression"].as_bool().unwrap_or(false);
            let status = if has_regression { "‚ùå REGRESSION" } else { "‚úÖ OK" };
            
            let comparison = &analysis_obj["comparison"];
            let throughput_change = comparison["throughput_change_percent"].as_f64().unwrap_or(0.0);
            let duration_change = comparison["duration_change_percent"].as_f64().unwrap_or(0.0);
            let memory_change = comparison["memory_change_percent"].as_f64().unwrap_or(0.0);
            
            output.push_str(&format!(
                "| {} | {} | {:+.2}% | {:+.2}% | {:+.2}% |\n",
                benchmark_type, status, throughput_change, duration_change, memory_change
            ));
        }
    }
    
    output
}

fn format_html(report: &serde_json::Value) -> String {
    let mut output = String::new();
    
    output.push_str("<!DOCTYPE html>\n<html>\n<head>\n");
    output.push_str("<title>Benchmark Comparison Report</title>\n");
    output.push_str("<style>\n");
    output.push_str("body { font-family: Arial, sans-serif; margin: 40px; }\n");
    output.push_str("table { border-collapse: collapse; width: 100%; }\n");
    output.push_str("th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }\n");
    output.push_str("th { background-color: #f2f2f2; }\n");
    output.push_str(".status-ok { color: green; }\n");
    output.push_str(".status-regression { color: red; }\n");
    output.push_str("</style>\n</head>\n<body>\n");
    
    output.push_str("<h1>Benchmark Comparison Report</h1>\n");
    
    if let Some(timestamp) = report["timestamp"].as_str() {
        output.push_str(&format!("<p><strong>Generated:</strong> {}</p>\n", timestamp));
    }
    
    let has_regressions = report["has_regressions"].as_bool().unwrap_or(false);
    let status_class = if has_regressions { "status-regression" } else { "status-ok" };
    let status_text = if has_regressions { "REGRESSIONS DETECTED" } else { "NO REGRESSIONS" };
    
    output.push_str(&format!("<p><strong>Status:</strong> <span class=\"{}\">{}</span></p>\n", status_class, status_text));
    
    if let Some(comparisons) = report["comparisons"].as_object() {
        output.push_str("<h2>Benchmark Results</h2>\n");
        output.push_str("<table>\n");
        output.push_str("<tr><th>Benchmark</th><th>Status</th><th>Throughput Change</th><th>Duration Change</th><th>Memory Change</th></tr>\n");
        
        for (benchmark_type, analysis) in comparisons {
            let analysis_obj = analysis.as_object().unwrap();
            let has_regression = analysis_obj["has_regression"].as_bool().unwrap_or(false);
            let status = if has_regression { 
                "<span class=\"status-regression\">REGRESSION</span>" 
            } else { 
                "<span class=\"status-ok\">OK</span>" 
            };
            
            let comparison = &analysis_obj["comparison"];
            let throughput_change = comparison["throughput_change_percent"].as_f64().unwrap_or(0.0);
            let duration_change = comparison["duration_change_percent"].as_f64().unwrap_or(0.0);
            let memory_change = comparison["memory_change_percent"].as_f64().unwrap_or(0.0);
            
            output.push_str(&format!(
                "<tr><td>{}</td><td>{}</td><td>{:+.2}%</td><td>{:+.2}%</td><td>{:+.2}%</td></tr>\n",
                benchmark_type, status, throughput_change, duration_change, memory_change
            ));
        }
        
        output.push_str("</table>\n");
    }
    
    output.push_str("</body>\n</html>\n");
    output
}

fn format_csv(report: &serde_json::Value) -> Result<String, Box<dyn std::error::Error>> {
    let mut output = String::new();
    
    output.push_str("Benchmark,Status,Throughput Change (%),Duration Change (%),Memory Change (%),Error Rate Delta\n");
    
    if let Some(comparisons) = report["comparisons"].as_object() {
        for (benchmark_type, analysis) in comparisons {
            let analysis_obj = analysis.as_object().unwrap();
            let has_regression = analysis_obj["has_regression"].as_bool().unwrap_or(false);
            let status = if has_regression { "REGRESSION" } else { "OK" };
            
            let comparison = &analysis_obj["comparison"];
            let throughput_change = comparison["throughput_change_percent"].as_f64().unwrap_or(0.0);
            let duration_change = comparison["duration_change_percent"].as_f64().unwrap_or(0.0);
            let memory_change = comparison["memory_change_percent"].as_f64().unwrap_or(0.0);
            let error_rate_delta = comparison["error_rate_delta"].as_f64().unwrap_or(0.0);
            
            output.push_str(&format!(
                "{},{},{:.2},{:.2},{:.2},{:.4}\n",
                benchmark_type, status, throughput_change, duration_change, memory_change, error_rate_delta
            ));
        }
    }
    
    Ok(output)
}
```

6. Update crates/cortex-benchmarks/Cargo.toml to add binary dependencies:

```toml
# Add to dependencies section:
clap = { version = "4.4", features = ["derive"] }
env_logger = "0.10"
chrono = { workspace = true, features = ["serde"] }

# Add binary definitions:
[[bin]]
name = "benchmark"
path = "src/bin/benchmark.rs"

[[bin]]
name = "compare-baseline"
path = "src/bin/compare-baseline.rs"

[[bin]]
name = "update-baseline"
path = "src/bin/update-baseline.rs"

[[bin]]
name = "check-regressions"
path = "src/bin/check-regressions.rs"

[[bin]]
name = "format-report"
path = "src/bin/format-report.rs"
```

7. Create crates/cortex-benchmarks/src/runners.rs:

```rust
//! Benchmark execution runners

use crate::types::*;
use crate::metrics::MetricsCollector;
use std::time::Instant;
use std::sync::Arc;
use std::thread;

/// Main benchmark runner
pub struct BenchmarkRunner {
    // Configuration and state
}

impl BenchmarkRunner {
    /// Create new benchmark runner
    pub fn new() -> Self {
        Self {}
    }
    
    /// Run a single benchmark
    pub fn run_benchmark(&self, config: BenchmarkConfig) -> Result<BenchmarkResult, BenchmarkError> {
        let start_time = Instant::now();
        
        println!("üî• Running {} warmup iterations...", config.warmup_iterations);
        
        // Warmup phase
        for _ in 0..config.warmup_iterations {
            self.execute_benchmark_iteration(&config)?;
        }
        
        println!("üìä Running {} measurement iterations...", config.iterations);
        
        // Measurement phase
        let metrics_collector = MetricsCollector::new(config.clone());
        metrics_collector.start_collection();
        
        for i in 0..config.iterations {
            if i % (config.iterations / 10).max(1) == 0 {
                println!("  Progress: {}/{}", i, config.iterations);
            }
            
            let handle = metrics_collector.start_operation();
            match self.execute_benchmark_iteration(&config) {
                Ok(_) => handle.complete(),
                Err(_) => handle.fail(),
            }
        }
        
        let metrics = metrics_collector.finalize();
        
        // Check against thresholds
        let threshold = config.benchmark_type.target_threshold();
        let mut failures = Vec::new();
        let mut passed = true;
        
        if let Some(max_duration) = threshold.max_duration {
            if metrics.avg_duration > max_duration {
                failures.push(format!("Average duration {:.3}ms exceeds maximum {:.3}ms", 
                    metrics.avg_duration.as_secs_f64() * 1000.0,
                    max_duration.as_secs_f64() * 1000.0));
                passed = false;
            }
        }
        
        if let Some(min_throughput) = threshold.min_throughput {
            if metrics.throughput < min_throughput {
                failures.push(format!("Throughput {:.2} ops/sec below minimum {:.2} ops/sec",
                    metrics.throughput, min_throughput));
                passed = false;
            }
        }
        
        if let Some(max_memory) = threshold.max_memory_mb {
            if metrics.peak_memory_mb > max_memory {
                failures.push(format!("Peak memory {:.2} MB exceeds maximum {:.2} MB",
                    metrics.peak_memory_mb, max_memory));
                passed = false;
            }
        }
        
        if let Some(max_error_rate) = threshold.max_error_rate {
            if metrics.error_rate > max_error_rate {
                failures.push(format!("Error rate {:.4} exceeds maximum {:.4}",
                    metrics.error_rate, max_error_rate));
                passed = false;
            }
        }
        
        Ok(BenchmarkResult {
            config,
            metrics,
            passed,
            failures,
            timestamp: chrono::Utc::now(),
            metadata: std::collections::HashMap::new(),
        })
    }
    
    /// Execute a single benchmark iteration
    fn execute_benchmark_iteration(&self, config: &BenchmarkConfig) -> Result<(), BenchmarkError> {
        match config.benchmark_type {
            BenchmarkType::TTFSEncoding => self.run_ttfs_encoding(config),
            BenchmarkType::ColumnAllocation => self.run_column_allocation(config),
            BenchmarkType::LateralInhibition => self.run_lateral_inhibition(config),
            BenchmarkType::MemoryConsolidation => self.run_memory_consolidation(config),
            BenchmarkType::SIMDOperations => self.run_simd_operations(config),
            BenchmarkType::FullPipeline => self.run_full_pipeline(config),
            BenchmarkType::StressTest => self.run_stress_test(config),
            BenchmarkType::MemoryBenchmark => self.run_memory_benchmark(config),
            BenchmarkType::ConcurrencyTest => self.run_concurrency_test(config),
            BenchmarkType::SimilarityComputation => self.run_similarity_computation(config),
        }
    }
    
    // Individual benchmark implementations (simplified for mock testing)
    
    fn run_ttfs_encoding(&self, config: &BenchmarkConfig) -> Result<(), BenchmarkError> {
        // Simulate TTFS encoding work
        let work_duration = std::time::Duration::from_micros(500);
        thread::sleep(work_duration);
        Ok(())
    }
    
    fn run_column_allocation(&self, config: &BenchmarkConfig) -> Result<(), BenchmarkError> {
        let work_duration = std::time::Duration::from_micros(1000);
        thread::sleep(work_duration);
        Ok(())
    }
    
    fn run_lateral_inhibition(&self, config: &BenchmarkConfig) -> Result<(), BenchmarkError> {
        let work_duration = std::time::Duration::from_micros(750);
        thread::sleep(work_duration);
        Ok(())
    }
    
    fn run_memory_consolidation(&self, config: &BenchmarkConfig) -> Result<(), BenchmarkError> {
        let work_duration = std::time::Duration::from_millis(50);
        thread::sleep(work_duration);
        Ok(())
    }
    
    fn run_simd_operations(&self, config: &BenchmarkConfig) -> Result<(), BenchmarkError> {
        let work_duration = std::time::Duration::from_micros(100);
        thread::sleep(work_duration);
        Ok(())
    }
    
    fn run_full_pipeline(&self, config: &BenchmarkConfig) -> Result<(), BenchmarkError> {
        let work_duration = std::time::Duration::from_millis(5);
        thread::sleep(work_duration);
        Ok(())
    }
    
    fn run_stress_test(&self, config: &BenchmarkConfig) -> Result<(), BenchmarkError> {
        if let Some(duration) = config.duration {
            thread::sleep(duration);
        } else {
            thread::sleep(std::time::Duration::from_millis(100));
        }
        Ok(())
    }
    
    fn run_memory_benchmark(&self, config: &BenchmarkConfig) -> Result<(), BenchmarkError> {
        // Allocate and deallocate memory
        let _data: Vec<u8> = vec![0; config.data_size * 1024];
        thread::sleep(std::time::Duration::from_micros(200));
        Ok(())
    }
    
    fn run_concurrency_test(&self, config: &BenchmarkConfig) -> Result<(), BenchmarkError> {
        let handles: Vec<_> = (0..config.concurrency)
            .map(|_| {
                thread::spawn(|| {
                    thread::sleep(std::time::Duration::from_micros(1000));
                })
            })
            .collect();
        
        for handle in handles {
            handle.join().map_err(|_| BenchmarkError::ConcurrencyError)?;
        }
        
        Ok(())
    }
    
    fn run_similarity_computation(&self, config: &BenchmarkConfig) -> Result<(), BenchmarkError> {
        // Simulate similarity calculations
        let work_duration = std::time::Duration::from_micros(300);
        thread::sleep(work_duration);
        Ok(())
    }
}

/// Benchmark execution suite for running multiple benchmarks
pub struct BenchmarkSuite {
    benchmarks: Vec<BenchmarkConfig>,
}

impl BenchmarkSuite {
    /// Create new benchmark suite
    pub fn new() -> Self {
        Self {
            benchmarks: Vec::new(),
        }
    }
    
    /// Add benchmark to suite
    pub fn add_benchmark(&mut self, config: BenchmarkConfig) {
        self.benchmarks.push(config);
    }
    
    /// Run all benchmarks in suite
    pub fn run_all(&self) -> Result<Vec<BenchmarkResult>, BenchmarkError> {
        let runner = BenchmarkRunner::new();
        let mut results = Vec::new();
        
        for config in &self.benchmarks {
            println!("üöÄ Running {}...", config.benchmark_type.description());
            let result = runner.run_benchmark(config.clone())?;
            results.push(result);
        }
        
        Ok(results)
    }
}

/// Benchmark execution errors
#[derive(Debug, thiserror::Error)]
pub enum BenchmarkError {
    #[error("Benchmark execution failed: {0}")]
    ExecutionFailed(String),
    
    #[error("Concurrency error during benchmark")]
    ConcurrencyError,
    
    #[error("Invalid configuration: {0}")]
    InvalidConfig(String),
    
    #[error("Timeout during benchmark execution")]
    Timeout,
}
```

8. Test the CLI tools:
   cd crates/cortex-benchmarks
   cargo build --release --bins

All automated benchmark tools are now implemented with comprehensive CLI interface and reporting.
```

## Success Criteria
- [ ] Complete CLI tool suite for benchmark execution
- [ ] Automated baseline comparison and updates
- [ ] Multi-format reporting (JSON, Markdown, HTML, CSV)
- [ ] Regression detection and failure handling
- [ ] Performance monitoring and alerting integration