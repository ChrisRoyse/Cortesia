# Micro-Phase 0.7.2: Benchmark Workflow Setup

## Objective
Create automated benchmark execution workflow with baseline comparison, regression detection, and PR integration.

## Prerequisites
- Basic CI workflow setup (0.7.1 complete)
- Benchmark framework implemented (0.6.1-0.6.3 complete)

## Input
- Benchmark framework from cortex-benchmarks crate
- CI/CD workflow foundation
- Performance baseline requirements

## Task Details

### Step 1: Create Benchmark Workflow
Setup automated benchmark execution on main branch and PRs.

### Step 2: Add Baseline Comparison
Implement baseline comparison and regression detection logic.

### Step 3: Create PR Integration
Add automated benchmark result comments to pull requests.

### Step 4: Setup Baseline Storage
Configure baseline file management and updates.

## Expected Output
- `.github/workflows/benchmarks.yml` created
- Baseline comparison automation
- PR comment integration
- Performance regression detection

## Verification Steps
1. Verify benchmark workflow executes
2. Test baseline comparison logic
3. Check PR comment generation
4. Confirm regression detection works

## Time Estimate
25-30 minutes

## AI Execution Prompt
```
Setup automated benchmark workflow with baseline comparison and PR integration.

1. Create .github/workflows/benchmarks.yml:

```yaml
name: Performance Benchmarks

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run benchmarks daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  CARGO_TERM_COLOR: always

jobs:
  benchmarks:
    name: Run Benchmarks
    runs-on: ubuntu-latest
    if: github.event_name != 'pull_request' || contains(github.event.pull_request.labels.*.name, 'benchmark')
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        # Fetch enough history for baseline comparison
        fetch-depth: 100

    - name: Install Rust toolchain
      uses: dtolnay/rust-toolchain@stable

    - name: Configure Rust cache
      uses: Swatinem/rust-cache@v2
      with:
        key: benchmarks

    - name: Install benchmark dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential

    - name: Build benchmark suite
      run: cargo build --release --package cortex-benchmarks

    - name: Create benchmark results directory
      run: mkdir -p benchmark-results

    - name: Run TTFS encoding benchmarks
      run: |
        cargo run --release --package cortex-benchmarks --bin benchmark -- \
          --benchmark-type ttfs-encoding \
          --iterations 1000 \
          --output benchmark-results/ttfs-encoding.json

    - name: Run column allocation benchmarks
      run: |
        cargo run --release --package cortex-benchmarks --bin benchmark -- \
          --benchmark-type column-allocation \
          --iterations 500 \
          --output benchmark-results/column-allocation.json

    - name: Run lateral inhibition benchmarks
      run: |
        cargo run --release --package cortex-benchmarks --bin benchmark -- \
          --benchmark-type lateral-inhibition \
          --iterations 300 \
          --output benchmark-results/lateral-inhibition.json

    - name: Compare with baseline
      run: |
        cargo run --release --package cortex-benchmarks --bin compare-baseline -- \
          --results-dir benchmark-results \
          --baseline-file baselines/main-baseline.json \
          --output comparison-report.json

    - name: Check for regressions
      run: |
        cargo run --release --package cortex-benchmarks --bin check-regressions -- \
          --comparison-report comparison-report.json \
          --fail-on-regression

    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results-${{ github.sha }}
        path: |
          benchmark-results/
          comparison-report.json
        retention-days: 90

    - name: Update baseline on main branch
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: |
        cargo run --release --package cortex-benchmarks --bin update-baseline -- \
          --results-dir benchmark-results \
          --baseline-file baselines/main-baseline.json \
          --git-commit ${{ github.sha }}

    - name: Commit updated baseline
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add baselines/main-baseline.json
        git diff --staged --quiet || git commit -m "Update performance baseline [skip ci]"
        git push

  benchmark-pr-comment:
    name: Benchmark PR Comment
    runs-on: ubuntu-latest
    needs: benchmarks
    if: github.event_name == 'pull_request' && contains(github.event.pull_request.labels.*.name, 'benchmark')
    
    steps:
    - name: Download benchmark results
      uses: actions/download-artifact@v3
      with:
        name: benchmark-results-${{ github.sha }}

    - name: Generate benchmark comment
      run: |
        echo "## ðŸš€ Benchmark Results" > benchmark-comment.md
        echo "" >> benchmark-comment.md
        echo "Performance comparison for commit ${{ github.sha }}:" >> benchmark-comment.md
        echo "" >> benchmark-comment.md
        
        # Add benchmark summary
        if [ -f comparison-report.json ]; then
          echo "### Summary" >> benchmark-comment.md
          cargo run --release --package cortex-benchmarks --bin format-report -- \
            --input comparison-report.json \
            --format markdown >> benchmark-comment.md
        fi
        
        echo "" >> benchmark-comment.md
        echo "Detailed results available in the [benchmark artifacts](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})." >> benchmark-comment.md

    - name: Comment PR
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const comment = fs.readFileSync('benchmark-comment.md', 'utf8');
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });
```

2. Create baselines directory and initial baseline file:
   mkdir -p baselines
   
   Create baselines/main-baseline.json:
   ```json
   {
     "TTFSEncoding": {
       "metrics": {
         "avg_duration": {"secs": 0, "nanos": 1000000},
         "median_duration": {"secs": 0, "nanos": 950000},
         "p95_duration": {"secs": 0, "nanos": 1500000},
         "p99_duration": {"secs": 0, "nanos": 2000000},
         "std_deviation": {"secs": 0, "nanos": 200000},
         "throughput": 1000.0,
         "memory_usage_mb": 8.0,
         "peak_memory_mb": 10.0,
         "error_rate": 0.01,
         "successful_ops": 99,
         "failed_ops": 1,
         "custom_metrics": {}
       },
       "metadata": {
         "created_at": 1700000000,
         "git_commit": null,
         "environment": {},
         "notes": "Initial baseline",
         "system_info": {
           "os": "linux",
           "arch": "x86_64",
           "cpu_cores": 4,
           "total_memory_gb": 16.0,
           "rust_version": "1.70.0"
         }
       },
       "version": 1
     }
   }
   ```

Benchmark workflow is now configured with automated execution and regression detection.
```

## Success Criteria
- [ ] Automated benchmark execution workflow
- [ ] Baseline comparison and updates
- [ ] Performance regression detection
- [ ] PR comment automation working