# Micro-Phase 0.7.2: Setup Test Coverage and Quality Gates

## Objective
Implement comprehensive test coverage monitoring, quality gates, and automated test generation for the CortexKG system.

## Prerequisites
- GitHub Actions CI/CD workflow setup (0.7.1 complete)
- All crates with test suites implemented
- Benchmark framework established

## Input
- Existing test suites across all crates
- Coverage requirements and thresholds
- Quality gate criteria

## Task Details

### Step 1: Configure Coverage Collection
Setup cargo-llvm-cov for comprehensive coverage analysis across all crates.

### Step 2: Implement Quality Gates
Create automated quality checks that must pass before merge.

### Step 3: Add Test Generation
Implement property-based testing and fuzz testing for critical components.

### Step 4: Create Coverage Reports
Generate detailed HTML and JSON coverage reports with trend analysis.

## Expected Output
- Comprehensive test coverage configuration
- Quality gate enforcement
- Property-based and fuzz testing
- Coverage reporting and analysis

## Verification Steps
1. Run coverage analysis locally
2. Verify quality gates block failing code
3. Check property-based tests generate valid cases
4. Confirm coverage reports are accurate

## Time Estimate
40-45 minutes

## AI Execution Prompt
```
Setup comprehensive test coverage and quality gates.

1. Create .cargo/config.toml:

```toml
[build]
rustflags = [
    # Enable more comprehensive warnings
    "-W", "rust_2018_idioms",
    "-W", "unused_lifetimes",
    "-W", "unused_qualifications",
    "-W", "missing_docs",
]

[env]
# Enable debug symbols in tests for better coverage
CARGO_INCREMENTAL = "0"
RUSTFLAGS = "-C instrument-coverage"
LLVM_PROFILE_FILE = "target/coverage/cargo-test-%p-%m.profraw"
```

2. Create scripts/coverage.sh:

```bash
#!/bin/bash
set -euo pipefail

echo "🔍 Running comprehensive test coverage analysis..."

# Clean previous coverage data
rm -rf target/coverage
mkdir -p target/coverage

# Set coverage environment
export CARGO_INCREMENTAL=0
export RUSTFLAGS="-C instrument-coverage"
export LLVM_PROFILE_FILE="target/coverage/cargo-test-%p-%m.profraw"

# Build all crates with coverage instrumentation
echo "📦 Building with coverage instrumentation..."
cargo build --workspace --all-features

# Run all tests
echo "🧪 Running test suite..."
cargo test --workspace --all-features

# Generate coverage report
echo "📊 Generating coverage report..."
cargo llvm-cov --workspace --all-features \
    --html --output-dir target/coverage/html \
    --lcov --output-path target/coverage/lcov.info \
    --json --output-path target/coverage/coverage.json

# Generate summary
echo "📋 Coverage Summary:"
cargo llvm-cov --workspace --all-features --summary-only

# Check minimum coverage threshold
COVERAGE_THRESHOLD=80
ACTUAL_COVERAGE=$(cargo llvm-cov --workspace --all-features --summary-only | grep -oP 'lines.*?\K\d+(?=\.\d+%)' || echo "0")

echo "Coverage: ${ACTUAL_COVERAGE}% (minimum: ${COVERAGE_THRESHOLD}%)"

if [ "$ACTUAL_COVERAGE" -lt "$COVERAGE_THRESHOLD" ]; then
    echo "❌ Coverage below threshold!"
    exit 1
else
    echo "✅ Coverage meets threshold!"
fi

echo "📁 Coverage reports available in target/coverage/"
```

3. Create scripts/quality-check.sh:

```bash
#!/bin/bash
set -euo pipefail

echo "🔍 Running quality checks..."

ERRORS=0

# Format check
echo "📝 Checking code formatting..."
if ! cargo fmt --all -- --check; then
    echo "❌ Code formatting issues found. Run 'cargo fmt' to fix."
    ERRORS=$((ERRORS + 1))
else
    echo "✅ Code formatting OK"
fi

# Clippy check
echo "📎 Running Clippy analysis..."
if ! cargo clippy --workspace --all-targets --all-features -- -D warnings; then
    echo "❌ Clippy warnings found"
    ERRORS=$((ERRORS + 1))
else
    echo "✅ Clippy analysis passed"
fi

# Security audit
echo "🔒 Running security audit..."
if ! cargo audit; then
    echo "❌ Security vulnerabilities found"
    ERRORS=$((ERRORS + 1))
else
    echo "✅ Security audit passed"
fi

# License check
echo "⚖️  Checking license compliance..."
if ! cargo deny check licenses; then
    echo "❌ License compliance issues found"
    ERRORS=$((ERRORS + 1))
else
    echo "✅ License compliance OK"
fi

# Dependency check
echo "📦 Checking dependencies..."
if ! cargo deny check bans; then
    echo "❌ Banned dependencies found"
    ERRORS=$((ERRORS + 1))
else
    echo "✅ Dependencies OK"
fi

# Documentation check
echo "📚 Checking documentation..."
if ! cargo doc --workspace --all-features --no-deps; then
    echo "❌ Documentation generation failed"
    ERRORS=$((ERRORS + 1))
else
    echo "✅ Documentation builds successfully"
fi

# Test documentation
echo "📖 Testing documentation examples..."
if ! cargo test --doc --workspace; then
    echo "❌ Documentation tests failed"
    ERRORS=$((ERRORS + 1))
else
    echo "✅ Documentation tests passed"
fi

if [ $ERRORS -eq 0 ]; then
    echo "🎉 All quality checks passed!"
    exit 0
else
    echo "💥 $ERRORS quality check(s) failed!"
    exit 1
fi
```

4. Create crates/cortex-test-utils/Cargo.toml:

```toml
[package]
name = "cortex-test-utils"
version.workspace = true
authors.workspace = true
edition.workspace = true

[dependencies]
# Property-based testing
proptest = "1.4"
quickcheck = "1.0"
quickcheck_macros = "1.0"

# Fuzzing support
arbitrary = { version = "1.3", features = ["derive"] }

# Test utilities
rstest = "0.18"
serial_test = "3.0"
tempfile = "3.8"
pretty_assertions = "1.4"

# Async testing
tokio-test = "0.4"

# Mock time
mock_instant = "0.3"

# Random data generation
fake = { version = "2.9", features = ["derive", "chrono"] }
rand = "0.8"

# Workspace crates
neuromorphic-core = { path = "../neuromorphic-core" }
ttfs-concepts = { path = "../ttfs-concepts" }
temporal-memory = { path = "../temporal-memory" }
snn-allocation-engine = { path = "../snn-allocation-engine" }
```

5. Create crates/cortex-test-utils/src/lib.rs:

```rust
//! Test utilities and property-based testing for CortexKG
//! 
//! Provides comprehensive testing infrastructure including:
//! - Property-based test generators
//! - Fuzzing support
//! - Mock data generation
//! - Test assertion helpers

pub mod generators;
pub mod assertions;
pub mod mocks;
pub mod properties;
pub mod fuzzing;

pub use generators::*;
pub use assertions::*;
pub use mocks::*;

/// Prelude for convenient imports
pub mod prelude {
    pub use super::generators::*;
    pub use super::assertions::*;
    pub use super::mocks::*;
    pub use proptest::prelude::*;
    pub use quickcheck::{QuickCheck, TestResult};
    pub use rstest::*;
    pub use pretty_assertions::{assert_eq, assert_ne};
}
```

6. Create crates/cortex-test-utils/src/generators.rs:

```rust
//! Property-based test generators for CortexKG types

use proptest::prelude::*;
use neuromorphic_core::ttfs_concept::TTFSConcept;
use neuromorphic_core::spiking_column::{ColumnId, ColumnState};
use std::time::Duration;

/// Generate valid TTFS concepts
pub fn arb_ttfs_concept() -> impl Strategy<Value = TTFSConcept> {
    (
        prop::string::string_regex("[a-zA-Z][a-zA-Z0-9_]{0,63}").unwrap(),
        prop::collection::vec(0.0f32..1.0f32, 32..128),
        prop::option::of(0.0f32..1.0f32),
    ).prop_map(|(name, features, threshold)| {
        let mut concept = TTFSConcept::new(&name);
        concept.set_features(features);
        if let Some(t) = threshold {
            concept.set_activation_threshold(t);
        }
        concept
    })
}

/// Generate valid column IDs
pub fn arb_column_id() -> impl Strategy<Value = ColumnId> {
    1u32..=1000000u32
}

/// Generate column states
pub fn arb_column_state() -> impl Strategy<Value = ColumnState> {
    prop_oneof![
        Just(ColumnState::Available),
        Just(ColumnState::Activated),
        Just(ColumnState::Competing),
        Just(ColumnState::Allocated),
        Just(ColumnState::Inhibited),
    ]
}

/// Generate activation levels
pub fn arb_activation_level() -> impl Strategy<Value = f32> {
    0.0f32..=1.0f32
}

/// Generate durations for timing tests
pub fn arb_duration() -> impl Strategy<Value = Duration> {
    (0u64..1000000u64).prop_map(Duration::from_micros)
}

/// Generate spike timings
pub fn arb_spike_timing() -> impl Strategy<Value = Duration> {
    (1u64..100000u64).prop_map(Duration::from_micros)
}

/// Generate feature vectors
pub fn arb_feature_vector() -> impl Strategy<Value = Vec<f32>> {
    prop::collection::vec(0.0f32..1.0f32, 16..256)
}

/// Generate similarity thresholds
pub fn arb_similarity_threshold() -> impl Strategy<Value = f32> {
    0.1f32..0.99f32
}

/// Generate benchmark configurations
pub fn arb_benchmark_config() -> impl Strategy<Value = (usize, usize, usize)> {
    (
        100usize..10000usize,  // iterations
        10usize..1000usize,    // warmup
        1usize..16usize,       // concurrency
    )
}

/// Generate memory sizes
pub fn arb_memory_size() -> impl Strategy<Value = usize> {
    prop_oneof![
        1024usize..1048576usize,        // 1KB - 1MB
        1048576usize..104857600usize,   // 1MB - 100MB
    ]
}

/// Generate coordinates for spatial tests
pub fn arb_coordinates() -> impl Strategy<Value = (f32, f32, f32)> {
    (
        -100.0f32..100.0f32,
        -100.0f32..100.0f32,
        -100.0f32..100.0f32,
    )
}

/// Generate inhibition weights
pub fn arb_inhibition_weight() -> impl Strategy<Value = f32> {
    0.0f32..1.0f32
}

#[cfg(test)]
mod tests {
    use super::*;

    proptest! {
        #[test]
        fn test_arb_ttfs_concept_valid(concept in arb_ttfs_concept()) {
            assert!(!concept.name().is_empty());
            assert!(!concept.features().is_empty());
            assert!(concept.features().len() >= 32);
            assert!(concept.features().iter().all(|&f| f >= 0.0 && f <= 1.0));
        }

        #[test]
        fn test_arb_column_id_range(id in arb_column_id()) {
            assert!(id >= 1);
            assert!(id <= 1000000);
        }

        #[test]
        fn test_arb_activation_level_range(level in arb_activation_level()) {
            assert!(level >= 0.0);
            assert!(level <= 1.0);
        }

        #[test]
        fn test_arb_duration_valid(duration in arb_duration()) {
            assert!(duration.as_micros() < 1000000);
        }
    }
}
```

7. Create crates/cortex-test-utils/src/assertions.rs:

```rust
//! Custom assertions for CortexKG testing

use neuromorphic_core::ttfs_concept::TTFSConcept;
use neuromorphic_core::spiking_column::{ColumnState, ColumnId};
use std::time::Duration;

/// Assert that two TTFS concepts are similar within threshold
pub fn assert_concepts_similar(a: &TTFSConcept, b: &TTFSConcept, threshold: f32) {
    let similarity = calculate_cosine_similarity(a.features(), b.features());
    assert!(
        similarity >= threshold,
        "Concepts '{}' and '{}' are not similar enough: {:.3} < {:.3}",
        a.name(), b.name(), similarity, threshold
    );
}

/// Assert that a column is in expected state
pub fn assert_column_state(column_id: ColumnId, expected: ColumnState, actual: ColumnState) {
    assert_eq!(
        expected, actual,
        "Column {} expected to be in state {:?}, but was {:?}",
        column_id, expected, actual
    );
}

/// Assert that duration is within expected range
pub fn assert_duration_within(actual: Duration, expected: Duration, tolerance_percent: f32) {
    let tolerance = expected.mul_f32(tolerance_percent / 100.0);
    let min_duration = expected.saturating_sub(tolerance);
    let max_duration = expected + tolerance;
    
    assert!(
        actual >= min_duration && actual <= max_duration,
        "Duration {:?} not within {}% of expected {:?} (range: {:?} - {:?})",
        actual, tolerance_percent, expected, min_duration, max_duration
    );
}

/// Assert that performance metrics meet requirements
pub fn assert_performance_acceptable(
    throughput: f64,
    min_throughput: f64,
    latency: Duration,
    max_latency: Duration,
    error_rate: f64,
    max_error_rate: f64,
) {
    assert!(
        throughput >= min_throughput,
        "Throughput {:.2} ops/sec below minimum {:.2} ops/sec",
        throughput, min_throughput
    );
    
    assert!(
        latency <= max_latency,
        "Latency {:?} exceeds maximum {:?}",
        latency, max_latency
    );
    
    assert!(
        error_rate <= max_error_rate,
        "Error rate {:.4} exceeds maximum {:.4}",
        error_rate, max_error_rate
    );
}

/// Assert that memory usage is within bounds
pub fn assert_memory_usage_acceptable(usage_mb: f64, max_mb: f64) {
    assert!(
        usage_mb <= max_mb,
        "Memory usage {:.2} MB exceeds maximum {:.2} MB",
        usage_mb, max_mb
    );
}

/// Assert that feature vector is normalized
pub fn assert_features_normalized(features: &[f32], tolerance: f32) {
    let magnitude: f32 = features.iter().map(|&x| x * x).sum::<f32>().sqrt();
    assert!(
        (magnitude - 1.0).abs() <= tolerance,
        "Feature vector not normalized: magnitude {:.6}, expected ~1.0 ± {:.6}",
        magnitude, tolerance
    );
}

/// Assert that vectors have valid similarity
pub fn assert_similarity_valid(similarity: f32) {
    assert!(
        similarity >= -1.0 && similarity <= 1.0,
        "Similarity value {:.6} out of valid range [-1.0, 1.0]",
        similarity
    );
}

/// Assert that allocation is consistent
pub fn assert_allocation_consistent(
    allocated_concepts: &[(ColumnId, TTFSConcept)],
    min_similarity_threshold: f32
) {
    for (i, (id_a, concept_a)) in allocated_concepts.iter().enumerate() {
        for (id_b, concept_b) in allocated_concepts.iter().skip(i + 1) {
            let similarity = calculate_cosine_similarity(concept_a.features(), concept_b.features());
            assert!(
                similarity < min_similarity_threshold,
                "Concepts in columns {} and {} are too similar: {:.3} >= {:.3}",
                id_a, id_b, similarity, min_similarity_threshold
            );
        }
    }
}

/// Calculate cosine similarity between feature vectors
fn calculate_cosine_similarity(a: &[f32], b: &[f32]) -> f32 {
    assert_eq!(a.len(), b.len(), "Feature vectors must have same length");
    
    let dot_product: f32 = a.iter().zip(b.iter()).map(|(x, y)| x * y).sum();
    let magnitude_a: f32 = a.iter().map(|x| x * x).sum::<f32>().sqrt();
    let magnitude_b: f32 = b.iter().map(|x| x * x).sum::<f32>().sqrt();
    
    if magnitude_a == 0.0 || magnitude_b == 0.0 {
        0.0
    } else {
        dot_product / (magnitude_a * magnitude_b)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_cosine_similarity() {
        let a = vec![1.0, 0.0, 0.0];
        let b = vec![1.0, 0.0, 0.0];
        assert_eq!(calculate_cosine_similarity(&a, &b), 1.0);
        
        let a = vec![1.0, 0.0];
        let b = vec![0.0, 1.0];
        assert_eq!(calculate_cosine_similarity(&a, &b), 0.0);
    }

    #[test]
    fn test_duration_within_tolerance() {
        let actual = Duration::from_millis(100);
        let expected = Duration::from_millis(95);
        
        // Should pass with 10% tolerance
        assert_duration_within(actual, expected, 10.0);
    }

    #[test]
    #[should_panic]
    fn test_duration_outside_tolerance() {
        let actual = Duration::from_millis(200);
        let expected = Duration::from_millis(100);
        
        // Should fail with 5% tolerance
        assert_duration_within(actual, expected, 5.0);
    }

    #[test]
    fn test_features_normalized() {
        let features = vec![0.6, 0.8]; // magnitude = 1.0
        assert_features_normalized(&features, 0.001);
    }

    #[test]
    #[should_panic]
    fn test_features_not_normalized() {
        let features = vec![1.0, 1.0]; // magnitude = sqrt(2)
        assert_features_normalized(&features, 0.001);
    }
}
```

8. Create deny.toml for dependency checking:

```toml
[licenses]
unlicensed = "deny"
allow = [
    "MIT",
    "Apache-2.0",
    "Apache-2.0 WITH LLVM-exception",
    "BSD-3-Clause",
    "ISC",
    "Unicode-DFS-2016",
]
deny = [
    "GPL-2.0",
    "GPL-3.0",
    "AGPL-1.0",
    "AGPL-3.0",
]

[bans]
multiple-versions = "warn"
wildcards = "allow"
deny = [
    # Deny old versions with security issues
    { name = "openssl", version = "<0.10.55" },
    { name = "rustls", version = "<0.20.0" },
]

[advisories]
vulnerability = "deny"
unmaintained = "warn"
unsound = "warn"
notice = "warn"
```

9. Make scripts executable:
   chmod +x scripts/coverage.sh
   chmod +x scripts/quality-check.sh

10. Update workspace Cargo.toml to include test utilities:
    Add "crates/cortex-test-utils" to members list.

All comprehensive test coverage and quality gates are now configured.
```

## Success Criteria
- [ ] Comprehensive coverage analysis with llvm-cov
- [ ] Quality gates enforcing code standards
- [ ] Property-based testing infrastructure
- [ ] Custom assertions for domain-specific testing
- [ ] Dependency and license compliance checking