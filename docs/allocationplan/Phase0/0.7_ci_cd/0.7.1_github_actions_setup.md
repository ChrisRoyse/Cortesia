# Micro-Phase 0.7.1: Setup GitHub Actions CI/CD

## Objective
Create GitHub Actions workflows for continuous integration, testing, and automated benchmark execution.

## Prerequisites
- All core components implemented (0.1-0.6 complete)
- Project workspace structure established
- Benchmark framework ready

## Input
- Cargo workspace configuration
- Test suites from all crates
- Benchmark requirements

## Task Details

### Step 1: Create Basic CI Workflow
Setup automated testing on push and pull requests.

### Step 2: Add Multi-Platform Testing
Configure testing across different operating systems and Rust versions.

### Step 3: Create Benchmark Workflow
Setup automated benchmark execution and baseline comparison.

### Step 4: Add Security and Quality Checks
Implement cargo audit, clippy, and formatting checks.

## Expected Output
- `.github/workflows/ci.yml` implemented
- `.github/workflows/benchmarks.yml` created
- Multi-platform test matrix
- Security and quality checks

## Verification Steps
1. Push to GitHub and verify CI runs
2. Check that all tests pass
3. Verify benchmark workflow executes
4. Confirm security checks work

## Time Estimate
45-50 minutes

## AI Execution Prompt
```
Setup comprehensive GitHub Actions CI/CD workflows.

1. Create .github/workflows directory:
   mkdir -p .github/workflows

2. Create .github/workflows/ci.yml:

```yaml
name: Continuous Integration

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  test:
    name: Test Suite
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        rust: [stable, beta]
        exclude:
          # Reduce CI load - only test beta on ubuntu
          - os: windows-latest
            rust: beta
          - os: macos-latest
            rust: beta

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Install Rust toolchain
      uses: dtolnay/rust-toolchain@master
      with:
        toolchain: ${{ matrix.rust }}
        components: rustfmt, clippy

    - name: Configure Rust cache
      uses: Swatinem/rust-cache@v2
      with:
        key: ${{ matrix.os }}-${{ matrix.rust }}

    - name: Check formatting
      run: cargo fmt --all -- --check

    - name: Run Clippy
      run: cargo clippy --all-targets --all-features -- -D warnings

    - name: Build all crates
      run: cargo build --workspace --all-features

    - name: Run tests
      run: cargo test --workspace --all-features

    - name: Run doc tests
      run: cargo test --doc --workspace

  security:
    name: Security Audit
    runs-on: ubuntu-latest
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Install Rust toolchain
      uses: dtolnay/rust-toolchain@stable

    - name: Install cargo-audit
      run: cargo install cargo-audit

    - name: Run security audit
      run: cargo audit

  coverage:
    name: Code Coverage
    runs-on: ubuntu-latest
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Install Rust toolchain
      uses: dtolnay/rust-toolchain@stable
      with:
        components: llvm-tools-preview

    - name: Install cargo-llvm-cov
      run: cargo install cargo-llvm-cov

    - name: Configure Rust cache
      uses: Swatinem/rust-cache@v2

    - name: Generate coverage report
      run: cargo llvm-cov --workspace --all-features --lcov --output-path lcov.info

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        files: lcov.info
        fail_ci_if_error: true

  build-release:
    name: Build Release
    runs-on: ubuntu-latest
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Install Rust toolchain
      uses: dtolnay/rust-toolchain@stable

    - name: Configure Rust cache
      uses: Swatinem/rust-cache@v2

    - name: Build release
      run: cargo build --release --workspace

    - name: Run release tests
      run: cargo test --release --workspace

  minimum-rust-version:
    name: Minimum Rust Version
    runs-on: ubuntu-latest
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Install minimum Rust version
      uses: dtolnay/rust-toolchain@master
      with:
        toolchain: "1.70.0"  # MSRV for the project

    - name: Configure Rust cache
      uses: Swatinem/rust-cache@v2

    - name: Build with MSRV
      run: cargo build --workspace

    - name: Test with MSRV
      run: cargo test --workspace
```

3. Create .github/workflows/benchmarks.yml:

```yaml
name: Performance Benchmarks

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run benchmarks daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  CARGO_TERM_COLOR: always

jobs:
  benchmarks:
    name: Run Benchmarks
    runs-on: ubuntu-latest
    if: github.event_name != 'pull_request' || contains(github.event.pull_request.labels.*.name, 'benchmark')
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        # Fetch enough history for baseline comparison
        fetch-depth: 100

    - name: Install Rust toolchain
      uses: dtolnay/rust-toolchain@stable

    - name: Configure Rust cache
      uses: Swatinem/rust-cache@v2
      with:
        key: benchmarks

    - name: Install benchmark dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential

    - name: Build benchmark suite
      run: cargo build --release --package cortex-benchmarks

    - name: Create benchmark results directory
      run: mkdir -p benchmark-results

    - name: Run TTFS encoding benchmarks
      run: |
        cargo run --release --package cortex-benchmarks --bin benchmark -- \
          --benchmark-type ttfs-encoding \
          --iterations 1000 \
          --output benchmark-results/ttfs-encoding.json

    - name: Run column allocation benchmarks
      run: |
        cargo run --release --package cortex-benchmarks --bin benchmark -- \
          --benchmark-type column-allocation \
          --iterations 500 \
          --output benchmark-results/column-allocation.json

    - name: Run lateral inhibition benchmarks
      run: |
        cargo run --release --package cortex-benchmarks --bin benchmark -- \
          --benchmark-type lateral-inhibition \
          --iterations 300 \
          --output benchmark-results/lateral-inhibition.json

    - name: Run memory consolidation benchmarks
      run: |
        cargo run --release --package cortex-benchmarks --bin benchmark -- \
          --benchmark-type memory-consolidation \
          --iterations 100 \
          --output benchmark-results/memory-consolidation.json

    - name: Run SIMD operations benchmarks
      run: |
        cargo run --release --package cortex-benchmarks --bin benchmark -- \
          --benchmark-type simd-operations \
          --iterations 2000 \
          --output benchmark-results/simd-operations.json

    - name: Compare with baseline
      run: |
        cargo run --release --package cortex-benchmarks --bin compare-baseline -- \
          --results-dir benchmark-results \
          --baseline-file baselines/main-baseline.json \
          --output comparison-report.json

    - name: Check for regressions
      run: |
        cargo run --release --package cortex-benchmarks --bin check-regressions -- \
          --comparison-report comparison-report.json \
          --fail-on-regression

    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results-${{ github.sha }}
        path: |
          benchmark-results/
          comparison-report.json
        retention-days: 90

    - name: Update baseline on main branch
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: |
        cargo run --release --package cortex-benchmarks --bin update-baseline -- \
          --results-dir benchmark-results \
          --baseline-file baselines/main-baseline.json \
          --git-commit ${{ github.sha }}

    - name: Commit updated baseline
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add baselines/main-baseline.json
        git diff --staged --quiet || git commit -m "Update performance baseline [skip ci]"
        git push

  benchmark-pr-comment:
    name: Benchmark PR Comment
    runs-on: ubuntu-latest
    needs: benchmarks
    if: github.event_name == 'pull_request' && contains(github.event.pull_request.labels.*.name, 'benchmark')
    
    steps:
    - name: Download benchmark results
      uses: actions/download-artifact@v3
      with:
        name: benchmark-results-${{ github.sha }}

    - name: Generate benchmark comment
      run: |
        echo "## ðŸš€ Benchmark Results" > benchmark-comment.md
        echo "" >> benchmark-comment.md
        echo "Performance comparison for commit ${{ github.sha }}:" >> benchmark-comment.md
        echo "" >> benchmark-comment.md
        
        # Add benchmark summary
        if [ -f comparison-report.json ]; then
          echo "### Summary" >> benchmark-comment.md
          cargo run --release --package cortex-benchmarks --bin format-report -- \
            --input comparison-report.json \
            --format markdown >> benchmark-comment.md
        fi
        
        echo "" >> benchmark-comment.md
        echo "Detailed results available in the [benchmark artifacts](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})." >> benchmark-comment.md

    - name: Comment PR
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const comment = fs.readFileSync('benchmark-comment.md', 'utf8');
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });
```

4. Create .github/workflows/dependency-update.yml:

```yaml
name: Dependency Updates

on:
  schedule:
    # Run weekly on Sundays at 3 AM UTC
    - cron: '0 3 * * 0'
  workflow_dispatch:

jobs:
  update-dependencies:
    name: Update Dependencies
    runs-on: ubuntu-latest
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Install Rust toolchain
      uses: dtolnay/rust-toolchain@stable

    - name: Install cargo-update
      run: cargo install cargo-update

    - name: Update dependencies
      run: |
        cargo update
        cargo check --workspace

    - name: Run tests with updated dependencies
      run: cargo test --workspace

    - name: Create Pull Request
      uses: peter-evans/create-pull-request@v5
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        commit-message: "chore: update dependencies"
        title: "Automated dependency updates"
        body: |
          This is an automated pull request to update project dependencies.
          
          Please review the changes and ensure all tests pass before merging.
          
          Changes made by cargo update.
        branch: automated/dependency-updates
        delete-branch: true
```

5. Create .github/workflows/release.yml:

```yaml
name: Release

on:
  push:
    tags:
      - 'v*'

env:
  CARGO_TERM_COLOR: always

jobs:
  test:
    name: Test Release
    runs-on: ubuntu-latest
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Install Rust toolchain
      uses: dtolnay/rust-toolchain@stable

    - name: Run tests
      run: cargo test --release --workspace

  build:
    name: Build Release Artifacts
    runs-on: ${{ matrix.os }}
    needs: test
    strategy:
      matrix:
        include:
          - os: ubuntu-latest
            target: x86_64-unknown-linux-gnu
            artifact_name: cortex-kg-linux-x86_64
          - os: windows-latest
            target: x86_64-pc-windows-msvc
            artifact_name: cortex-kg-windows-x86_64.exe
          - os: macos-latest
            target: x86_64-apple-darwin
            artifact_name: cortex-kg-macos-x86_64

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Install Rust toolchain
      uses: dtolnay/rust-toolchain@stable
      with:
        targets: ${{ matrix.target }}

    - name: Build release binary
      run: cargo build --release --target ${{ matrix.target }}

    - name: Upload artifact
      uses: actions/upload-artifact@v3
      with:
        name: ${{ matrix.artifact_name }}
        path: target/${{ matrix.target }}/release/cortex-kg*

  release:
    name: Create Release
    runs-on: ubuntu-latest
    needs: build
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Download all artifacts
      uses: actions/download-artifact@v3

    - name: Create Release
      uses: softprops/action-gh-release@v1
      with:
        files: '**/cortex-kg*'
        generate_release_notes: true
        draft: false
        prerelease: ${{ contains(github.ref, '-') }}
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
```

6. Create baselines directory and initial baseline file:
   mkdir -p baselines
   
   Create baselines/main-baseline.json:
   ```json
   {
     "TTFSEncoding": {
       "metrics": {
         "avg_duration": {"secs": 0, "nanos": 1000000},
         "median_duration": {"secs": 0, "nanos": 950000},
         "p95_duration": {"secs": 0, "nanos": 1500000},
         "p99_duration": {"secs": 0, "nanos": 2000000},
         "std_deviation": {"secs": 0, "nanos": 200000},
         "throughput": 1000.0,
         "memory_usage_mb": 8.0,
         "peak_memory_mb": 10.0,
         "error_rate": 0.01,
         "successful_ops": 99,
         "failed_ops": 1,
         "custom_metrics": {}
       },
       "metadata": {
         "created_at": 1700000000,
         "git_commit": null,
         "environment": {},
         "notes": "Initial baseline",
         "system_info": {
           "os": "linux",
           "arch": "x86_64",
           "cpu_cores": 4,
           "total_memory_gb": 16.0,
           "rust_version": "1.70.0"
         }
       },
       "version": 1
     }
   }
   ```

7. Create .github/ISSUE_TEMPLATE/performance_regression.md:

```markdown
---
name: Performance Regression
about: Report a performance regression detected by benchmarks
title: 'Performance regression in [COMPONENT]'
labels: ['performance', 'regression', 'priority-high']
assignees: ''

---

## Regression Details

**Affected Component:** [e.g., TTFS Encoding, Column Allocation]
**Detected in Commit:** [commit hash]
**Detection Date:** [date]

## Metrics Comparison

**Baseline Performance:**
- Throughput: [X ops/sec]
- Average Duration: [X ms]
- Memory Usage: [X MB]
- Error Rate: [X%]

**Current Performance:**
- Throughput: [X ops/sec] ([Â±X%] change)
- Average Duration: [X ms] ([Â±X%] change)
- Memory Usage: [X MB] ([Â±X%] change)
- Error Rate: [X%] ([Â±X%] change)

## Additional Context

**Severity:** [Minor/Major/Critical]
**Likely Cause:** [if known]
**Related Changes:** [link to recent commits or PRs]

## Action Items

- [ ] Investigate root cause
- [ ] Develop fix
- [ ] Verify fix with benchmarks
- [ ] Update baseline if appropriate
```

All CI/CD workflows are now configured for comprehensive testing, benchmarking, and release management.
```

## Success Criteria
- [ ] Comprehensive CI workflow with multi-platform testing
- [ ] Automated benchmark execution and comparison
- [ ] Security auditing and dependency updates
- [ ] Release automation with artifacts
- [ ] Performance regression detection and reporting