# Micro-Phase 0.6.1: Define Benchmark Types and Metrics

## Objective
Define the benchmark framework types, metrics, and infrastructure for measuring Cortesia performance.

## Prerequisites
- All core components implemented (0.2-0.5 complete)
- Understanding of performance requirements from Phase 0

## Input
- Performance targets from Phase 0 specification
- Neuromorphic-specific metrics requirements
- Benchmark infrastructure needs

## Task Details

### Step 1: Define Core Benchmark Types
Create benchmark categories for different performance aspects.

### Step 2: Implement Metrics Collection
Build metrics gathering and aggregation system.

### Step 3: Create Performance Baselines
Define baseline measurements for comparison.

### Step 4: Add Benchmark Configuration
Allow configurable benchmark parameters.

## Expected Output
- Benchmark framework structure defined
- Metrics collection system
- Baseline measurement capabilities
- Configurable benchmark parameters

## Verification Steps
1. Verify benchmark types cover all requirements
2. Check metrics collection works
3. Confirm baseline establishment
4. Test configuration system

## Time Estimate
35-40 minutes

## AI Execution Prompt
```
Create the benchmark framework for Cortesia performance measurement.

1. First, create a new crate for benchmarks:
   cd Cortesia
   cargo new --lib crates/neuromorphic-benchmarks

2. Update workspace Cargo.toml to include the new crate:
   Add "crates/neuromorphic-benchmarks" to the members list.

3. Create crates/neuromorphic-benchmarks/Cargo.toml:
```toml
[package]
name = "neuromorphic-benchmarks"
version.workspace = true
authors.workspace = true
edition.workspace = true

[dependencies]
neuromorphic-core = { path = "../neuromorphic-core" }
snn-allocation-engine = { path = "../snn-allocation-engine" }
temporal-memory = { path = "../temporal-memory" }
snn-mocks = { path = "../snn-mocks" }

# Benchmarking and metrics
criterion = { version = "0.5", features = ["html_reports"] }
serde = { workspace = true }
serde_json = "1.0"
chrono = { workspace = true }
uuid = { workspace = true }
tokio = { workspace = true }

# Statistical analysis
statistical = "1.0"
```

4. Create src/lib.rs:
```rust
//! Benchmark framework for Cortesia neuromorphic system
//! 
//! Provides comprehensive performance measurement and analysis
//! for spiking neural network components.

pub mod types;
pub mod metrics;
pub mod runners;
pub mod baseline;
pub mod reporting;

pub use types::{BenchmarkType, BenchmarkConfig, BenchmarkResult};
pub use metrics::{MetricsCollector, PerformanceMetrics};
pub use runners::{BenchmarkRunner, BenchmarkSuite};
pub use baseline::{BaselineManager, BaselineMetrics};

/// Prelude for convenient imports
pub mod prelude {
    pub use super::types::*;
    pub use super::metrics::*;
    pub use super::runners::*;
    pub use super::baseline::*;
}
```

5. Create src/types.rs:
```rust
//! Core benchmark types and configurations

use serde::{Deserialize, Serialize};
use std::time::Duration;
use std::collections::HashMap;

/// Types of benchmarks for neuromorphic components
#[derive(Debug, Clone, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub enum BenchmarkType {
    /// TTFS encoding performance
    TTFSEncoding,
    
    /// Spiking column allocation speed
    ColumnAllocation,
    
    /// Lateral inhibition convergence
    LateralInhibition,
    
    /// Memory branch consolidation
    MemoryConsolidation,
    
    /// SIMD operations performance
    SIMDOperations,
    
    /// End-to-end allocation pipeline
    FullPipeline,
    
    /// Stress testing under load
    StressTest,
    
    /// Memory usage and leaks
    MemoryBenchmark,
    
    /// Thread safety and concurrency
    ConcurrencyTest,
    
    /// Spike pattern similarity
    SimilarityComputation,
}

impl BenchmarkType {
    /// Get human-readable description
    pub fn description(&self) -> &'static str {
        match self {
            Self::TTFSEncoding => "Time-to-First-Spike encoding performance",
            Self::ColumnAllocation => "Cortical column allocation speed",
            Self::LateralInhibition => "Winner-take-all inhibition timing",
            Self::MemoryConsolidation => "Memory branch merge performance",
            Self::SIMDOperations => "SIMD vector operation speed",
            Self::FullPipeline => "Complete allocation pipeline timing",
            Self::StressTest => "Performance under sustained load",
            Self::MemoryBenchmark => "Memory usage and leak detection",
            Self::ConcurrencyTest => "Thread safety and parallel performance",
            Self::SimilarityComputation => "Concept similarity calculation speed",
        }
    }
    
    /// Get target performance threshold
    pub fn target_threshold(&self) -> PerformanceThreshold {
        match self {
            Self::TTFSEncoding => PerformanceThreshold {
                max_duration: Some(Duration::from_millis(1)),
                min_throughput: Some(1000.0), // operations per second
                max_memory_mb: Some(10.0),
                max_error_rate: Some(0.01),
            },
            Self::ColumnAllocation => PerformanceThreshold {
                max_duration: Some(Duration::from_millis(5)),
                min_throughput: Some(200.0),
                max_memory_mb: Some(50.0),
                max_error_rate: Some(0.05),
            },
            Self::LateralInhibition => PerformanceThreshold {
                max_duration: Some(Duration::from_millis(2)),
                min_throughput: Some(500.0),
                max_memory_mb: Some(20.0),
                max_error_rate: Some(0.02),
            },
            Self::MemoryConsolidation => PerformanceThreshold {
                max_duration: Some(Duration::from_millis(100)),
                min_throughput: Some(10.0),
                max_memory_mb: Some(100.0),
                max_error_rate: Some(0.1),
            },
            Self::SIMDOperations => PerformanceThreshold {
                max_duration: Some(Duration::from_micros(100)),
                min_throughput: Some(10000.0),
                max_memory_mb: Some(5.0),
                max_error_rate: Some(0.001),
            },
            Self::FullPipeline => PerformanceThreshold {
                max_duration: Some(Duration::from_millis(10)),
                min_throughput: Some(100.0),
                max_memory_mb: Some(200.0),
                max_error_rate: Some(0.05),
            },
            Self::StressTest => PerformanceThreshold {
                max_duration: None, // Duration varies
                min_throughput: Some(50.0),
                max_memory_mb: Some(500.0),
                max_error_rate: Some(0.1),
            },
            Self::MemoryBenchmark => PerformanceThreshold {
                max_duration: None,
                min_throughput: None,
                max_memory_mb: Some(1000.0),
                max_error_rate: Some(0.0), // No memory leaks allowed
            },
            Self::ConcurrencyTest => PerformanceThreshold {
                max_duration: Some(Duration::from_millis(50)),
                min_throughput: Some(20.0),
                max_memory_mb: Some(300.0),
                max_error_rate: Some(0.0), // No race conditions
            },
            Self::SimilarityComputation => PerformanceThreshold {
                max_duration: Some(Duration::from_millis(1)),
                min_throughput: Some(1000.0),
                max_memory_mb: Some(10.0),
                max_error_rate: Some(0.01),
            },
        }
    }
}

/// Performance thresholds for pass/fail criteria
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceThreshold {
    /// Maximum allowed duration
    pub max_duration: Option<Duration>,
    
    /// Minimum required throughput (ops/sec)
    pub min_throughput: Option<f64>,
    
    /// Maximum memory usage in MB
    pub max_memory_mb: Option<f64>,
    
    /// Maximum error rate (0.0 to 1.0)
    pub max_error_rate: Option<f64>,
}

/// Configuration for benchmark execution
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BenchmarkConfig {
    /// Type of benchmark to run
    pub benchmark_type: BenchmarkType,
    
    /// Number of iterations
    pub iterations: usize,
    
    /// Warmup iterations
    pub warmup_iterations: usize,
    
    /// Input data size
    pub data_size: usize,
    
    /// Concurrent threads/workers
    pub concurrency: usize,
    
    /// Duration for stress tests
    pub duration: Option<Duration>,
    
    /// Custom parameters
    pub parameters: HashMap<String, serde_json::Value>,
    
    /// Whether to save detailed results
    pub save_details: bool,
    
    /// Output directory for results
    pub output_dir: Option<String>,
}

impl Default for BenchmarkConfig {
    fn default() -> Self {
        Self {
            benchmark_type: BenchmarkType::TTFSEncoding,
            iterations: 1000,
            warmup_iterations: 100,
            data_size: 100,
            concurrency: 1,
            duration: None,
            parameters: HashMap::new(),
            save_details: false,
            output_dir: None,
        }
    }
}

/// Result of a benchmark run
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BenchmarkResult {
    /// Benchmark configuration
    pub config: BenchmarkConfig,
    
    /// Performance metrics
    pub metrics: PerformanceMetrics,
    
    /// Whether benchmark passed thresholds
    pub passed: bool,
    
    /// Failure reasons if any
    pub failures: Vec<String>,
    
    /// Timestamp of benchmark
    pub timestamp: chrono::DateTime<chrono::Utc>,
    
    /// Additional metadata
    pub metadata: HashMap<String, serde_json::Value>,
}

/// Performance metrics collected during benchmarks
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceMetrics {
    /// Average duration
    pub avg_duration: Duration,
    
    /// Median duration
    pub median_duration: Duration,
    
    /// 95th percentile duration
    pub p95_duration: Duration,
    
    /// 99th percentile duration
    pub p99_duration: Duration,
    
    /// Standard deviation
    pub std_deviation: Duration,
    
    /// Throughput (operations per second)
    pub throughput: f64,
    
    /// Memory usage in MB
    pub memory_usage_mb: f64,
    
    /// Peak memory usage in MB
    pub peak_memory_mb: f64,
    
    /// Error rate (0.0 to 1.0)
    pub error_rate: f64,
    
    /// Number of successful operations
    pub successful_ops: usize,
    
    /// Number of failed operations
    pub failed_ops: usize,
    
    /// Custom metrics
    pub custom_metrics: HashMap<String, f64>,
}

impl Default for PerformanceMetrics {
    fn default() -> Self {
        Self {
            avg_duration: Duration::ZERO,
            median_duration: Duration::ZERO,
            p95_duration: Duration::ZERO,
            p99_duration: Duration::ZERO,
            std_deviation: Duration::ZERO,
            throughput: 0.0,
            memory_usage_mb: 0.0,
            peak_memory_mb: 0.0,
            error_rate: 0.0,
            successful_ops: 0,
            failed_ops: 0,
            custom_metrics: HashMap::new(),
        }
    }
}

/// Benchmark execution context
#[derive(Debug, Clone)]
pub struct BenchmarkContext {
    /// Unique benchmark run ID
    pub run_id: uuid::Uuid,
    
    /// Start timestamp
    pub start_time: chrono::DateTime<chrono::Utc>,
    
    /// System information
    pub system_info: SystemInfo,
    
    /// Environment variables
    pub environment: HashMap<String, String>,
}

/// System information for benchmark context
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SystemInfo {
    /// Number of CPU cores
    pub cpu_cores: usize,
    
    /// Total system memory in MB
    pub total_memory_mb: u64,
    
    /// Operating system
    pub os: String,
    
    /// Rust version
    pub rust_version: String,
    
    /// Build configuration (debug/release)
    pub build_config: String,
}

impl Default for SystemInfo {
    fn default() -> Self {
        Self {
            cpu_cores: num_cpus::get(),
            total_memory_mb: 8192, // Default assumption
            os: std::env::consts::OS.to_string(),
            rust_version: "unknown".to_string(),
            build_config: if cfg!(debug_assertions) { "debug" } else { "release" }.to_string(),
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_benchmark_type_descriptions() {
        for benchmark_type in [
            BenchmarkType::TTFSEncoding,
            BenchmarkType::ColumnAllocation,
            BenchmarkType::LateralInhibition,
            BenchmarkType::MemoryConsolidation,
        ] {
            assert!(!benchmark_type.description().is_empty());
        }
    }
    
    #[test]
    fn test_performance_thresholds() {
        let threshold = BenchmarkType::TTFSEncoding.target_threshold();
        assert!(threshold.max_duration.is_some());
        assert!(threshold.min_throughput.is_some());
    }
    
    #[test]
    fn test_benchmark_config_serialization() {
        let config = BenchmarkConfig::default();
        let serialized = serde_json::to_string(&config).unwrap();
        let deserialized: BenchmarkConfig = serde_json::from_str(&serialized).unwrap();
        
        assert_eq!(config.benchmark_type, deserialized.benchmark_type);
        assert_eq!(config.iterations, deserialized.iterations);
    }
}
```

6. Add num_cpus dependency to Cargo.toml:
   num_cpus = "1.16"

7. Create placeholder files for other modules:
   touch src/metrics.rs
   touch src/runners.rs  
   touch src/baseline.rs
   touch src/reporting.rs

   Add to each:
   ```rust
   //! [Module description]
   
   // TODO: Implement in next micro-phase
   ```

8. Run tests:
   cd crates/neuromorphic-benchmarks
   cargo test

All tests should pass.
```

## Success Criteria
- [ ] Comprehensive benchmark types defined
- [ ] Performance thresholds for each type
- [ ] Configurable benchmark parameters
- [ ] Metrics collection structure
- [ ] System information capture