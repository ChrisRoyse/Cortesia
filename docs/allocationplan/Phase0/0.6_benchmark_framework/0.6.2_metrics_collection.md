# Micro-Phase 0.6.2: Implement Metrics Collection System

## Objective
Implement the metrics collection system for gathering performance data during benchmark execution.

## Prerequisites
- Benchmark types and configuration defined (0.6.1 complete)
- neuromorphic-benchmarks crate created

## Input
- BenchmarkType definitions from 0.6.1
- PerformanceMetrics structure
- System resource monitoring requirements

## Task Details

### Step 1: Implement MetricsCollector
Create the core metrics collection engine with real-time monitoring.

### Step 2: Add Statistical Analysis
Implement percentile calculations, standard deviation, and throughput metrics.

### Step 3: Create Resource Monitoring
Add memory usage tracking and system resource monitoring.

### Step 4: Implement Data Aggregation
Build aggregation utilities for batch benchmark results.

## Expected Output
- `src/metrics.rs` fully implemented
- Real-time metrics collection
- Statistical analysis capabilities
- Resource monitoring utilities

## Verification Steps
1. Run `cargo test metrics` tests
2. Verify metrics collection accuracy
3. Check statistical calculations
4. Confirm resource monitoring works

## Time Estimate
40-45 minutes

## AI Execution Prompt
```
Implement the metrics collection system for benchmarks.

1. Replace src/metrics.rs content:

```rust
//! Metrics collection and analysis for benchmarks

use crate::types::{PerformanceMetrics, BenchmarkConfig, SystemInfo};
use std::time::{Duration, Instant};
use std::collections::HashMap;
use std::sync::{Arc, Mutex};
use serde::{Deserialize, Serialize};

/// Real-time metrics collector
pub struct MetricsCollector {
    start_time: Instant,
    measurements: Arc<Mutex<Vec<Measurement>>>,
    config: Arc<BenchmarkConfig>,
    system_monitor: SystemMonitor,
}

/// Individual measurement point
#[derive(Debug, Clone)]
struct Measurement {
    timestamp: Instant,
    duration: Duration,
    success: bool,
    memory_usage: u64,
    custom_data: HashMap<String, f64>,
}

/// System resource monitor
struct SystemMonitor {
    initial_memory: u64,
    peak_memory: Arc<Mutex<u64>>,
    monitoring: Arc<Mutex<bool>>,
}

impl MetricsCollector {
    /// Create new metrics collector
    pub fn new(config: BenchmarkConfig) -> Self {
        Self {
            start_time: Instant::now(),
            measurements: Arc::new(Mutex::new(Vec::new())),
            config: Arc::new(config),
            system_monitor: SystemMonitor::new(),
        }
    }
    
    /// Start collecting metrics
    pub fn start_collection(&self) {
        self.system_monitor.start_monitoring();
    }
    
    /// Record a measurement
    pub fn record_measurement(&self, duration: Duration, success: bool) {
        self.record_measurement_with_data(duration, success, HashMap::new());
    }
    
    /// Record measurement with custom data
    pub fn record_measurement_with_data(
        &self, 
        duration: Duration, 
        success: bool, 
        custom_data: HashMap<String, f64>
    ) {
        let memory_usage = self.system_monitor.current_memory_usage();
        
        let measurement = Measurement {
            timestamp: Instant::now(),
            duration,
            success,
            memory_usage,
            custom_data,
        };
        
        self.measurements.lock().unwrap().push(measurement);
        self.system_monitor.update_peak_memory(memory_usage);
    }
    
    /// Record operation start (returns handle for automatic timing)
    pub fn start_operation(&self) -> OperationHandle {
        OperationHandle::new(self.clone())
    }
    
    /// Finalize collection and generate metrics
    pub fn finalize(self) -> PerformanceMetrics {
        self.system_monitor.stop_monitoring();
        
        let measurements = self.measurements.lock().unwrap();
        
        if measurements.is_empty() {
            return PerformanceMetrics::default();
        }
        
        // Extract timing data
        let durations: Vec<Duration> = measurements.iter()
            .map(|m| m.duration)
            .collect();
        
        let successful_ops = measurements.iter()
            .filter(|m| m.success)
            .count();
        
        let failed_ops = measurements.len() - successful_ops;
        
        // Calculate statistics
        let stats = DurationStatistics::from_durations(&durations);
        
        // Calculate throughput
        let total_time = self.start_time.elapsed();
        let throughput = if total_time.as_secs_f64() > 0.0 {
            successful_ops as f64 / total_time.as_secs_f64()
        } else {
            0.0
        };
        
        // Memory metrics
        let memory_usage_mb = measurements.iter()
            .map(|m| m.memory_usage as f64 / 1024.0 / 1024.0)
            .sum::<f64>() / measurements.len() as f64;
        
        let peak_memory_mb = *self.system_monitor.peak_memory.lock().unwrap() as f64 / 1024.0 / 1024.0;
        
        // Error rate
        let error_rate = failed_ops as f64 / measurements.len() as f64;
        
        // Custom metrics aggregation
        let custom_metrics = self.aggregate_custom_metrics(&measurements);
        
        PerformanceMetrics {
            avg_duration: stats.mean,
            median_duration: stats.median,
            p95_duration: stats.p95,
            p99_duration: stats.p99,
            std_deviation: stats.std_dev,
            throughput,
            memory_usage_mb,
            peak_memory_mb,
            error_rate,
            successful_ops,
            failed_ops,
            custom_metrics,
        }
    }
    
    /// Aggregate custom metrics from measurements
    fn aggregate_custom_metrics(&self, measurements: &[Measurement]) -> HashMap<String, f64> {
        let mut aggregated = HashMap::new();
        let mut counts = HashMap::new();
        
        for measurement in measurements {
            for (key, value) in &measurement.custom_data {
                *aggregated.entry(key.clone()).or_insert(0.0) += value;
                *counts.entry(key.clone()).or_insert(0) += 1;
            }
        }
        
        // Calculate averages
        for (key, total) in &mut aggregated {
            if let Some(&count) = counts.get(key) {
                *total /= count as f64;
            }
        }
        
        aggregated
    }
    
    /// Get current measurements count
    pub fn measurement_count(&self) -> usize {
        self.measurements.lock().unwrap().len()
    }
    
    /// Get real-time throughput
    pub fn current_throughput(&self) -> f64 {
        let measurements = self.measurements.lock().unwrap();
        let elapsed = self.start_time.elapsed();
        
        if elapsed.as_secs_f64() > 0.0 {
            measurements.len() as f64 / elapsed.as_secs_f64()
        } else {
            0.0
        }
    }
}

impl Clone for MetricsCollector {
    fn clone(&self) -> Self {
        Self {
            start_time: self.start_time,
            measurements: Arc::clone(&self.measurements),
            config: Arc::clone(&self.config),
            system_monitor: self.system_monitor.clone(),
        }
    }
}

/// Handle for automatic operation timing
pub struct OperationHandle {
    collector: MetricsCollector,
    start_time: Instant,
    custom_data: HashMap<String, f64>,
}

impl OperationHandle {
    fn new(collector: MetricsCollector) -> Self {
        Self {
            collector,
            start_time: Instant::now(),
            custom_data: HashMap::new(),
        }
    }
    
    /// Add custom data to this operation
    pub fn add_data(mut self, key: &str, value: f64) -> Self {
        self.custom_data.insert(key.to_string(), value);
        self
    }
    
    /// Complete the operation successfully
    pub fn complete(self) {
        let duration = self.start_time.elapsed();
        self.collector.record_measurement_with_data(duration, true, self.custom_data);
    }
    
    /// Complete the operation with failure
    pub fn fail(self) {
        let duration = self.start_time.elapsed();
        self.collector.record_measurement_with_data(duration, false, self.custom_data);
    }
}

/// Statistics calculator for duration data
struct DurationStatistics {
    mean: Duration,
    median: Duration,
    p95: Duration,
    p99: Duration,
    std_dev: Duration,
}

impl DurationStatistics {
    fn from_durations(durations: &[Duration]) -> Self {
        if durations.is_empty() {
            return Self {
                mean: Duration::ZERO,
                median: Duration::ZERO,
                p95: Duration::ZERO,
                p99: Duration::ZERO,
                std_dev: Duration::ZERO,
            };
        }
        
        let mut sorted_nanos: Vec<u128> = durations.iter()
            .map(|d| d.as_nanos())
            .collect();
        sorted_nanos.sort_unstable();
        
        // Calculate percentiles
        let median = Self::percentile(&sorted_nanos, 50.0);
        let p95 = Self::percentile(&sorted_nanos, 95.0);
        let p99 = Self::percentile(&sorted_nanos, 99.0);
        
        // Calculate mean
        let sum: u128 = sorted_nanos.iter().sum();
        let mean_nanos = sum / sorted_nanos.len() as u128;
        let mean = Duration::from_nanos(mean_nanos as u64);
        
        // Calculate standard deviation
        let variance = sorted_nanos.iter()
            .map(|&nanos| {
                let diff = nanos as f64 - mean_nanos as f64;
                diff * diff
            })
            .sum::<f64>() / sorted_nanos.len() as f64;
        
        let std_dev = Duration::from_nanos(variance.sqrt() as u64);
        
        Self { mean, median, p95, p99, std_dev }
    }
    
    fn percentile(sorted_nanos: &[u128], percentile: f64) -> Duration {
        if sorted_nanos.is_empty() {
            return Duration::ZERO;
        }
        
        let index = (percentile / 100.0 * (sorted_nanos.len() - 1) as f64).round() as usize;
        let index = index.min(sorted_nanos.len() - 1);
        
        Duration::from_nanos(sorted_nanos[index] as u64)
    }
}

/// System resource monitoring
impl SystemMonitor {
    fn new() -> Self {
        Self {
            initial_memory: Self::get_memory_usage(),
            peak_memory: Arc::new(Mutex::new(0)),
            monitoring: Arc::new(Mutex::new(false)),
        }
    }
    
    fn clone(&self) -> Self {
        Self {
            initial_memory: self.initial_memory,
            peak_memory: Arc::clone(&self.peak_memory),
            monitoring: Arc::clone(&self.monitoring),
        }
    }
    
    fn start_monitoring(&self) {
        *self.monitoring.lock().unwrap() = true;
        *self.peak_memory.lock().unwrap() = self.initial_memory;
        
        // Start background monitoring thread
        let peak_memory = Arc::clone(&self.peak_memory);
        let monitoring = Arc::clone(&self.monitoring);
        
        std::thread::spawn(move || {
            while *monitoring.lock().unwrap() {
                let current = Self::get_memory_usage();
                let mut peak = peak_memory.lock().unwrap();
                if current > *peak {
                    *peak = current;
                }
                std::thread::sleep(Duration::from_millis(100));
            }
        });
    }
    
    fn stop_monitoring(&self) {
        *self.monitoring.lock().unwrap() = false;
    }
    
    fn current_memory_usage(&self) -> u64 {
        Self::get_memory_usage()
    }
    
    fn update_peak_memory(&self, current: u64) {
        let mut peak = self.peak_memory.lock().unwrap();
        if current > *peak {
            *peak = current;
        }
    }
    
    #[cfg(target_os = "windows")]
    fn get_memory_usage() -> u64 {
        // Simplified memory usage for Windows
        // In real implementation, would use Windows API
        8 * 1024 * 1024 // 8MB default
    }
    
    #[cfg(not(target_os = "windows"))]
    fn get_memory_usage() -> u64 {
        // Simplified memory usage for Unix-like systems
        // In real implementation, would read from /proc/self/status
        match std::fs::read_to_string("/proc/self/status") {
            Ok(content) => {
                for line in content.lines() {
                    if line.starts_with("VmRSS:") {
                        if let Some(kb_str) = line.split_whitespace().nth(1) {
                            if let Ok(kb) = kb_str.parse::<u64>() {
                                return kb * 1024; // Convert KB to bytes
                            }
                        }
                    }
                }
                8 * 1024 * 1024 // 8MB fallback
            }
            Err(_) => 8 * 1024 * 1024, // 8MB fallback
        }
    }
}

/// Batch metrics analysis utilities
pub struct BatchAnalyzer;

impl BatchAnalyzer {
    /// Analyze multiple benchmark results
    pub fn analyze_batch(results: &[PerformanceMetrics]) -> BatchAnalysis {
        if results.is_empty() {
            return BatchAnalysis::default();
        }
        
        let avg_throughput = results.iter()
            .map(|r| r.throughput)
            .sum::<f64>() / results.len() as f64;
        
        let avg_duration = Duration::from_nanos(
            results.iter()
                .map(|r| r.avg_duration.as_nanos())
                .sum::<u128>() / results.len() as u128
        );
        
        let total_operations = results.iter()
            .map(|r| r.successful_ops + r.failed_ops)
            .sum();
        
        let total_errors = results.iter()
            .map(|r| r.failed_ops)
            .sum();
        
        let overall_error_rate = if total_operations > 0 {
            total_errors as f64 / total_operations as f64
        } else {
            0.0
        };
        
        let peak_memory = results.iter()
            .map(|r| r.peak_memory_mb)
            .fold(0.0, f64::max);
        
        BatchAnalysis {
            total_benchmarks: results.len(),
            avg_throughput,
            avg_duration,
            total_operations,
            overall_error_rate,
            peak_memory_mb: peak_memory,
        }
    }
    
    /// Compare two sets of metrics
    pub fn compare_metrics(baseline: &PerformanceMetrics, current: &PerformanceMetrics) -> MetricsComparison {
        let throughput_change = if baseline.throughput > 0.0 {
            (current.throughput - baseline.throughput) / baseline.throughput
        } else {
            0.0
        };
        
        let duration_change = if baseline.avg_duration.as_nanos() > 0 {
            (current.avg_duration.as_nanos() as f64 - baseline.avg_duration.as_nanos() as f64) 
                / baseline.avg_duration.as_nanos() as f64
        } else {
            0.0
        };
        
        let memory_change = if baseline.peak_memory_mb > 0.0 {
            (current.peak_memory_mb - baseline.peak_memory_mb) / baseline.peak_memory_mb
        } else {
            0.0
        };
        
        MetricsComparison {
            throughput_change_percent: throughput_change * 100.0,
            duration_change_percent: duration_change * 100.0,
            memory_change_percent: memory_change * 100.0,
            error_rate_delta: current.error_rate - baseline.error_rate,
        }
    }
}

/// Analysis of multiple benchmark runs
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BatchAnalysis {
    pub total_benchmarks: usize,
    pub avg_throughput: f64,
    pub avg_duration: Duration,
    pub total_operations: usize,
    pub overall_error_rate: f64,
    pub peak_memory_mb: f64,
}

impl Default for BatchAnalysis {
    fn default() -> Self {
        Self {
            total_benchmarks: 0,
            avg_throughput: 0.0,
            avg_duration: Duration::ZERO,
            total_operations: 0,
            overall_error_rate: 0.0,
            peak_memory_mb: 0.0,
        }
    }
}

/// Comparison between two metrics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MetricsComparison {
    pub throughput_change_percent: f64,
    pub duration_change_percent: f64,
    pub memory_change_percent: f64,
    pub error_rate_delta: f64,
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::types::BenchmarkType;
    
    #[test]
    fn test_metrics_collection() {
        let config = BenchmarkConfig {
            benchmark_type: BenchmarkType::TTFSEncoding,
            iterations: 100,
            ..Default::default()
        };
        
        let collector = MetricsCollector::new(config);
        collector.start_collection();
        
        // Simulate measurements
        for i in 0..10 {
            let duration = Duration::from_millis(i * 10 + 50);
            let success = i % 5 != 0; // 80% success rate
            collector.record_measurement(duration, success);
        }
        
        let metrics = collector.finalize();
        
        assert_eq!(metrics.successful_ops, 8);
        assert_eq!(metrics.failed_ops, 2);
        assert_eq!(metrics.error_rate, 0.2);
        assert!(metrics.avg_duration > Duration::ZERO);
        assert!(metrics.throughput > 0.0);
    }
    
    #[test]
    fn test_operation_handle() {
        let config = BenchmarkConfig::default();
        let collector = MetricsCollector::new(config);
        
        let handle = collector.start_operation()
            .add_data("operations", 100.0);
        
        std::thread::sleep(Duration::from_millis(10));
        handle.complete();
        
        assert_eq!(collector.measurement_count(), 1);
        
        let metrics = collector.finalize();
        assert_eq!(metrics.successful_ops, 1);
        assert!(metrics.custom_metrics.contains_key("operations"));
    }
    
    #[test]
    fn test_duration_statistics() {
        let durations = vec![
            Duration::from_millis(10),
            Duration::from_millis(20),
            Duration::from_millis(30),
            Duration::from_millis(40),
            Duration::from_millis(50),
        ];
        
        let stats = DurationStatistics::from_durations(&durations);
        
        assert_eq!(stats.median, Duration::from_millis(30));
        assert_eq!(stats.mean, Duration::from_millis(30));
        assert_eq!(stats.p95, Duration::from_millis(50));
    }
    
    #[test]
    fn test_batch_analysis() {
        let metrics1 = PerformanceMetrics {
            throughput: 100.0,
            avg_duration: Duration::from_millis(10),
            successful_ops: 95,
            failed_ops: 5,
            peak_memory_mb: 50.0,
            ..Default::default()
        };
        
        let metrics2 = PerformanceMetrics {
            throughput: 90.0,
            avg_duration: Duration::from_millis(12),
            successful_ops: 88,
            failed_ops: 12,
            peak_memory_mb: 60.0,
            ..Default::default()
        };
        
        let analysis = BatchAnalyzer::analyze_batch(&[metrics1, metrics2]);
        
        assert_eq!(analysis.total_benchmarks, 2);
        assert_eq!(analysis.avg_throughput, 95.0);
        assert_eq!(analysis.peak_memory_mb, 60.0);
    }
    
    #[test]
    fn test_metrics_comparison() {
        let baseline = PerformanceMetrics {
            throughput: 100.0,
            avg_duration: Duration::from_millis(10),
            peak_memory_mb: 50.0,
            error_rate: 0.05,
            ..Default::default()
        };
        
        let current = PerformanceMetrics {
            throughput: 110.0,
            avg_duration: Duration::from_millis(9),
            peak_memory_mb: 55.0,
            error_rate: 0.03,
            ..Default::default()
        };
        
        let comparison = BatchAnalyzer::compare_metrics(&baseline, &current);
        
        assert_eq!(comparison.throughput_change_percent, 10.0);
        assert_eq!(comparison.duration_change_percent, -10.0);
        assert_eq!(comparison.memory_change_percent, 10.0);
        assert_eq!(comparison.error_rate_delta, -0.02);
    }
}
```

2. Run tests:
   cd crates/neuromorphic-benchmarks
   cargo test metrics

All tests should pass.
```

## Success Criteria
- [ ] MetricsCollector with real-time monitoring
- [ ] Statistical analysis (percentiles, std dev)
- [ ] System resource monitoring
- [ ] Batch analysis utilities
- [ ] Metrics comparison functionality