# Micro-Phase 0.6.3: Establish Performance Baselines

## Objective
Create the baseline management system for tracking performance benchmarks over time and detecting regressions.

## Prerequisites
- Benchmark types defined (0.6.1 complete)
- Metrics collection implemented (0.6.2 complete)

## Input
- PerformanceMetrics structure
- BenchmarkType definitions
- Historical performance data requirements

## Task Details

### Step 1: Implement BaselineManager
Create the baseline storage and retrieval system with versioning.

### Step 2: Add Regression Detection
Implement automatic regression detection against established baselines.

### Step 3: Create Baseline Persistence
Add file-based storage for baseline data with JSON serialization.

### Step 4: Implement Baseline Analysis
Build comparison and trending analysis for baseline data.

## Expected Output
- `src/baseline.rs` fully implemented
- Baseline storage and retrieval
- Regression detection algorithms
- Baseline persistence and loading

## Verification Steps
1. Run `cargo test baseline` tests
2. Verify baseline storage/retrieval
3. Check regression detection accuracy
4. Confirm persistence works correctly

## Time Estimate
35-40 minutes

## AI Execution Prompt
```
Implement the baseline management system for performance tracking.

1. Replace src/baseline.rs content:

```rust
//! Baseline management for performance tracking

use crate::types::{PerformanceMetrics, BenchmarkType, BenchmarkResult};
use crate::metrics::MetricsComparison;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::{Path, PathBuf};
use std::fs;
use std::time::{SystemTime, UNIX_EPOCH};

/// Manages performance baselines and regression detection
pub struct BaselineManager {
    baselines: HashMap<BenchmarkType, BaselineEntry>,
    storage_path: Option<PathBuf>,
    regression_thresholds: RegressionThresholds,
}

/// A baseline entry with metadata
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BaselineEntry {
    pub metrics: PerformanceMetrics,
    pub metadata: BaselineMetadata,
    pub version: u32,
}

/// Metadata for baseline entries
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BaselineMetadata {
    pub created_at: u64, // Unix timestamp
    pub git_commit: Option<String>,
    pub environment: HashMap<String, String>,
    pub notes: Option<String>,
    pub system_info: SystemInfo,
}

/// System information for baseline context
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SystemInfo {
    pub os: String,
    pub arch: String,
    pub cpu_cores: usize,
    pub total_memory_gb: f64,
    pub rust_version: String,
}

/// Thresholds for regression detection
#[derive(Debug, Clone)]
pub struct RegressionThresholds {
    /// Maximum allowed throughput decrease (as percentage)
    pub max_throughput_decrease: f64,
    
    /// Maximum allowed duration increase (as percentage)
    pub max_duration_increase: f64,
    
    /// Maximum allowed memory increase (as percentage)
    pub max_memory_increase: f64,
    
    /// Maximum allowed error rate increase
    pub max_error_rate_increase: f64,
}

impl Default for RegressionThresholds {
    fn default() -> Self {
        Self {
            max_throughput_decrease: 10.0, // 10% throughput loss
            max_duration_increase: 15.0,   // 15% duration increase
            max_memory_increase: 20.0,     // 20% memory increase
            max_error_rate_increase: 0.05, // 5% error rate increase
        }
    }
}

/// Result of regression analysis
#[derive(Debug, Clone)]
pub struct RegressionAnalysis {
    pub has_regression: bool,
    pub issues: Vec<RegressionIssue>,
    pub comparison: MetricsComparison,
    pub severity: RegressionSeverity,
}

/// Individual regression issue
#[derive(Debug, Clone)]
pub struct RegressionIssue {
    pub metric: String,
    pub current_value: f64,
    pub baseline_value: f64,
    pub change_percent: f64,
    pub threshold: f64,
    pub severity: RegressionSeverity,
}

/// Severity levels for regressions
#[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord)]
pub enum RegressionSeverity {
    None,
    Minor,
    Major,
    Critical,
}

impl BaselineManager {
    /// Create new baseline manager
    pub fn new() -> Self {
        Self {
            baselines: HashMap::new(),
            storage_path: None,
            regression_thresholds: RegressionThresholds::default(),
        }
    }
    
    /// Create with storage path
    pub fn with_storage<P: AsRef<Path>>(path: P) -> Self {
        Self {
            baselines: HashMap::new(),
            storage_path: Some(path.as_ref().to_path_buf()),
            regression_thresholds: RegressionThresholds::default(),
        }
    }
    
    /// Set regression thresholds
    pub fn with_thresholds(mut self, thresholds: RegressionThresholds) -> Self {
        self.regression_thresholds = thresholds;
        self
    }
    
    /// Load baselines from storage
    pub fn load_baselines(&mut self) -> Result<(), BaselineError> {
        if let Some(ref path) = self.storage_path {
            if path.exists() {
                let content = fs::read_to_string(path)
                    .map_err(|e| BaselineError::IoError(e.to_string()))?;
                
                let stored_baselines: HashMap<BenchmarkType, BaselineEntry> = 
                    serde_json::from_str(&content)
                        .map_err(|e| BaselineError::ParseError(e.to_string()))?;
                
                self.baselines = stored_baselines;
            }
        }
        Ok(())
    }
    
    /// Save baselines to storage
    pub fn save_baselines(&self) -> Result<(), BaselineError> {
        if let Some(ref path) = self.storage_path {
            // Ensure parent directory exists
            if let Some(parent) = path.parent() {
                fs::create_dir_all(parent)
                    .map_err(|e| BaselineError::IoError(e.to_string()))?;
            }
            
            let content = serde_json::to_string_pretty(&self.baselines)
                .map_err(|e| BaselineError::SerializeError(e.to_string()))?;
            
            fs::write(path, content)
                .map_err(|e| BaselineError::IoError(e.to_string()))?;
        }
        Ok(())
    }
    
    /// Set baseline for a benchmark type
    pub fn set_baseline(&mut self, 
                        benchmark_type: BenchmarkType, 
                        metrics: PerformanceMetrics,
                        git_commit: Option<String>,
                        notes: Option<String>) -> Result<(), BaselineError> {
        
        let version = self.baselines.get(&benchmark_type)
            .map(|entry| entry.version + 1)
            .unwrap_or(1);
        
        let metadata = BaselineMetadata {
            created_at: SystemTime::now()
                .duration_since(UNIX_EPOCH)
                .unwrap()
                .as_secs(),
            git_commit,
            environment: Self::collect_environment(),
            notes,
            system_info: Self::collect_system_info(),
        };
        
        let entry = BaselineEntry {
            metrics,
            metadata,
            version,
        };
        
        self.baselines.insert(benchmark_type, entry);
        self.save_baselines()?;
        
        Ok(())
    }
    
    /// Get baseline for a benchmark type
    pub fn get_baseline(&self, benchmark_type: &BenchmarkType) -> Option<&BaselineEntry> {
        self.baselines.get(benchmark_type)
    }
    
    /// Check for regressions against baseline
    pub fn check_regression(&self, 
                           benchmark_type: &BenchmarkType, 
                           current_metrics: &PerformanceMetrics) -> Option<RegressionAnalysis> {
        
        let baseline_entry = self.baselines.get(benchmark_type)?;
        let baseline_metrics = &baseline_entry.metrics;
        
        let comparison = crate::metrics::BatchAnalyzer::compare_metrics(
            baseline_metrics, 
            current_metrics
        );
        
        let mut issues = Vec::new();
        let mut max_severity = RegressionSeverity::None;
        
        // Check throughput regression
        if comparison.throughput_change_percent < -self.regression_thresholds.max_throughput_decrease {
            let severity = self.calculate_severity(
                comparison.throughput_change_percent.abs(),
                self.regression_thresholds.max_throughput_decrease
            );
            
            if severity > max_severity {
                max_severity = severity.clone();
            }
            
            issues.push(RegressionIssue {
                metric: "throughput".to_string(),
                current_value: current_metrics.throughput,
                baseline_value: baseline_metrics.throughput,
                change_percent: comparison.throughput_change_percent,
                threshold: -self.regression_thresholds.max_throughput_decrease,
                severity,
            });
        }
        
        // Check duration regression
        if comparison.duration_change_percent > self.regression_thresholds.max_duration_increase {
            let severity = self.calculate_severity(
                comparison.duration_change_percent,
                self.regression_thresholds.max_duration_increase
            );
            
            if severity > max_severity {
                max_severity = severity.clone();
            }
            
            issues.push(RegressionIssue {
                metric: "avg_duration".to_string(),
                current_value: current_metrics.avg_duration.as_secs_f64(),
                baseline_value: baseline_metrics.avg_duration.as_secs_f64(),
                change_percent: comparison.duration_change_percent,
                threshold: self.regression_thresholds.max_duration_increase,
                severity,
            });
        }
        
        // Check memory regression
        if comparison.memory_change_percent > self.regression_thresholds.max_memory_increase {
            let severity = self.calculate_severity(
                comparison.memory_change_percent,
                self.regression_thresholds.max_memory_increase
            );
            
            if severity > max_severity {
                max_severity = severity.clone();
            }
            
            issues.push(RegressionIssue {
                metric: "peak_memory_mb".to_string(),
                current_value: current_metrics.peak_memory_mb,
                baseline_value: baseline_metrics.peak_memory_mb,
                change_percent: comparison.memory_change_percent,
                threshold: self.regression_thresholds.max_memory_increase,
                severity,
            });
        }
        
        // Check error rate regression
        if comparison.error_rate_delta > self.regression_thresholds.max_error_rate_increase {
            let severity = self.calculate_severity(
                comparison.error_rate_delta * 100.0, // Convert to percentage
                self.regression_thresholds.max_error_rate_increase * 100.0
            );
            
            if severity > max_severity {
                max_severity = severity.clone();
            }
            
            issues.push(RegressionIssue {
                metric: "error_rate".to_string(),
                current_value: current_metrics.error_rate,
                baseline_value: baseline_metrics.error_rate,
                change_percent: comparison.error_rate_delta * 100.0,
                threshold: self.regression_thresholds.max_error_rate_increase,
                severity,
            });
        }
        
        Some(RegressionAnalysis {
            has_regression: !issues.is_empty(),
            issues,
            comparison,
            severity: max_severity,
        })
    }
    
    /// Calculate regression severity based on change magnitude
    fn calculate_severity(&self, change_percent: f64, threshold: f64) -> RegressionSeverity {
        let ratio = change_percent / threshold;
        
        if ratio >= 3.0 {
            RegressionSeverity::Critical
        } else if ratio >= 2.0 {
            RegressionSeverity::Major
        } else if ratio >= 1.0 {
            RegressionSeverity::Minor
        } else {
            RegressionSeverity::None
        }
    }
    
    /// Update baseline if metrics represent improvement
    pub fn maybe_update_baseline(&mut self, 
                                benchmark_type: BenchmarkType,
                                current_metrics: PerformanceMetrics,
                                git_commit: Option<String>) -> Result<bool, BaselineError> {
        
        if let Some(baseline_entry) = self.baselines.get(&benchmark_type) {
            let baseline_metrics = &baseline_entry.metrics;
            
            // Check if current metrics are better
            let is_improvement = 
                current_metrics.throughput > baseline_metrics.throughput * 1.05 || // 5% better throughput
                current_metrics.avg_duration < baseline_metrics.avg_duration.mul_f32(0.95) || // 5% faster
                current_metrics.error_rate < baseline_metrics.error_rate * 0.9; // 10% fewer errors
            
            if is_improvement {
                self.set_baseline(benchmark_type, current_metrics, git_commit, 
                                Some("Automatic update due to performance improvement".to_string()))?;
                return Ok(true);
            }
        } else {
            // No baseline exists, set current as baseline
            self.set_baseline(benchmark_type, current_metrics, git_commit, 
                            Some("Initial baseline".to_string()))?;
            return Ok(true);
        }
        
        Ok(false)
    }
    
    /// Get all benchmark types with baselines
    pub fn benchmark_types(&self) -> Vec<BenchmarkType> {
        self.baselines.keys().cloned().collect()
    }
    
    /// Generate baseline report
    pub fn generate_report(&self) -> BaselineReport {
        let entries: Vec<_> = self.baselines.iter()
            .map(|(benchmark_type, entry)| {
                BaselineReportEntry {
                    benchmark_type: benchmark_type.clone(),
                    version: entry.version,
                    created_at: entry.metadata.created_at,
                    throughput: entry.metrics.throughput,
                    avg_duration_ms: entry.metrics.avg_duration.as_millis() as f64,
                    error_rate: entry.metrics.error_rate,
                    memory_mb: entry.metrics.peak_memory_mb,
                }
            })
            .collect();
        
        BaselineReport {
            total_baselines: entries.len(),
            entries,
            generated_at: SystemTime::now()
                .duration_since(UNIX_EPOCH)
                .unwrap()
                .as_secs(),
        }
    }
    
    /// Collect environment variables
    fn collect_environment() -> HashMap<String, String> {
        let mut env = HashMap::new();
        
        if let Ok(value) = std::env::var("RUST_VERSION") {
            env.insert("RUST_VERSION".to_string(), value);
        }
        if let Ok(value) = std::env::var("CARGO_PKG_VERSION") {
            env.insert("CARGO_PKG_VERSION".to_string(), value);
        }
        if let Ok(value) = std::env::var("TARGET") {
            env.insert("TARGET".to_string(), value);
        }
        
        env
    }
    
    /// Collect system information
    fn collect_system_info() -> SystemInfo {
        SystemInfo {
            os: std::env::consts::OS.to_string(),
            arch: std::env::consts::ARCH.to_string(),
            cpu_cores: num_cpus::get(),
            total_memory_gb: 16.0, // Simplified - would use system API in real implementation
            rust_version: "1.70.0".to_string(), // Simplified - would get actual version
        }
    }
}

/// Baseline management errors
#[derive(Debug, thiserror::Error)]
pub enum BaselineError {
    #[error("I/O error: {0}")]
    IoError(String),
    
    #[error("Parse error: {0}")]
    ParseError(String),
    
    #[error("Serialize error: {0}")]
    SerializeError(String),
    
    #[error("No baseline found for benchmark type")]
    NoBaseline,
}

/// Report of all baselines
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BaselineReport {
    pub total_baselines: usize,
    pub entries: Vec<BaselineReportEntry>,
    pub generated_at: u64,
}

/// Entry in baseline report
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BaselineReportEntry {
    pub benchmark_type: BenchmarkType,
    pub version: u32,
    pub created_at: u64,
    pub throughput: f64,
    pub avg_duration_ms: f64,
    pub error_rate: f64,
    pub memory_mb: f64,
}

/// Baseline metrics for export/comparison
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BaselineMetrics {
    pub benchmark_type: BenchmarkType,
    pub throughput: f64,
    pub avg_duration_ms: f64,
    pub p95_duration_ms: f64,
    pub error_rate: f64,
    pub memory_usage_mb: f64,
    pub created_at: u64,
    pub version: u32,
}

impl From<(&BenchmarkType, &BaselineEntry)> for BaselineMetrics {
    fn from((benchmark_type, entry): (&BenchmarkType, &BaselineEntry)) -> Self {
        Self {
            benchmark_type: benchmark_type.clone(),
            throughput: entry.metrics.throughput,
            avg_duration_ms: entry.metrics.avg_duration.as_millis() as f64,
            p95_duration_ms: entry.metrics.p95_duration.as_millis() as f64,
            error_rate: entry.metrics.error_rate,
            memory_usage_mb: entry.metrics.peak_memory_mb,
            created_at: entry.metadata.created_at,
            version: entry.version,
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::time::Duration;
    use tempfile::NamedTempFile;
    
    fn create_test_metrics() -> PerformanceMetrics {
        PerformanceMetrics {
            avg_duration: Duration::from_millis(100),
            median_duration: Duration::from_millis(95),
            p95_duration: Duration::from_millis(150),
            p99_duration: Duration::from_millis(200),
            std_deviation: Duration::from_millis(20),
            throughput: 100.0,
            memory_usage_mb: 50.0,
            peak_memory_mb: 60.0,
            error_rate: 0.01,
            successful_ops: 99,
            failed_ops: 1,
            custom_metrics: HashMap::new(),
        }
    }
    
    #[test]
    fn test_baseline_creation() {
        let mut manager = BaselineManager::new();
        let metrics = create_test_metrics();
        
        let result = manager.set_baseline(
            BenchmarkType::TTFSEncoding,
            metrics.clone(),
            Some("abc123".to_string()),
            Some("Initial baseline".to_string())
        );
        
        assert!(result.is_ok());
        
        let baseline = manager.get_baseline(&BenchmarkType::TTFSEncoding);
        assert!(baseline.is_some());
        
        let entry = baseline.unwrap();
        assert_eq!(entry.version, 1);
        assert_eq!(entry.metrics.throughput, metrics.throughput);
    }
    
    #[test]
    fn test_regression_detection() {
        let mut manager = BaselineManager::new();
        let baseline_metrics = create_test_metrics();
        
        // Set baseline
        manager.set_baseline(
            BenchmarkType::TTFSEncoding,
            baseline_metrics,
            None,
            None
        ).unwrap();
        
        // Create regressed metrics
        let regressed_metrics = PerformanceMetrics {
            throughput: 80.0, // 20% decrease - should trigger regression
            avg_duration: Duration::from_millis(120), // 20% increase
            error_rate: 0.08, // 7% increase
            ..create_test_metrics()
        };
        
        let analysis = manager.check_regression(&BenchmarkType::TTFSEncoding, &regressed_metrics);
        assert!(analysis.is_some());
        
        let analysis = analysis.unwrap();
        assert!(analysis.has_regression);
        assert!(!analysis.issues.is_empty());
        assert!(analysis.severity >= RegressionSeverity::Minor);
    }
    
    #[test]
    fn test_baseline_persistence() {
        let temp_file = NamedTempFile::new().unwrap();
        let path = temp_file.path().to_path_buf();
        
        {
            let mut manager = BaselineManager::with_storage(&path);
            let metrics = create_test_metrics();
            
            manager.set_baseline(
                BenchmarkType::TTFSEncoding,
                metrics,
                None,
                None
            ).unwrap();
        }
        
        // Load in new manager
        let mut new_manager = BaselineManager::with_storage(&path);
        new_manager.load_baselines().unwrap();
        
        let baseline = new_manager.get_baseline(&BenchmarkType::TTFSEncoding);
        assert!(baseline.is_some());
    }
    
    #[test]
    fn test_automatic_baseline_update() {
        let mut manager = BaselineManager::new();
        let baseline_metrics = create_test_metrics();
        
        manager.set_baseline(
            BenchmarkType::TTFSEncoding,
            baseline_metrics,
            None,
            None
        ).unwrap();
        
        // Create improved metrics
        let improved_metrics = PerformanceMetrics {
            throughput: 110.0, // 10% improvement
            avg_duration: Duration::from_millis(90), // 10% faster
            error_rate: 0.005, // 50% fewer errors
            ..create_test_metrics()
        };
        
        let updated = manager.maybe_update_baseline(
            BenchmarkType::TTFSEncoding,
            improved_metrics.clone(),
            Some("def456".to_string())
        ).unwrap();
        
        assert!(updated);
        
        let new_baseline = manager.get_baseline(&BenchmarkType::TTFSEncoding).unwrap();
        assert_eq!(new_baseline.version, 2);
        assert_eq!(new_baseline.metrics.throughput, improved_metrics.throughput);
    }
    
    #[test]
    fn test_baseline_report() {
        let mut manager = BaselineManager::new();
        
        // Add multiple baselines
        for (i, benchmark_type) in [
            BenchmarkType::TTFSEncoding,
            BenchmarkType::ColumnAllocation,
            BenchmarkType::LateralInhibition,
        ].iter().enumerate() {
            let mut metrics = create_test_metrics();
            metrics.throughput = 100.0 + i as f64 * 10.0;
            
            manager.set_baseline(
                benchmark_type.clone(),
                metrics,
                None,
                None
            ).unwrap();
        }
        
        let report = manager.generate_report();
        assert_eq!(report.total_baselines, 3);
        assert_eq!(report.entries.len(), 3);
    }
}
```

2. Add num_cpus and thiserror dependencies to cortex-benchmarks Cargo.toml if not already present:
   ```toml
   num_cpus = "1.16"
   thiserror = "1.0"
   tempfile = { version = "3.8", optional = true }
   
   [features]
   default = []
   test-utils = ["tempfile"]
   ```

3. Run tests:
   cd crates/cortex-benchmarks
   cargo test baseline

All tests should pass.
```

## Success Criteria
- [ ] BaselineManager with versioning
- [ ] Automatic regression detection
- [ ] Baseline persistence to JSON files
- [ ] Improvement-based automatic updates
- [ ] Comprehensive baseline reporting