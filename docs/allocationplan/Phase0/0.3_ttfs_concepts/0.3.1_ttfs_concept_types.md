# Micro-Phase 0.3.1: Define TTFS Concept Types

## Objective
Define the Time-to-First-Spike (TTFS) concept representation with spike patterns and neural encoding.

## Prerequisites
- neuromorphic-core crate structure ready (0.2 complete)
- Understanding of TTFS encoding principles

## Input
- TTFS encoding specification from Phase 0
- Spike pattern requirements
- Semantic feature representation needs

## Task Details

### Step 1: Define Core TTFS Types
Create the fundamental types for spike-based concept encoding.

### Step 2: Implement Spike Pattern Structure
Build the spike pattern representation with timing information.

### Step 3: Create Concept Metadata
Add semantic and structural metadata to concepts.

### Step 4: Add Serialization Support
Enable persistence of TTFS concepts.

## Expected Output
- `src/ttfs_concept.rs` file updated with full implementation
- `src/ttfs_concept/` module created with submodules
- Spike pattern encoding structures defined
- Tests passing

## Verification Steps
1. Run `cargo test ttfs_concept` tests
2. Verify spike patterns have proper timing
3. Check serialization works correctly
4. Confirm concept creation is ergonomic

## Time Estimate
30-40 minutes

## AI Execution Prompt
```
Implement the TTFS concept types and structures.

1. Create the module structure:
   mkdir -p crates/neuromorphic-core/src/ttfs_concept
   touch crates/neuromorphic-core/src/ttfs_concept/mod.rs
   touch crates/neuromorphic-core/src/ttfs_concept/spike_pattern.rs
   touch crates/neuromorphic-core/src/ttfs_concept/encoding.rs

2. Update src/ttfs_concept.rs to be a module declaration:
```rust
//! TTFS-encoded concept representation

pub mod spike_pattern;
pub mod encoding;

use serde::{Deserialize, Serialize};
use std::time::Duration;

pub use spike_pattern::{SpikePattern, SpikeEvent};
pub use encoding::{TTFSEncoder, EncodingConfig};

/// Time-to-First-Spike encoded concept
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TTFSConcept {
    /// Unique identifier
    pub id: uuid::Uuid,
    
    /// Human-readable name
    pub name: String,
    
    /// Semantic features for neural encoding
    pub semantic_features: Vec<f32>,
    
    /// Spike pattern representation
    pub spike_pattern: SpikePattern,
    
    /// Concept metadata
    pub metadata: ConceptMetadata,
    
    /// Creation timestamp
    pub created_at: chrono::DateTime<chrono::Utc>,
}

/// Metadata associated with a concept
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ConceptMetadata {
    /// Source of the concept (e.g., "parsed", "inferred", "manual")
    pub source: String,
    
    /// Confidence score (0.0 to 1.0)
    pub confidence: f32,
    
    /// Parent concept ID if this is a specialized concept
    pub parent_id: Option<uuid::Uuid>,
    
    /// Properties as key-value pairs
    pub properties: std::collections::HashMap<String, String>,
    
    /// Tags for categorization
    pub tags: Vec<String>,
}

impl TTFSConcept {
    /// Create a new TTFS concept
    pub fn new(name: &str) -> Self {
        Self {
            id: uuid::Uuid::new_v4(),
            name: name.to_string(),
            semantic_features: Vec::new(),
            spike_pattern: SpikePattern::default(),
            metadata: ConceptMetadata::default(),
            created_at: chrono::Utc::now(),
        }
    }
    
    /// Create with semantic features
    pub fn with_features(name: &str, features: Vec<f32>) -> Self {
        let mut concept = Self::new(name);
        concept.semantic_features = features;
        
        // Generate spike pattern from features
        let encoder = TTFSEncoder::default();
        concept.spike_pattern = encoder.encode(&features);
        
        concept
    }
    
    /// Add a property to the concept
    pub fn add_property(&mut self, key: String, value: String) {
        self.metadata.properties.insert(key, value);
    }
    
    /// Set parent concept
    pub fn set_parent(&mut self, parent_id: uuid::Uuid) {
        self.metadata.parent_id = Some(parent_id);
    }
    
    /// Get time-to-first-spike
    pub fn time_to_first_spike(&self) -> Option<Duration> {
        self.spike_pattern.first_spike_time()
    }
}

impl Default for ConceptMetadata {
    fn default() -> Self {
        Self {
            source: "unknown".to_string(),
            confidence: 1.0,
            parent_id: None,
            properties: std::collections::HashMap::new(),
            tags: Vec::new(),
        }
    }
}
```

3. Implement src/ttfs_concept/spike_pattern.rs:
```rust
//! Spike pattern representation for TTFS encoding

use serde::{Deserialize, Serialize};
use std::time::Duration;

/// A spike event in the TTFS pattern
#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
pub struct SpikeEvent {
    /// Neuron that spiked
    pub neuron_id: u32,
    
    /// Time since pattern start
    pub timestamp: Duration,
    
    /// Spike amplitude (normalized 0.0 to 1.0)
    pub amplitude: f32,
    
    /// Frequency component (Hz)
    pub frequency: f32,
}

/// Pattern of spikes representing a concept
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SpikePattern {
    /// Ordered sequence of spike events
    pub events: Vec<SpikeEvent>,
    
    /// Total pattern duration
    pub duration: Duration,
    
    /// Pattern complexity metric
    pub complexity: f32,
    
    /// Spike density (spikes per millisecond)
    pub density: f32,
}

impl SpikePattern {
    /// Create a new spike pattern
    pub fn new(events: Vec<SpikeEvent>) -> Self {
        let duration = events.iter()
            .map(|e| e.timestamp)
            .max()
            .unwrap_or(Duration::ZERO);
        
        let density = if duration.as_millis() > 0 {
            events.len() as f32 / duration.as_millis() as f32
        } else {
            0.0
        };
        
        let complexity = Self::calculate_complexity(&events);
        
        Self {
            events,
            duration,
            complexity,
            density,
        }
    }
    
    /// Get time of first spike
    pub fn first_spike_time(&self) -> Option<Duration> {
        self.events.iter()
            .map(|e| e.timestamp)
            .min()
    }
    
    /// Get time of last spike
    pub fn last_spike_time(&self) -> Option<Duration> {
        self.events.iter()
            .map(|e| e.timestamp)
            .max()
    }
    
    /// Calculate inter-spike intervals
    pub fn inter_spike_intervals(&self) -> Vec<Duration> {
        let mut sorted_events = self.events.clone();
        sorted_events.sort_by_key(|e| e.timestamp);
        
        sorted_events.windows(2)
            .map(|pair| pair[1].timestamp - pair[0].timestamp)
            .collect()
    }
    
    /// Calculate pattern complexity based on temporal structure
    fn calculate_complexity(events: &[SpikeEvent]) -> f32 {
        if events.len() < 2 {
            return 0.0;
        }
        
        // Complexity based on:
        // 1. Number of unique neurons
        // 2. Temporal distribution
        // 3. Frequency diversity
        
        let unique_neurons = events.iter()
            .map(|e| e.neuron_id)
            .collect::<std::collections::HashSet<_>>()
            .len();
        
        let frequency_variance = Self::calculate_frequency_variance(events);
        let temporal_entropy = Self::calculate_temporal_entropy(events);
        
        let complexity = (unique_neurons as f32 / events.len() as f32) 
            * frequency_variance 
            * temporal_entropy;
        
        complexity.clamp(0.0, 1.0)
    }
    
    fn calculate_frequency_variance(events: &[SpikeEvent]) -> f32 {
        if events.is_empty() {
            return 0.0;
        }
        
        let mean_freq = events.iter().map(|e| e.frequency).sum::<f32>() / events.len() as f32;
        let variance = events.iter()
            .map(|e| (e.frequency - mean_freq).powi(2))
            .sum::<f32>() / events.len() as f32;
        
        variance.sqrt() / 100.0 // Normalize assuming max frequency ~100Hz
    }
    
    fn calculate_temporal_entropy(events: &[SpikeEvent]) -> f32 {
        // Simple entropy calculation based on spike timing distribution
        let total_duration = events.iter()
            .map(|e| e.timestamp.as_millis())
            .max()
            .unwrap_or(1) as f32;
        
        let mut bins = vec![0u32; 10]; // 10 time bins
        
        for event in events {
            let bin = ((event.timestamp.as_millis() as f32 / total_duration) * 9.0) as usize;
            bins[bin.min(9)] += 1;
        }
        
        let total = events.len() as f32;
        let entropy = bins.iter()
            .filter(|&&count| count > 0)
            .map(|&count| {
                let p = count as f32 / total;
                -p * p.ln()
            })
            .sum::<f32>();
        
        entropy / 2.3 // Normalize by ln(10)
    }
}

impl Default for SpikePattern {
    fn default() -> Self {
        Self {
            events: Vec::new(),
            duration: Duration::ZERO,
            complexity: 0.0,
            density: 0.0,
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_spike_pattern_creation() {
        let events = vec![
            SpikeEvent {
                neuron_id: 1,
                timestamp: Duration::from_millis(10),
                amplitude: 0.8,
                frequency: 40.0,
            },
            SpikeEvent {
                neuron_id: 2,
                timestamp: Duration::from_millis(25),
                amplitude: 0.6,
                frequency: 60.0,
            },
        ];
        
        let pattern = SpikePattern::new(events);
        
        assert_eq!(pattern.events.len(), 2);
        assert_eq!(pattern.duration, Duration::from_millis(25));
        assert!(pattern.density > 0.0);
        assert_eq!(pattern.first_spike_time(), Some(Duration::from_millis(10)));
    }
    
    #[test]
    fn test_inter_spike_intervals() {
        let events = vec![
            SpikeEvent {
                neuron_id: 1,
                timestamp: Duration::from_millis(0),
                amplitude: 1.0,
                frequency: 50.0,
            },
            SpikeEvent {
                neuron_id: 2,
                timestamp: Duration::from_millis(10),
                amplitude: 1.0,
                frequency: 50.0,
            },
            SpikeEvent {
                neuron_id: 3,
                timestamp: Duration::from_millis(30),
                amplitude: 1.0,
                frequency: 50.0,
            },
        ];
        
        let pattern = SpikePattern::new(events);
        let isis = pattern.inter_spike_intervals();
        
        assert_eq!(isis.len(), 2);
        assert_eq!(isis[0], Duration::from_millis(10));
        assert_eq!(isis[1], Duration::from_millis(20));
    }
}
```

4. Create placeholder for encoding.rs:
```rust
//! TTFS encoding utilities

use super::spike_pattern::{SpikePattern, SpikeEvent};
use std::time::Duration;

#[derive(Debug, Clone)]
pub struct EncodingConfig {
    pub max_spike_time: Duration,
    pub num_neurons: u32,
}

impl Default for EncodingConfig {
    fn default() -> Self {
        Self {
            max_spike_time: Duration::from_millis(100),
            num_neurons: 100,
        }
    }
}

pub struct TTFSEncoder {
    config: EncodingConfig,
}

impl Default for TTFSEncoder {
    fn default() -> Self {
        Self {
            config: EncodingConfig::default(),
        }
    }
}

impl TTFSEncoder {
    pub fn encode(&self, features: &[f32]) -> SpikePattern {
        // Placeholder implementation
        let events = features.iter()
            .enumerate()
            .filter_map(|(i, &feature)| {
                if feature > 0.5 {
                    Some(SpikeEvent {
                        neuron_id: i as u32,
                        timestamp: Duration::from_millis((100.0 * (1.0 - feature)) as u64),
                        amplitude: feature,
                        frequency: 50.0 * feature,
                    })
                } else {
                    None
                }
            })
            .collect();
        
        SpikePattern::new(events)
    }
}
```

5. Update Cargo.toml to add uuid and chrono dependencies:
   Add to [dependencies]:
   uuid = { version = "1.6", features = ["serde", "v4"] }
   chrono = { version = "0.4", features = ["serde"] }

6. Run tests:
   cd crates/neuromorphic-core
   cargo test ttfs_concept
```

## Success Criteria
- [ ] TTFSConcept type fully defined
- [ ] SpikePattern structure implemented
- [ ] Spike timing calculations working
- [ ] Metadata and properties support added
- [ ] Serialization with serde functional